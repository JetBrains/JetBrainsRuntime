From 1411794eeca7b619a2761d277586a855106a3c58 Mon Sep 17 00:00:00 2001
From: Vladimir Dvorak <vladimir.dvorak@jetbrains.com>
Date: Wed, 14 Nov 2018 21:09:39 +0100
Subject: [PATCH 01/34] Apply basic dcevm11 patch

---
 src/hotspot/share/ci/ciObjectFactory.cpp      |   25 +
 src/hotspot/share/ci/ciObjectFactory.hpp      |    3 +
 .../share/classfile/classFileParser.cpp       |   18 +-
 .../share/classfile/classFileParser.hpp       |    7 +
 src/hotspot/share/classfile/classLoader.cpp   |    1 +
 .../share/classfile/classLoaderDataGraph.cpp  |   12 +
 .../share/classfile/classLoaderDataGraph.hpp  |    5 +
 .../share/classfile/classLoaderExt.cpp        |    1 +
 src/hotspot/share/classfile/dictionary.cpp    |   44 +-
 src/hotspot/share/classfile/dictionary.hpp    |   10 +
 src/hotspot/share/classfile/javaClasses.cpp   |   63 +
 src/hotspot/share/classfile/javaClasses.hpp   |   53 +
 .../share/classfile/javaClasses.inline.hpp    |    8 +
 src/hotspot/share/classfile/klassFactory.cpp  |    3 +
 src/hotspot/share/classfile/klassFactory.hpp  |    1 +
 .../share/classfile/loaderConstraints.cpp     |   15 +-
 .../share/classfile/loaderConstraints.hpp     |    3 +
 .../share/classfile/systemDictionary.cpp      |   47 +-
 .../share/classfile/systemDictionary.hpp      |    9 +-
 .../classfile/systemDictionaryShared.cpp      |    2 +-
 src/hotspot/share/classfile/verifier.cpp      |    2 +-
 src/hotspot/share/classfile/verifier.hpp      |    1 +
 src/hotspot/share/classfile/vmSymbols.hpp     |    8 +
 .../share/gc/cms/compactibleFreeListSpace.cpp | 3148 +++++++++++++++++
 .../share/gc/cms/compactibleFreeListSpace.hpp |  759 ++++
 src/hotspot/share/gc/serial/genMarkSweep.cpp  |    4 +
 src/hotspot/share/gc/serial/markSweep.cpp     |   99 +
 src/hotspot/share/gc/serial/markSweep.hpp     |    7 +
 src/hotspot/share/gc/shared/gcConfig.cpp      |    5 +-
 src/hotspot/share/gc/shared/space.cpp         |  200 +-
 src/hotspot/share/gc/shared/space.hpp         |   18 +-
 src/hotspot/share/gc/shared/space.inline.hpp  |   49 +-
 .../share/interpreter/linkResolver.cpp        |    7 +-
 .../jfrEventClassTransformer.cpp              |    1 +
 src/hotspot/share/memory/universe.cpp         |   39 +
 src/hotspot/share/memory/universe.hpp         |   12 +
 src/hotspot/share/oops/cpCache.cpp            |   32 +-
 src/hotspot/share/oops/cpCache.hpp            |   16 +-
 src/hotspot/share/oops/instanceKlass.cpp      |   55 +-
 src/hotspot/share/oops/instanceKlass.hpp      |    7 +
 src/hotspot/share/oops/klass.cpp              |   29 +-
 src/hotspot/share/oops/klass.hpp              |   47 +
 src/hotspot/share/oops/method.cpp             |    6 +
 src/hotspot/share/oops/method.hpp             |   20 +
 src/hotspot/share/prims/jni.cpp               |    1 +
 src/hotspot/share/prims/jvm.cpp               |    1 +
 .../prims/jvmtiEnhancedRedefineClasses.cpp    | 2255 ++++++++++++
 .../prims/jvmtiEnhancedRedefineClasses.hpp    |  202 ++
 src/hotspot/share/prims/jvmtiEnv.cpp          |   44 +-
 src/hotspot/share/prims/jvmtiExport.cpp       |    2 +-
 src/hotspot/share/prims/jvmtiExport.hpp       |    1 +
 .../share/prims/jvmtiGetLoadedClasses.cpp     |   19 +-
 src/hotspot/share/prims/jvmtiImpl.cpp         |    5 +
 src/hotspot/share/runtime/arguments.cpp       |   32 +
 src/hotspot/share/runtime/arguments.hpp       |    1 +
 src/hotspot/share/runtime/globals.hpp         |    5 +
 .../share/runtime/interfaceSupport.inline.hpp |    4 +-
 src/hotspot/share/runtime/javaCalls.cpp       |    3 +-
 src/hotspot/share/runtime/mutexLocker.cpp     |    4 +-
 src/hotspot/share/runtime/mutexLocker.hpp     |    2 +
 src/hotspot/share/runtime/reflection.cpp      |    6 +
 61 files changed, 7423 insertions(+), 65 deletions(-)
 create mode 100644 src/hotspot/share/gc/cms/compactibleFreeListSpace.cpp
 create mode 100644 src/hotspot/share/gc/cms/compactibleFreeListSpace.hpp
 create mode 100644 src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.cpp
 create mode 100644 src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.hpp

diff --git a/src/hotspot/share/ci/ciObjectFactory.cpp b/src/hotspot/share/ci/ciObjectFactory.cpp
index f74f5e627a0..d713570fccf 100644
--- a/src/hotspot/share/ci/ciObjectFactory.cpp
+++ b/src/hotspot/share/ci/ciObjectFactory.cpp
@@ -722,3 +722,28 @@ void ciObjectFactory::print() {
              _unloaded_instances->length(),
              _unloaded_klasses->length());
 }
+
+
+int ciObjectFactory::compare_cimetadata(ciMetadata** a, ciMetadata** b) {
+  Metadata* am = (*a)->constant_encoding();
+  Metadata* bm = (*b)->constant_encoding();
+  return ((am > bm) ? 1 : ((am == bm) ? 0 : -1));
+}
+
+// FIXME: review... Resoring the ciObject arrays after class redefinition
+void ciObjectFactory::resort_shared_ci_metadata() {
+  if (_shared_ci_metadata == NULL) return;
+  _shared_ci_metadata->sort(ciObjectFactory::compare_cimetadata);
+
+#ifdef ASSERT
+  if (CIObjectFactoryVerify) {
+    Metadata* last = NULL;
+    for (int j = 0; j< _shared_ci_metadata->length(); j++) {
+      Metadata* o = _shared_ci_metadata->at(j)->constant_encoding();
+      assert(last < o, "out of order");
+      last = o;
+    }
+  }
+#endif // ASSERT
+}
+
diff --git a/src/hotspot/share/ci/ciObjectFactory.hpp b/src/hotspot/share/ci/ciObjectFactory.hpp
index f78ae05a905..875462c3168 100644
--- a/src/hotspot/share/ci/ciObjectFactory.hpp
+++ b/src/hotspot/share/ci/ciObjectFactory.hpp
@@ -88,6 +88,7 @@ private:
 
   ciInstance* get_unloaded_instance(ciInstanceKlass* klass);
 
+  static int compare_cimetadata(ciMetadata** a, ciMetadata** b);
 public:
   static bool is_initialized() { return _initialized; }
 
@@ -144,6 +145,8 @@ public:
 
   void print_contents();
   void print();
+
+  static void resort_shared_ci_metadata();
 };
 
 #endif // SHARE_CI_CIOBJECTFACTORY_HPP
diff --git a/src/hotspot/share/classfile/classFileParser.cpp b/src/hotspot/share/classfile/classFileParser.cpp
index 5e81e786dc5..0192e9e006f 100644
--- a/src/hotspot/share/classfile/classFileParser.cpp
+++ b/src/hotspot/share/classfile/classFileParser.cpp
@@ -964,6 +964,8 @@ void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,
                                                   CHECK);
       }
 
+      interf = (Klass *) maybe_newest(interf);
+
       if (!interf->is_interface()) {
         THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),
                   err_msg("class %s can not implement %s, because it is not an interface (%s)",
@@ -4012,7 +4014,7 @@ const InstanceKlass* ClassFileParser::parse_super_class(ConstantPool* const cp,
     // However, make sure it is not an array type.
     bool is_array = false;
     if (cp->tag_at(super_class_index).is_klass()) {
-      super_klass = InstanceKlass::cast(cp->resolved_klass_at(super_class_index));
+      super_klass = InstanceKlass::cast(maybe_newest(cp->resolved_klass_at(super_class_index)));
       if (need_verify)
         is_array = super_klass->is_array_klass();
     } else if (need_verify) {
@@ -4615,7 +4617,10 @@ void ClassFileParser::set_precomputed_flags(InstanceKlass* ik) {
   if (!_has_empty_finalizer) {
     if (_has_finalizer ||
         (super != NULL && super->has_finalizer())) {
-      ik->set_has_finalizer();
+        // FIXME - condition from previous DCEVM version, however after reload new finelize() method is not active
+        if (ik->old_version() == NULL || ik->old_version()->has_finalizer()) {
+          ik->set_has_finalizer();
+        }
     }
   }
 
@@ -6062,6 +6067,7 @@ ClassFileParser::ClassFileParser(ClassFileStream* stream,
                                  ClassLoaderData* loader_data,
                                  const ClassLoadInfo* cl_info,
                                  Publicity pub_level,
+                                 const bool pick_newest,
                                  TRAPS) :
   _stream(stream),
   _class_name(NULL),
@@ -6125,7 +6131,8 @@ ClassFileParser::ClassFileParser(ClassFileStream* stream,
   _has_finalizer(false),
   _has_empty_finalizer(false),
   _has_vanilla_constructor(false),
-  _max_bootstrap_specifier_index(-1) {
+  _max_bootstrap_specifier_index(-1),
+  _pick_newest(pick_newest) {
 
   _class_name = name != NULL ? name : vmSymbols::unknown_class_name();
   _class_name->increment_refcount();
@@ -6604,14 +6611,15 @@ void ClassFileParser::post_process_parsed_stream(const ClassFileStream* const st
         CHECK);
     }
     Handle loader(THREAD, _loader_data->class_loader());
-    _super_klass = (const InstanceKlass*)
+    const Klass* super_klass =
                        SystemDictionary::resolve_super_or_fail(_class_name,
                                                                super_class_name,
                                                                loader,
                                                                _protection_domain,
                                                                true,
                                                                CHECK);
-  }
+   _super_klass = (const InstanceKlass*) maybe_newest(super_klass);
+ }
 
   if (_super_klass != NULL) {
     if (_super_klass->has_nonstatic_concrete_methods()) {
diff --git a/src/hotspot/share/classfile/classFileParser.hpp b/src/hotspot/share/classfile/classFileParser.hpp
index bf5fccf340c..6660616ccad 100644
--- a/src/hotspot/share/classfile/classFileParser.hpp
+++ b/src/hotspot/share/classfile/classFileParser.hpp
@@ -150,6 +150,9 @@ class ClassFileParser {
   const intArray* _method_ordering;
   GrowableArray<Method*>* _all_mirandas;
 
+  // Enhanced class redefinition
+  const bool _pick_newest;
+
   enum { fixed_buffer_size = 128 };
   u_char _linenumbertable_buffer[fixed_buffer_size];
 
@@ -541,6 +544,8 @@ class ClassFileParser {
                      TRAPS);
 
   void update_class_name(Symbol* new_name);
+  // Enhanced class redefinition
+  inline const Klass* maybe_newest(const Klass* klass) const { return klass != NULL && _pick_newest ? klass->newest_version() : klass; }
 
  public:
   ClassFileParser(ClassFileStream* stream,
@@ -548,6 +553,7 @@ class ClassFileParser {
                   ClassLoaderData* loader_data,
                   const ClassLoadInfo* cl_info,
                   Publicity pub_level,
+                  const bool pick_newest,
                   TRAPS);
 
   ~ClassFileParser();
@@ -576,6 +582,7 @@ class ClassFileParser {
   ClassLoaderData* loader_data() const { return _loader_data; }
   const Symbol* class_name() const { return _class_name; }
   const InstanceKlass* super_klass() const { return _super_klass; }
+  Array<Klass*>* local_interfaces() const { return _local_interfaces; }
 
   ReferenceType reference_type() const { return _rt; }
   AccessFlags access_flags() const { return _access_flags; }
diff --git a/src/hotspot/share/classfile/classLoader.cpp b/src/hotspot/share/classfile/classLoader.cpp
index a53ccea0933..93ef3ea254d 100644
--- a/src/hotspot/share/classfile/classLoader.cpp
+++ b/src/hotspot/share/classfile/classLoader.cpp
@@ -1275,6 +1275,7 @@ InstanceKlass* ClassLoader::load_class(Symbol* name, bool search_append_only, TR
                                                            name,
                                                            loader_data,
                                                            cl_info,
+                                                           false, // pick_newest
                                                            THREAD);
   if (HAS_PENDING_EXCEPTION) {
     if (DumpSharedSpaces) {
diff --git a/src/hotspot/share/classfile/classLoaderDataGraph.cpp b/src/hotspot/share/classfile/classLoaderDataGraph.cpp
index 9b104d9ba4d..db0fbcc45dc 100644
--- a/src/hotspot/share/classfile/classLoaderDataGraph.cpp
+++ b/src/hotspot/share/classfile/classLoaderDataGraph.cpp
@@ -439,6 +439,18 @@ void ClassLoaderDataGraph::dictionary_classes_do(void f(InstanceKlass*, TRAPS),
   }
 }
 
+void ClassLoaderDataGraph::dictionary_classes_do(KlassClosure* klass_closure) {
+  FOR_ALL_DICTIONARY(cld) {
+    cld->dictionary()->classes_do(klass_closure);
+  }
+}
+
+void ClassLoaderDataGraph::rollback_redefinition() {
+  FOR_ALL_DICTIONARY(cld) {
+    cld->dictionary()->rollback_redefinition();
+  }
+}
+
 void ClassLoaderDataGraph::verify_dictionary() {
   FOR_ALL_DICTIONARY(cld) {
     cld->dictionary()->verify();
diff --git a/src/hotspot/share/classfile/classLoaderDataGraph.hpp b/src/hotspot/share/classfile/classLoaderDataGraph.hpp
index 4251afab3dc..f380aa3fa34 100644
--- a/src/hotspot/share/classfile/classLoaderDataGraph.hpp
+++ b/src/hotspot/share/classfile/classLoaderDataGraph.hpp
@@ -102,6 +102,11 @@ class ClassLoaderDataGraph : public AllStatic {
   // Added for initialize_itable_for_klass to handle exceptions.
   static void dictionary_classes_do(void f(InstanceKlass*, TRAPS), TRAPS);
 
+  static void dictionary_classes_do(KlassClosure* klass_closure);
+
+  // Enhanced class redefinition
+  static void rollback_redefinition();
+
   // VM_CounterDecay iteration support
   static InstanceKlass* try_get_next_class();
   static void adjust_saved_class(ClassLoaderData* cld);
diff --git a/src/hotspot/share/classfile/classLoaderExt.cpp b/src/hotspot/share/classfile/classLoaderExt.cpp
index 701c155da19..9bf4f38cf21 100644
--- a/src/hotspot/share/classfile/classLoaderExt.cpp
+++ b/src/hotspot/share/classfile/classLoaderExt.cpp
@@ -290,6 +290,7 @@ InstanceKlass* ClassLoaderExt::load_class(Symbol* name, const char* path, TRAPS)
                                                            name,
                                                            loader_data,
                                                            cl_info,
+                                                           false,
                                                            THREAD);
 
   if (HAS_PENDING_EXCEPTION) {
diff --git a/src/hotspot/share/classfile/dictionary.cpp b/src/hotspot/share/classfile/dictionary.cpp
index 8b10f5abdb1..45c979a1738 100644
--- a/src/hotspot/share/classfile/dictionary.cpp
+++ b/src/hotspot/share/classfile/dictionary.cpp
@@ -216,6 +216,19 @@ void Dictionary::classes_do(void f(InstanceKlass*)) {
   }
 }
 
+void Dictionary::classes_do(KlassClosure* closure) {
+  for (int index = 0; index < table_size(); index++) {
+    for (DictionaryEntry* probe = bucket(index);
+                          probe != NULL;
+                          probe = probe->next()) {
+      InstanceKlass* k = probe->instance_klass();
+      if (loader_data() == k->class_loader_data()) {
+        closure->do_klass(k);
+      }
+    }
+  }
+}
+
 // Added for initialize_itable_for_klass to handle exceptions
 //   Just the classes from defining class loaders
 void Dictionary::classes_do(void f(InstanceKlass*, TRAPS), TRAPS) {
@@ -297,6 +310,33 @@ DictionaryEntry* Dictionary::get_entry(int index, unsigned int hash,
   return NULL;
 }
 
+bool Dictionary::update_klass(unsigned int hash, Symbol* name, ClassLoaderData* loader_data, InstanceKlass* k, InstanceKlass* old_klass) {
+  // There are several entries for the same class in the dictionary: One extra entry for each parent classloader of the classloader of the class.
+  bool found = false;
+  for (int index = 0; index < table_size(); index++) {
+    for (DictionaryEntry* entry = bucket(index); entry != NULL; entry = entry->next()) {
+      if (entry->instance_klass() == old_klass) {
+        entry->set_literal(k);
+        found = true;
+      }
+    }
+  }
+  return found;
+}
+
+void Dictionary::rollback_redefinition() {
+  for (int index = 0; index < table_size(); index++) {
+    for (DictionaryEntry* entry = bucket(index);
+                          entry != NULL;
+                          entry = entry->next()) {
+      if (entry->instance_klass()->is_redefining()) {
+        entry->set_literal((InstanceKlass*) entry->instance_klass()->old_version());
+      }
+    }
+  }
+}
+
+
 
 InstanceKlass* Dictionary::find(unsigned int hash, Symbol* name,
                                 Handle protection_domain) {
@@ -305,7 +345,7 @@ InstanceKlass* Dictionary::find(unsigned int hash, Symbol* name,
   int index = hash_to_index(hash);
   DictionaryEntry* entry = get_entry(index, hash, name);
   if (entry != NULL && entry->is_valid_protection_domain(protection_domain)) {
-    return entry->instance_klass();
+    return old_if_redefined(entry->instance_klass());
   } else {
     return NULL;
   }
@@ -317,7 +357,7 @@ InstanceKlass* Dictionary::find_class(int index, unsigned int hash,
   assert (index == index_for(name), "incorrect index?");
 
   DictionaryEntry* entry = get_entry(index, hash, name);
-  return (entry != NULL) ? entry->instance_klass() : NULL;
+  return old_if_redefined((entry != NULL) ? entry->instance_klass() : NULL);
 }
 
 
diff --git a/src/hotspot/share/classfile/dictionary.hpp b/src/hotspot/share/classfile/dictionary.hpp
index 99b9799753f..b4f5cd5c4df 100644
--- a/src/hotspot/share/classfile/dictionary.hpp
+++ b/src/hotspot/share/classfile/dictionary.hpp
@@ -65,6 +65,7 @@ public:
   InstanceKlass* find_class(int index, unsigned int hash, Symbol* name);
 
   void classes_do(void f(InstanceKlass*));
+  void classes_do(KlassClosure* closure);
   void classes_do(void f(InstanceKlass*, TRAPS), TRAPS);
   void all_entries_do(KlassClosure* closure);
   void classes_do(MetaspaceClosure* it);
@@ -104,6 +105,15 @@ public:
   }
 
   void free_entry(DictionaryEntry* entry);
+
+  // Enhanced class redefinition
+  bool update_klass(unsigned int hash, Symbol* name, ClassLoaderData* loader_data, InstanceKlass* k, InstanceKlass* old_klass);
+
+  void rollback_redefinition();
+
+  static InstanceKlass* old_if_redefined(InstanceKlass* k) {
+    return (k != NULL && k->is_redefining()) ? ((InstanceKlass* )k->old_version()) : k;
+  }
 };
 
 // An entry in the class loader data dictionaries, this describes a class as
diff --git a/src/hotspot/share/classfile/javaClasses.cpp b/src/hotspot/share/classfile/javaClasses.cpp
index ca1f1f0d71d..9b086a241f7 100644
--- a/src/hotspot/share/classfile/javaClasses.cpp
+++ b/src/hotspot/share/classfile/javaClasses.cpp
@@ -2540,6 +2540,8 @@ void java_lang_Throwable::fill_in_stack_trace(Handle throwable, const methodHand
         skip_throwableInit_check = true;
       }
     }
+    // (DCEVM): Line numbers from newest version must be used for EMCP-swapped methods
+    method = method->newest_version();
     if (method->is_hidden()) {
       if (skip_hidden) {
         if (total_count == 0) {
@@ -3747,6 +3749,62 @@ void java_lang_invoke_DirectMethodHandle::serialize_offsets(SerializeClosure* f)
 }
 #endif
 
+// Support for java_lang_invoke_DirectMethodHandle$StaticAccessor
+
+int java_lang_invoke_DirectMethodHandle_StaticAccessor::_static_offset_offset;
+
+long java_lang_invoke_DirectMethodHandle_StaticAccessor::static_offset(oop dmh) {
+  assert(_static_offset_offset != 0, "");
+  return dmh->long_field(_static_offset_offset);
+}
+
+void java_lang_invoke_DirectMethodHandle_StaticAccessor::set_static_offset(oop dmh, long static_offset) {
+  assert(_static_offset_offset != 0, "");
+  dmh->long_field_put(_static_offset_offset, static_offset);
+}
+
+#define DIRECTMETHODHANDLE_STATIC_ACCESSOR_FIELDS_DO(macro) \
+  macro(_static_offset_offset, k, vmSymbols::static_offset_name(), long_signature, false)
+
+void java_lang_invoke_DirectMethodHandle_StaticAccessor::compute_offsets() {
+  InstanceKlass* k = SystemDictionary::DirectMethodHandle_StaticAccessor_klass();
+  DIRECTMETHODHANDLE_STATIC_ACCESSOR_FIELDS_DO(FIELD_COMPUTE_OFFSET);
+}
+
+#if INCLUDE_CDS
+void java_lang_invoke_DirectMethodHandle_StaticAccessor::serialize_offsets(SerializeClosure* f) {
+  DIRECTMETHODHANDLE_STATIC_ACCESSOR_FIELDS_DO(FIELD_SERIALIZE_OFFSET);
+}
+#endif
+
+// Support for java_lang_invoke_DirectMethodHandle$Accessor
+
+int java_lang_invoke_DirectMethodHandle_Accessor::_field_offset_offset;
+
+int java_lang_invoke_DirectMethodHandle_Accessor::field_offset(oop dmh) {
+  assert(_field_offset_offset != 0, "");
+  return dmh->int_field(_field_offset_offset);
+}
+
+void java_lang_invoke_DirectMethodHandle_Accessor::set_field_offset(oop dmh, int field_offset) {
+  assert(_field_offset_offset != 0, "");
+  dmh->int_field_put(_field_offset_offset, field_offset);
+}
+
+#define DIRECTMETHODHANDLE_ACCESSOR_FIELDS_DO(macro) \
+  macro(_field_offset_offset, k, vmSymbols::field_offset_name(), int_signature, false)
+
+void java_lang_invoke_DirectMethodHandle_Accessor::compute_offsets() {
+  InstanceKlass* k = SystemDictionary::DirectMethodHandle_Accessor_klass();
+  DIRECTMETHODHANDLE_ACCESSOR_FIELDS_DO(FIELD_COMPUTE_OFFSET);
+}
+
+#if INCLUDE_CDS
+void java_lang_invoke_DirectMethodHandle_Accessor::serialize_offsets(SerializeClosure* f) {
+  DIRECTMETHODHANDLE_ACCESSOR_FIELDS_DO(FIELD_SERIALIZE_OFFSET);
+}
+#endif
+
 // Support for java_lang_invoke_MethodHandle
 
 int java_lang_invoke_MethodHandle::_type_offset;
@@ -3938,6 +3996,11 @@ void java_lang_invoke_ResolvedMethodName::set_vmholder(oop resolved_method, oop
   resolved_method->obj_field_put(_vmholder_offset, holder);
 }
 
+void java_lang_invoke_ResolvedMethodName::set_vmholder_offset(oop resolved_method, Method* m) {
+  assert(is_instance(resolved_method), "wrong type");
+  resolved_method->obj_field_put(_vmholder_offset, m->method_holder()->java_mirror());
+}
+
 oop java_lang_invoke_ResolvedMethodName::find_resolved_method(const methodHandle& m, TRAPS) {
   const Method* method = m();
 
diff --git a/src/hotspot/share/classfile/javaClasses.hpp b/src/hotspot/share/classfile/javaClasses.hpp
index e33391c63e0..a68c5139151 100644
--- a/src/hotspot/share/classfile/javaClasses.hpp
+++ b/src/hotspot/share/classfile/javaClasses.hpp
@@ -56,6 +56,8 @@ class RecordComponent;
   f(java_lang_invoke_MethodType) \
   f(java_lang_invoke_CallSite) \
   f(java_lang_invoke_ConstantCallSite) \
+  f(java_lang_invoke_DirectMethodHandle_StaticAccessor) \
+  f(java_lang_invoke_DirectMethodHandle_Accessor) \
   f(java_lang_invoke_MethodHandleNatives_CallSiteContext) \
   f(java_security_AccessControlContext) \
   f(java_lang_reflect_AccessibleObject) \
@@ -256,6 +258,7 @@ class java_lang_Class : AllStatic {
   static void set_component_mirror(oop java_class, oop comp_mirror);
   static void initialize_mirror_fields(Klass* k, Handle mirror, Handle protection_domain,
                                        Handle classData, TRAPS);
+  static void initialize_mirror_fields(Klass* k, Handle mirror, Handle protection_domain, TRAPS);
   static void set_mirror_module_field(Klass* K, Handle mirror, Handle module, TRAPS);
  public:
   static void allocate_fixup_lists();
@@ -1002,6 +1005,55 @@ class java_lang_invoke_DirectMethodHandle: AllStatic {
   static int member_offset()           { CHECK_INIT(_member_offset); }
 };
 
+// Interface to java.lang.invoke.DirectMethodHandle$StaticAccessor objects
+
+class java_lang_invoke_DirectMethodHandle_StaticAccessor: AllStatic {
+  friend class JavaClasses;
+
+ private:
+  static int _static_offset_offset;               // offset to static field
+
+  static void compute_offsets();
+
+ public:
+  // Accessors
+  static long      static_offset(oop dmh);
+  static void  set_static_offset(oop dmh, long value);
+
+  // Testers
+  static bool is_subclass(Klass* klass) {
+    return klass->is_subclass_of(SystemDictionary::DirectMethodHandle_StaticAccessor_klass());
+  }
+  static bool is_instance(oop obj);
+
+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;
+};
+
+// Interface to java.lang.invoke.DirectMethodHandle$Accessor objects
+
+class java_lang_invoke_DirectMethodHandle_Accessor: AllStatic {
+  friend class JavaClasses;
+
+ private:
+  static int _field_offset_offset;               // offset to field
+
+  static void compute_offsets();
+
+ public:
+  // Accessors
+  static int      field_offset(oop dmh);
+  static void set_field_offset(oop dmh, int value);
+
+  // Testers
+  static bool is_subclass(Klass* klass) {
+    return klass->is_subclass_of(SystemDictionary::DirectMethodHandle_Accessor_klass());
+  }
+  static bool is_instance(oop obj);
+
+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;
+};
+
+
 // Interface to java.lang.invoke.LambdaForm objects
 // (These are a private interface for managing adapter code generation.)
 
@@ -1053,6 +1105,7 @@ class java_lang_invoke_ResolvedMethodName : AllStatic {
 
   static Method* vmtarget(oop resolved_method);
   static void set_vmtarget(oop resolved_method, Method* method);
+  static void set_vmholder_offset(oop resolved_method, Method* method);
 
   static void set_vmholder(oop resolved_method, oop holder);
 
diff --git a/src/hotspot/share/classfile/javaClasses.inline.hpp b/src/hotspot/share/classfile/javaClasses.inline.hpp
index a654516a377..2c6ec3c5c23 100644
--- a/src/hotspot/share/classfile/javaClasses.inline.hpp
+++ b/src/hotspot/share/classfile/javaClasses.inline.hpp
@@ -239,6 +239,14 @@ inline bool java_lang_invoke_DirectMethodHandle::is_instance(oop obj) {
   return obj != NULL && is_subclass(obj->klass());
 }
 
+inline bool java_lang_invoke_DirectMethodHandle_StaticAccessor::is_instance(oop obj) {
+  return obj != NULL && is_subclass(obj->klass());
+}
+
+inline bool java_lang_invoke_DirectMethodHandle_Accessor::is_instance(oop obj) {
+  return obj != NULL && is_subclass(obj->klass());
+}
+
 inline bool java_lang_Module::is_instance(oop obj) {
   return obj != NULL && obj->klass() == SystemDictionary::Module_klass();
 }
diff --git a/src/hotspot/share/classfile/klassFactory.cpp b/src/hotspot/share/classfile/klassFactory.cpp
index 8efe2b0ca0c..65e74e7c851 100644
--- a/src/hotspot/share/classfile/klassFactory.cpp
+++ b/src/hotspot/share/classfile/klassFactory.cpp
@@ -85,6 +85,7 @@ InstanceKlass* KlassFactory::check_shared_class_file_load_hook(
                              loader_data,
                              &cl_info,
                              ClassFileParser::BROADCAST, // publicity level
+                             false,
                              CHECK_NULL);
       const ClassInstanceInfo* cl_inst_info = cl_info.class_hidden_info_ptr();
       InstanceKlass* new_ik = parser.create_instance_klass(true, // changed_by_loadhook
@@ -168,6 +169,7 @@ InstanceKlass* KlassFactory::create_from_stream(ClassFileStream* stream,
                                                 Symbol* name,
                                                 ClassLoaderData* loader_data,
                                                 const ClassLoadInfo& cl_info,
+                                                const bool pick_newest,
                                                 TRAPS) {
   assert(stream != NULL, "invariant");
   assert(loader_data != NULL, "invariant");
@@ -201,6 +203,7 @@ InstanceKlass* KlassFactory::create_from_stream(ClassFileStream* stream,
                          loader_data,
                          &cl_info,
                          ClassFileParser::BROADCAST, // publicity level
+                         pick_newest,
                          CHECK_NULL);
 
   const ClassInstanceInfo* cl_inst_info = cl_info.class_hidden_info_ptr();
diff --git a/src/hotspot/share/classfile/klassFactory.hpp b/src/hotspot/share/classfile/klassFactory.hpp
index 97d49a52294..790b1435dce 100644
--- a/src/hotspot/share/classfile/klassFactory.hpp
+++ b/src/hotspot/share/classfile/klassFactory.hpp
@@ -73,6 +73,7 @@ class KlassFactory : AllStatic {
                                            Symbol* name,
                                            ClassLoaderData* loader_data,
                                            const ClassLoadInfo& cl_info,
+                                           const bool pick_newest,
                                            TRAPS);
  public:
   static InstanceKlass* check_shared_class_file_load_hook(
diff --git a/src/hotspot/share/classfile/loaderConstraints.cpp b/src/hotspot/share/classfile/loaderConstraints.cpp
index e258f1fd1aa..fd1bd46b8bd 100644
--- a/src/hotspot/share/classfile/loaderConstraints.cpp
+++ b/src/hotspot/share/classfile/loaderConstraints.cpp
@@ -91,6 +91,19 @@ LoaderConstraintEntry** LoaderConstraintTable::find_loader_constraint(
   return pp;
 }
 
+void LoaderConstraintTable::update_after_redefinition() {
+  for (int index = 0; index < table_size(); index++) {
+    LoaderConstraintEntry** p = bucket_addr(index);
+    while(*p) {
+      LoaderConstraintEntry* probe = *p;
+      if (probe->klass() != NULL) {
+        // We swap the class with the newest version with an assumption that the hash will be the same
+        probe->set_klass((InstanceKlass*) probe->klass()->newest_version());
+      }
+      p = probe->next_addr();
+    }
+  }
+}
 
 void LoaderConstraintTable::purge_loader_constraints() {
   assert_locked_or_safepoint(SystemDictionary_lock);
@@ -446,7 +459,7 @@ void LoaderConstraintTable::verify(PlaceholderTable* placeholders) {
         if (k != NULL) {
           // We found the class in the dictionary, so we should
           // make sure that the Klass* matches what we already have.
-          guarantee(k == probe->klass(), "klass should be in dictionary");
+          guarantee(k == probe->klass()->newest_version(), "klass should be in dictionary");
         } else {
           // If we don't find the class in the dictionary, it
           // has to be in the placeholders table.
diff --git a/src/hotspot/share/classfile/loaderConstraints.hpp b/src/hotspot/share/classfile/loaderConstraints.hpp
index 0a6e4da2a5b..d0fa9df73e2 100644
--- a/src/hotspot/share/classfile/loaderConstraints.hpp
+++ b/src/hotspot/share/classfile/loaderConstraints.hpp
@@ -55,6 +55,9 @@ public:
     return (LoaderConstraintEntry**)Hashtable<InstanceKlass*, mtClass>::bucket_addr(i);
   }
 
+  // (DCEVM) update all klasses with newest version
+  void update_after_redefinition();
+
   // Check class loader constraints
   bool add_entry(Symbol* name, InstanceKlass* klass1, Handle loader1,
                                     InstanceKlass* klass2, Handle loader2);
diff --git a/src/hotspot/share/classfile/systemDictionary.cpp b/src/hotspot/share/classfile/systemDictionary.cpp
index 3a460f95b59..e6b25e90f18 100644
--- a/src/hotspot/share/classfile/systemDictionary.cpp
+++ b/src/hotspot/share/classfile/systemDictionary.cpp
@@ -265,6 +265,7 @@ Klass* SystemDictionary::resolve_or_fail(Symbol* class_name, Handle class_loader
     // can return a null klass
     klass = handle_resolution_exception(class_name, throw_error, klass, THREAD);
   }
+  assert(klass == NULL || klass->new_version() == NULL || klass->newest_version()->is_redefining(), "must be");
   return klass;
 }
 
@@ -966,6 +967,7 @@ InstanceKlass* SystemDictionary::resolve_instance_class_or_null(Symbol* name,
     ClassLoaderData* loader_data = k->class_loader_data();
     MutexLocker mu(THREAD, SystemDictionary_lock);
     InstanceKlass* kk = find_class(name, loader_data);
+    // FIXME: (kk == k() && !k->is_redefining()) || (k->is_redefining() && kk == k->old_version())
     assert(kk == k, "should be present in dictionary");
   }
 #endif
@@ -1091,6 +1093,7 @@ InstanceKlass* SystemDictionary::parse_stream(Symbol* class_name,
                                                       class_name,
                                                       loader_data,
                                                       cl_info,
+                                                      false, // pick_newest
                                                       CHECK_NULL);
 
   if ((cl_info.is_hidden() || is_unsafe_anon_class) && k != NULL) {
@@ -1145,10 +1148,13 @@ InstanceKlass* SystemDictionary::resolve_from_stream(Symbol* class_name,
                                                      Handle class_loader,
                                                      Handle protection_domain,
                                                      ClassFileStream* st,
+                                                     InstanceKlass* old_klass,
                                                      TRAPS) {
 
   HandleMark hm(THREAD);
 
+  bool is_redefining = (old_klass != NULL);
+
   // Classloaders that support parallelism, e.g. bootstrap classloader,
   // do not acquire lock here
   bool DoObjectLock = true;
@@ -1172,6 +1178,7 @@ InstanceKlass* SystemDictionary::resolve_from_stream(Symbol* class_name,
  InstanceKlass* k = NULL;
 
 #if INCLUDE_CDS
+  // FIXME: what to do during redefinition?
   if (!DumpSharedSpaces) {
     k = SystemDictionaryShared::lookup_from_stream(class_name,
                                                    class_loader,
@@ -1186,7 +1193,12 @@ InstanceKlass* SystemDictionary::resolve_from_stream(Symbol* class_name,
       return NULL;
     }
     ClassLoadInfo cl_info(protection_domain);
-    k = KlassFactory::create_from_stream(st, class_name, loader_data, cl_info, CHECK_NULL);
+    k = KlassFactory::create_from_stream(st, class_name, loader_data, cl_info, is_redefining, CHECK_NULL);
+  }
+
+  if (is_redefining && k != NULL) {
+    k->set_redefining(true);
+    k->set_old_version(old_klass);
   }
 
   assert(k != NULL, "no klass created");
@@ -1196,7 +1208,7 @@ InstanceKlass* SystemDictionary::resolve_from_stream(Symbol* class_name,
   // Add class just loaded
   // If a class loader supports parallel classloading handle parallel define requests
   // find_or_define_instance_class may return a different InstanceKlass
-  if (is_parallelCapable(class_loader)) {
+  if (is_parallelCapable(class_loader) && !is_redefining) {
     InstanceKlass* defined_k = find_or_define_instance_class(h_name, class_loader, k, THREAD);
     if (!HAS_PENDING_EXCEPTION && defined_k != k) {
       // If a parallel capable class loader already defined this class, register 'k' for cleanup.
@@ -1205,7 +1217,7 @@ InstanceKlass* SystemDictionary::resolve_from_stream(Symbol* class_name,
       k = defined_k;
     }
   } else {
-    define_instance_class(k, THREAD);
+    define_instance_class(k, old_klass, THREAD);
   }
 
   // If defining the class throws an exception register 'k' for cleanup.
@@ -1220,7 +1232,7 @@ InstanceKlass* SystemDictionary::resolve_from_stream(Symbol* class_name,
     MutexLocker mu(THREAD, SystemDictionary_lock);
 
     Klass* check = find_class(h_name, k->class_loader_data());
-    assert(check == k, "should be present in the dictionary");
+    assert((check == k && !k->is_redefining()) || (k->is_redefining() && check == k->old_version()), "should be present in the dictionary");
   } );
 
   return k;
@@ -1701,11 +1713,12 @@ static void post_class_define_event(InstanceKlass* k, const ClassLoaderData* def
   }
 }
 
-void SystemDictionary::define_instance_class(InstanceKlass* k, TRAPS) {
+void SystemDictionary::define_instance_class(InstanceKlass* k, InstanceKlass* old_klass, TRAPS) {
 
   HandleMark hm(THREAD);
   ClassLoaderData* loader_data = k->class_loader_data();
   Handle class_loader_h(THREAD, loader_data->class_loader());
+  bool is_redefining = (old_klass != NULL);
 
  // for bootstrap and other parallel classloaders don't acquire lock,
  // use placeholder token
@@ -1730,7 +1743,11 @@ void SystemDictionary::define_instance_class(InstanceKlass* k, TRAPS) {
   Symbol*  name_h = k->name();
   Dictionary* dictionary = loader_data->dictionary();
   unsigned int d_hash = dictionary->compute_hash(name_h);
-  check_constraints(d_hash, k, class_loader_h, true, CHECK);
+  if (is_redefining) {
+    bool ok = dictionary->update_klass(d_hash, name_h, loader_data, k, old_klass);
+    assert (ok, "must have found old class and updated!");
+  }
+  check_constraints(d_hash, k, class_loader_h, !is_redefining, CHECK);
 
   // Register class just loaded with class loader (placed in ArrayList)
   // Note we do this before updating the dictionary, as this can
@@ -1764,7 +1781,7 @@ void SystemDictionary::define_instance_class(InstanceKlass* k, TRAPS) {
   k->eager_initialize(THREAD);
 
   // notify jvmti
-  if (JvmtiExport::should_post_class_load()) {
+  if (!is_redefining && JvmtiExport::should_post_class_load()) {
       assert(THREAD->is_Java_thread(), "thread->is_Java_thread()");
       JvmtiExport::post_class_load((JavaThread *) THREAD, k);
 
@@ -1842,7 +1859,7 @@ InstanceKlass* SystemDictionary::find_or_define_instance_class(Symbol* class_nam
     }
   }
 
-  define_instance_class(k, THREAD);
+  define_instance_class(k, NULL, THREAD);
 
   Handle linkage_exception = Handle(); // null handle
 
@@ -1972,6 +1989,18 @@ void SystemDictionary::add_to_hierarchy(InstanceKlass* k, TRAPS) {
   }
 }
 
+// Enhanced class redefinition
+void SystemDictionary::remove_from_hierarchy(InstanceKlass* k) {
+    assert(k != NULL, "just checking");
+
+  // remove receiver from sibling list
+  k->remove_from_sibling_list();
+}
+
+void SystemDictionary::update_constraints_after_redefinition() {
+  constraints()->update_after_redefinition();
+}
+
 // ----------------------------------------------------------------------------
 // GC support
 
@@ -2251,7 +2280,7 @@ void SystemDictionary::check_constraints(unsigned int d_hash,
       // also hold array classes.
 
       assert(check->is_instance_klass(), "noninstance in systemdictionary");
-      if ((defining == true) || (k != check)) {
+      if ((defining == true) || ((k != check) && k->old_version() != check)) {
         throwException = true;
         ss.print("loader %s", loader_data->loader_name_and_id());
         ss.print(" attempted duplicate %s definition for %s. (%s)",
diff --git a/src/hotspot/share/classfile/systemDictionary.hpp b/src/hotspot/share/classfile/systemDictionary.hpp
index 7fd274b8d77..4547449dbec 100644
--- a/src/hotspot/share/classfile/systemDictionary.hpp
+++ b/src/hotspot/share/classfile/systemDictionary.hpp
@@ -209,6 +209,8 @@ class EventClassLoad;
                                                                                                                 \
   /* support for dynamic typing; it's OK if these are NULL in earlier JDKs */                                   \
   do_klass(DirectMethodHandle_klass,                    java_lang_invoke_DirectMethodHandle                   ) \
+  do_klass(DirectMethodHandle_StaticAccessor_klass,     java_lang_invoke_DirectMethodHandle_StaticAccessor    ) \
+  do_klass(DirectMethodHandle_Accessor_klass,           java_lang_invoke_DirectMethodHandle_Accessor          ) \
   do_klass(MethodHandle_klass,                          java_lang_invoke_MethodHandle                         ) \
   do_klass(VarHandle_klass,                             java_lang_invoke_VarHandle                            ) \
   do_klass(MemberName_klass,                            java_lang_invoke_MemberName                           ) \
@@ -334,6 +336,7 @@ public:
                                             Handle class_loader,
                                             Handle protection_domain,
                                             ClassFileStream* st,
+                                            InstanceKlass* old_klass,
                                             TRAPS);
 
   // Lookup an already loaded class. If not found NULL is returned.
@@ -451,6 +454,10 @@ public:
   static bool is_well_known_klass(Symbol* class_name);
 #endif
 
+  // Enhanced class redefinition
+  static void remove_from_hierarchy(InstanceKlass* k);
+  static void update_constraints_after_redefinition();
+
 protected:
   // Returns the class loader data to be used when looking up/updating the
   // system dictionary.
@@ -622,7 +629,7 @@ protected:
   // after waiting, but before reentering SystemDictionary_lock
   // to preserve lock order semantics.
   static void double_lock_wait(Handle lockObject, TRAPS);
-  static void define_instance_class(InstanceKlass* k, TRAPS);
+  static void define_instance_class(InstanceKlass* k, InstanceKlass* old_klass, TRAPS);
   static InstanceKlass* find_or_define_instance_class(Symbol* class_name,
                                                 Handle class_loader,
                                                 InstanceKlass* k, TRAPS);
diff --git a/src/hotspot/share/classfile/systemDictionaryShared.cpp b/src/hotspot/share/classfile/systemDictionaryShared.cpp
index 96c53e059a9..91dd2f2701a 100644
--- a/src/hotspot/share/classfile/systemDictionaryShared.cpp
+++ b/src/hotspot/share/classfile/systemDictionaryShared.cpp
@@ -1079,7 +1079,7 @@ InstanceKlass* SystemDictionaryShared::find_or_load_shared_class(
 
       k = load_shared_class_for_builtin_loader(name, class_loader, THREAD);
       if (k != NULL) {
-        define_instance_class(k, CHECK_NULL);
+        define_instance_class(k, NULL, CHECK_NULL);
       }
     }
   }
diff --git a/src/hotspot/share/classfile/verifier.cpp b/src/hotspot/share/classfile/verifier.cpp
index 9711662698b..4f9c750d549 100644
--- a/src/hotspot/share/classfile/verifier.cpp
+++ b/src/hotspot/share/classfile/verifier.cpp
@@ -259,7 +259,7 @@ bool Verifier::is_eligible_for_verification(InstanceKlass* klass, bool should_ve
   Symbol* name = klass->name();
   Klass* refl_magic_klass = SystemDictionary::reflect_MagicAccessorImpl_klass();
 
-  bool is_reflect = refl_magic_klass != NULL && klass->is_subtype_of(refl_magic_klass);
+  bool is_reflect = refl_magic_klass != NULL && (klass->is_subtype_of(refl_magic_klass) || klass->is_subtype_of(refl_magic_klass->newest_version()));
 
   return (should_verify_for(klass->class_loader(), should_verify_class) &&
     // return if the class is a bootstrapping class
diff --git a/src/hotspot/share/classfile/verifier.hpp b/src/hotspot/share/classfile/verifier.hpp
index e629b4f5623..eedb57e5c9d 100644
--- a/src/hotspot/share/classfile/verifier.hpp
+++ b/src/hotspot/share/classfile/verifier.hpp
@@ -374,6 +374,7 @@ class ClassVerifier : public StackObj {
 
   VerificationType object_type() const;
 
+  InstanceKlass*      _klass_to_verify;
   InstanceKlass*      _klass;  // the class being verified
   methodHandle        _method; // current method being verified
   VerificationType    _this_type; // the verification type of the current class
diff --git a/src/hotspot/share/classfile/vmSymbols.hpp b/src/hotspot/share/classfile/vmSymbols.hpp
index 28cb9bae2e2..99c5ac98995 100644
--- a/src/hotspot/share/classfile/vmSymbols.hpp
+++ b/src/hotspot/share/classfile/vmSymbols.hpp
@@ -292,6 +292,8 @@
   template(java_lang_invoke_CallSite,                 "java/lang/invoke/CallSite")                \
   template(java_lang_invoke_ConstantCallSite,         "java/lang/invoke/ConstantCallSite")        \
   template(java_lang_invoke_DirectMethodHandle,       "java/lang/invoke/DirectMethodHandle")      \
+  template(java_lang_invoke_DirectMethodHandle_StaticAccessor, "java/lang/invoke/DirectMethodHandle$StaticAccessor") \
+  template(java_lang_invoke_DirectMethodHandle_Accessor, "java/lang/invoke/DirectMethodHandle$Accessor") \
   template(java_lang_invoke_MutableCallSite,          "java/lang/invoke/MutableCallSite")         \
   template(java_lang_invoke_VolatileCallSite,         "java/lang/invoke/VolatileCallSite")        \
   template(java_lang_invoke_MethodHandle,             "java/lang/invoke/MethodHandle")            \
@@ -459,6 +461,12 @@
   template(big_endian_name,                           "BIG_ENDIAN")                               \
   template(use_unaligned_access_name,                 "UNALIGNED_ACCESS")                         \
   template(data_cache_line_flush_size_name,           "DATA_CACHE_LINE_FLUSH_SIZE")               \
+  template(static_offset_name,                        "staticOffset")                             \
+  template(static_base_name,                          "staticBase")                               \
+  template(field_offset_name,                         "fieldOffset")                              \
+  template(field_type_name,                           "fieldType")                                \
+                                                                                                  \
+   /* name symbols needed by intrinsics */                                                         \
                                                                                                   \
   /* name symbols needed by intrinsics */                                                         \
   VM_INTRINSICS_DO(VM_INTRINSIC_IGNORE, VM_SYMBOL_IGNORE, template, VM_SYMBOL_IGNORE, VM_ALIAS_IGNORE) \
diff --git a/src/hotspot/share/gc/cms/compactibleFreeListSpace.cpp b/src/hotspot/share/gc/cms/compactibleFreeListSpace.cpp
new file mode 100644
index 00000000000..a93f764f1b9
--- /dev/null
+++ b/src/hotspot/share/gc/cms/compactibleFreeListSpace.cpp
@@ -0,0 +1,3148 @@
+/*
+ * Copyright (c) 2001, 2018, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include "precompiled.hpp"
+#include "gc/cms/cmsHeap.hpp"
+#include "gc/cms/cmsLockVerifier.hpp"
+#include "gc/cms/compactibleFreeListSpace.hpp"
+#include "gc/cms/concurrentMarkSweepGeneration.inline.hpp"
+#include "gc/cms/concurrentMarkSweepThread.hpp"
+#include "gc/shared/blockOffsetTable.inline.hpp"
+#include "gc/shared/collectedHeap.inline.hpp"
+#include "gc/shared/genOopClosures.inline.hpp"
+#include "gc/shared/space.inline.hpp"
+#include "gc/shared/spaceDecorator.hpp"
+#include "logging/log.hpp"
+#include "logging/logStream.hpp"
+#include "memory/allocation.inline.hpp"
+#include "memory/binaryTreeDictionary.inline.hpp"
+#include "memory/iterator.inline.hpp"
+#include "memory/resourceArea.hpp"
+#include "memory/universe.hpp"
+#include "oops/access.inline.hpp"
+#include "oops/compressedOops.inline.hpp"
+#include "oops/oop.inline.hpp"
+#include "runtime/globals.hpp"
+#include "runtime/handles.inline.hpp"
+#include "runtime/init.hpp"
+#include "runtime/java.hpp"
+#include "runtime/orderAccess.hpp"
+#include "runtime/vmThread.hpp"
+#include "utilities/align.hpp"
+#include "utilities/copy.hpp"
+
+// Specialize for AdaptiveFreeList which tries to avoid
+// splitting a chunk of a size that is under populated in favor of
+// an over populated size.  The general get_better_list() just returns
+// the current list.
+template <>
+TreeList<FreeChunk, AdaptiveFreeList<FreeChunk> >*
+TreeList<FreeChunk, AdaptiveFreeList<FreeChunk> >::get_better_list(
+  BinaryTreeDictionary<FreeChunk, ::AdaptiveFreeList<FreeChunk> >* dictionary) {
+  // A candidate chunk has been found.  If it is already under
+  // populated, get a chunk associated with the hint for this
+  // chunk.
+
+  TreeList<FreeChunk, ::AdaptiveFreeList<FreeChunk> >* curTL = this;
+  if (curTL->surplus() <= 0) {
+    /* Use the hint to find a size with a surplus, and reset the hint. */
+    TreeList<FreeChunk, ::AdaptiveFreeList<FreeChunk> >* hintTL = this;
+    while (hintTL->hint() != 0) {
+      assert(hintTL->hint() > hintTL->size(),
+        "hint points in the wrong direction");
+      hintTL = dictionary->find_list(hintTL->hint());
+      assert(curTL != hintTL, "Infinite loop");
+      if (hintTL == NULL ||
+          hintTL == curTL /* Should not happen but protect against it */ ) {
+        // No useful hint.  Set the hint to NULL and go on.
+        curTL->set_hint(0);
+        break;
+      }
+      assert(hintTL->size() > curTL->size(), "hint is inconsistent");
+      if (hintTL->surplus() > 0) {
+        // The hint led to a list that has a surplus.  Use it.
+        // Set the hint for the candidate to an overpopulated
+        // size.
+        curTL->set_hint(hintTL->size());
+        // Change the candidate.
+        curTL = hintTL;
+        break;
+      }
+    }
+  }
+  return curTL;
+}
+
+void AFLBinaryTreeDictionary::dict_census_update(size_t size, bool split, bool birth) {
+  TreeList<FreeChunk, AdaptiveFreeList<FreeChunk> >* nd = find_list(size);
+  if (nd) {
+    if (split) {
+      if (birth) {
+        nd->increment_split_births();
+        nd->increment_surplus();
+      }  else {
+        nd->increment_split_deaths();
+        nd->decrement_surplus();
+      }
+    } else {
+      if (birth) {
+        nd->increment_coal_births();
+        nd->increment_surplus();
+      } else {
+        nd->increment_coal_deaths();
+        nd->decrement_surplus();
+      }
+    }
+  }
+  // A list for this size may not be found (nd == 0) if
+  //   This is a death where the appropriate list is now
+  //     empty and has been removed from the list.
+  //   This is a birth associated with a LinAB.  The chunk
+  //     for the LinAB is not in the dictionary.
+}
+
+bool AFLBinaryTreeDictionary::coal_dict_over_populated(size_t size) {
+  if (FLSAlwaysCoalesceLarge) return true;
+
+  TreeList<FreeChunk, AdaptiveFreeList<FreeChunk> >* list_of_size = find_list(size);
+  // None of requested size implies overpopulated.
+  return list_of_size == NULL || list_of_size->coal_desired() <= 0 ||
+         list_of_size->count() > list_of_size->coal_desired();
+}
+
+// For each list in the tree, calculate the desired, desired
+// coalesce, count before sweep, and surplus before sweep.
+class BeginSweepClosure : public AscendTreeCensusClosure<FreeChunk, AdaptiveFreeList<FreeChunk> > {
+  double _percentage;
+  float _inter_sweep_current;
+  float _inter_sweep_estimate;
+  float _intra_sweep_estimate;
+
+ public:
+  BeginSweepClosure(double p, float inter_sweep_current,
+                              float inter_sweep_estimate,
+                              float intra_sweep_estimate) :
+   _percentage(p),
+   _inter_sweep_current(inter_sweep_current),
+   _inter_sweep_estimate(inter_sweep_estimate),
+   _intra_sweep_estimate(intra_sweep_estimate) { }
+
+  void do_list(AdaptiveFreeList<FreeChunk>* fl) {
+    double coalSurplusPercent = _percentage;
+    fl->compute_desired(_inter_sweep_current, _inter_sweep_estimate, _intra_sweep_estimate);
+    fl->set_coal_desired((ssize_t)((double)fl->desired() * coalSurplusPercent));
+    fl->set_before_sweep(fl->count());
+    fl->set_bfr_surp(fl->surplus());
+  }
+};
+
+void AFLBinaryTreeDictionary::begin_sweep_dict_census(double coalSurplusPercent,
+  float inter_sweep_current, float inter_sweep_estimate, float intra_sweep_estimate) {
+  BeginSweepClosure bsc(coalSurplusPercent, inter_sweep_current,
+                        inter_sweep_estimate,
+                        intra_sweep_estimate);
+  bsc.do_tree(root());
+}
+
+// Calculate surpluses for the lists in the tree.
+class setTreeSurplusClosure : public AscendTreeCensusClosure<FreeChunk, AdaptiveFreeList<FreeChunk> > {
+  double percentage;
+ public:
+  setTreeSurplusClosure(double v) { percentage = v; }
+
+  void do_list(AdaptiveFreeList<FreeChunk>* fl) {
+    double splitSurplusPercent = percentage;
+    fl->set_surplus(fl->count() -
+                   (ssize_t)((double)fl->desired() * splitSurplusPercent));
+  }
+};
+
+void AFLBinaryTreeDictionary::set_tree_surplus(double splitSurplusPercent) {
+  setTreeSurplusClosure sts(splitSurplusPercent);
+  sts.do_tree(root());
+}
+
+// Set hints for the lists in the tree.
+class setTreeHintsClosure : public DescendTreeCensusClosure<FreeChunk, AdaptiveFreeList<FreeChunk> > {
+  size_t hint;
+ public:
+  setTreeHintsClosure(size_t v) { hint = v; }
+
+  void do_list(AdaptiveFreeList<FreeChunk>* fl) {
+    fl->set_hint(hint);
+    assert(fl->hint() == 0 || fl->hint() > fl->size(),
+      "Current hint is inconsistent");
+    if (fl->surplus() > 0) {
+      hint = fl->size();
+    }
+  }
+};
+
+void AFLBinaryTreeDictionary::set_tree_hints(void) {
+  setTreeHintsClosure sth(0);
+  sth.do_tree(root());
+}
+
+// Save count before previous sweep and splits and coalesces.
+class clearTreeCensusClosure : public AscendTreeCensusClosure<FreeChunk, AdaptiveFreeList<FreeChunk> > {
+  void do_list(AdaptiveFreeList<FreeChunk>* fl) {
+    fl->set_prev_sweep(fl->count());
+    fl->set_coal_births(0);
+    fl->set_coal_deaths(0);
+    fl->set_split_births(0);
+    fl->set_split_deaths(0);
+  }
+};
+
+void AFLBinaryTreeDictionary::clear_tree_census(void) {
+  clearTreeCensusClosure ctc;
+  ctc.do_tree(root());
+}
+
+// Do reporting and post sweep clean up.
+void AFLBinaryTreeDictionary::end_sweep_dict_census(double splitSurplusPercent) {
+  // Does walking the tree 3 times hurt?
+  set_tree_surplus(splitSurplusPercent);
+  set_tree_hints();
+  LogTarget(Trace, gc, freelist, stats) log;
+  if (log.is_enabled()) {
+    LogStream out(log);
+    report_statistics(&out);
+  }
+  clear_tree_census();
+}
+
+// Print census information - counts, births, deaths, etc.
+// for each list in the tree.  Also print some summary
+// information.
+class PrintTreeCensusClosure : public AscendTreeCensusClosure<FreeChunk, AdaptiveFreeList<FreeChunk> > {
+  int _print_line;
+  size_t _total_free;
+  AdaptiveFreeList<FreeChunk> _total;
+
+ public:
+  PrintTreeCensusClosure() {
+    _print_line = 0;
+    _total_free = 0;
+  }
+  AdaptiveFreeList<FreeChunk>* total() { return &_total; }
+  size_t total_free() { return _total_free; }
+
+  void do_list(AdaptiveFreeList<FreeChunk>* fl) {
+    LogStreamHandle(Debug, gc, freelist, census) out;
+
+    if (++_print_line >= 40) {
+      AdaptiveFreeList<FreeChunk>::print_labels_on(&out, "size");
+      _print_line = 0;
+    }
+    fl->print_on(&out);
+    _total_free +=           fl->count()             * fl->size()        ;
+    total()->set_count(      total()->count()        + fl->count()      );
+    total()->set_bfr_surp(   total()->bfr_surp()     + fl->bfr_surp()    );
+    total()->set_surplus(    total()->split_deaths() + fl->surplus()    );
+    total()->set_desired(    total()->desired()      + fl->desired()    );
+    total()->set_prev_sweep(  total()->prev_sweep()   + fl->prev_sweep()  );
+    total()->set_before_sweep(total()->before_sweep() + fl->before_sweep());
+    total()->set_coal_births( total()->coal_births()  + fl->coal_births() );
+    total()->set_coal_deaths( total()->coal_deaths()  + fl->coal_deaths() );
+    total()->set_split_births(total()->split_births() + fl->split_births());
+    total()->set_split_deaths(total()->split_deaths() + fl->split_deaths());
+  }
+};
+
+void AFLBinaryTreeDictionary::print_dict_census(outputStream* st) const {
+
+  st->print_cr("BinaryTree");
+  AdaptiveFreeList<FreeChunk>::print_labels_on(st, "size");
+  PrintTreeCensusClosure ptc;
+  ptc.do_tree(root());
+
+  AdaptiveFreeList<FreeChunk>* total = ptc.total();
+  AdaptiveFreeList<FreeChunk>::print_labels_on(st, " ");
+  total->print_on(st, "TOTAL\t");
+  st->print_cr("total_free(words): " SIZE_FORMAT_W(16) " growth: %8.5f  deficit: %8.5f",
+               ptc.total_free(),
+               (double)(total->split_births() + total->coal_births()
+                      - total->split_deaths() - total->coal_deaths())
+               /(total->prev_sweep() != 0 ? (double)total->prev_sweep() : 1.0),
+              (double)(total->desired() - total->count())
+              /(total->desired() != 0 ? (double)total->desired() : 1.0));
+}
+
+/////////////////////////////////////////////////////////////////////////
+//// CompactibleFreeListSpace
+/////////////////////////////////////////////////////////////////////////
+
+// highest ranked  free list lock rank
+int CompactibleFreeListSpace::_lockRank = Mutex::leaf + 3;
+
+// Defaults are 0 so things will break badly if incorrectly initialized.
+size_t CompactibleFreeListSpace::IndexSetStart  = 0;
+size_t CompactibleFreeListSpace::IndexSetStride = 0;
+size_t CompactibleFreeListSpace::_min_chunk_size_in_bytes = 0;
+
+size_t MinChunkSize = 0;
+
+void CompactibleFreeListSpace::set_cms_values() {
+  // Set CMS global values
+  assert(MinChunkSize == 0, "already set");
+
+  // MinChunkSize should be a multiple of MinObjAlignment and be large enough
+  // for chunks to contain a FreeChunk.
+  _min_chunk_size_in_bytes = align_up(sizeof(FreeChunk), MinObjAlignmentInBytes);
+  MinChunkSize = _min_chunk_size_in_bytes / BytesPerWord;
+
+  assert(IndexSetStart == 0 && IndexSetStride == 0, "already set");
+  IndexSetStart  = MinChunkSize;
+  IndexSetStride = MinObjAlignment;
+}
+
+// Constructor
+CompactibleFreeListSpace::CompactibleFreeListSpace(BlockOffsetSharedArray* bs, MemRegion mr) :
+  _bt(bs, mr),
+  // free list locks are in the range of values taken by _lockRank
+  // This range currently is [_leaf+2, _leaf+3]
+  // Note: this requires that CFLspace c'tors
+  // are called serially in the order in which the locks are
+  // are acquired in the program text. This is true today.
+  _freelistLock(_lockRank--, "CompactibleFreeListSpace._lock", true,
+                Monitor::_safepoint_check_sometimes),
+  _parDictionaryAllocLock(Mutex::leaf - 1,  // == rank(ExpandHeap_lock) - 1
+                          "CompactibleFreeListSpace._dict_par_lock", true,
+                          Monitor::_safepoint_check_never),
+  _rescan_task_size(CardTable::card_size_in_words * BitsPerWord *
+                    CMSRescanMultiple),
+  _marking_task_size(CardTable::card_size_in_words * BitsPerWord *
+                    CMSConcMarkMultiple),
+  _collector(NULL),
+  _preconsumptionDirtyCardClosure(NULL)
+{
+  assert(sizeof(FreeChunk) / BytesPerWord <= MinChunkSize,
+         "FreeChunk is larger than expected");
+  _bt.set_space(this);
+  initialize(mr, SpaceDecorator::Clear, SpaceDecorator::Mangle);
+
+  _dictionary = new AFLBinaryTreeDictionary(mr);
+
+  assert(_dictionary != NULL, "CMS dictionary initialization");
+  // The indexed free lists are initially all empty and are lazily
+  // filled in on demand. Initialize the array elements to NULL.
+  initializeIndexedFreeListArray();
+
+  _smallLinearAllocBlock.set(0, 0, 1024*SmallForLinearAlloc,
+                             SmallForLinearAlloc);
+
+  // CMSIndexedFreeListReplenish should be at least 1
+  CMSIndexedFreeListReplenish = MAX2((uintx)1, CMSIndexedFreeListReplenish);
+  _promoInfo.setSpace(this);
+  if (UseCMSBestFit) {
+    _fitStrategy = FreeBlockBestFitFirst;
+  } else {
+    _fitStrategy = FreeBlockStrategyNone;
+  }
+  check_free_list_consistency();
+
+  // Initialize locks for parallel case.
+  for (size_t i = IndexSetStart; i < IndexSetSize; i += IndexSetStride) {
+    _indexedFreeListParLocks[i] = new Mutex(Mutex::leaf - 1, // == ExpandHeap_lock - 1
+                                            "a freelist par lock", true, Mutex::_safepoint_check_sometimes);
+    DEBUG_ONLY(
+      _indexedFreeList[i].set_protecting_lock(_indexedFreeListParLocks[i]);
+    )
+  }
+  _dictionary->set_par_lock(&_parDictionaryAllocLock);
+
+  _used_stable = 0;
+}
+
+HeapWord* CompactibleFreeListSpace::forward_compact_top(size_t size,
+                                    CompactPoint* cp, HeapWord* compact_top) {
+  ShouldNotReachHere();
+  return NULL;
+}
+
+// Like CompactibleSpace forward() but always calls cross_threshold() to
+// update the block offset table.  Removed initialize_threshold call because
+// CFLS does not use a block offset array for contiguous spaces.
+HeapWord* CompactibleFreeListSpace::forward(oop q, size_t size,
+                                    CompactPoint* cp, HeapWord* compact_top) {
+  // q is alive
+  // First check if we should switch compaction space
+  assert(this == cp->space, "'this' should be current compaction space.");
+  size_t compaction_max_size = pointer_delta(end(), compact_top);
+  assert(adjustObjectSize(size) == cp->space->adjust_object_size_v(size),
+    "virtual adjustObjectSize_v() method is not correct");
+  size_t adjusted_size = adjustObjectSize(size);
+  assert(compaction_max_size >= MinChunkSize || compaction_max_size == 0,
+         "no small fragments allowed");
+  assert(minimum_free_block_size() == MinChunkSize,
+         "for de-virtualized reference below");
+  // Can't leave a nonzero size, residual fragment smaller than MinChunkSize
+  if (adjusted_size + MinChunkSize > compaction_max_size &&
+      adjusted_size != compaction_max_size) {
+    do {
+      // switch to next compaction space
+      cp->space->set_compaction_top(compact_top);
+      cp->space = cp->space->next_compaction_space();
+      if (cp->space == NULL) {
+        cp->gen = CMSHeap::heap()->young_gen();
+        assert(cp->gen != NULL, "compaction must succeed");
+        cp->space = cp->gen->first_compaction_space();
+        assert(cp->space != NULL, "generation must have a first compaction space");
+      }
+      compact_top = cp->space->bottom();
+      cp->space->set_compaction_top(compact_top);
+      // The correct adjusted_size may not be the same as that for this method
+      // (i.e., cp->space may no longer be "this" so adjust the size again.
+      // Use the virtual method which is not used above to save the virtual
+      // dispatch.
+      adjusted_size = cp->space->adjust_object_size_v(size);
+      compaction_max_size = pointer_delta(cp->space->end(), compact_top);
+      assert(cp->space->minimum_free_block_size() == 0, "just checking");
+    } while (adjusted_size > compaction_max_size);
+  }
+
+  // store the forwarding pointer into the mark word
+  if ((HeapWord*)q != compact_top) {
+    q->forward_to(oop(compact_top));
+    assert(q->is_gc_marked(), "encoding the pointer should preserve the mark");
+  } else {
+    // if the object isn't moving we can just set the mark to the default
+    // mark and handle it specially later on.
+    q->init_mark_raw();
+    assert(q->forwardee() == NULL, "should be forwarded to NULL");
+  }
+
+  compact_top += adjusted_size;
+
+  // we need to update the offset table so that the beginnings of objects can be
+  // found during scavenge.  Note that we are updating the offset table based on
+  // where the object will be once the compaction phase finishes.
+
+  // Always call cross_threshold().  A contiguous space can only call it when
+  // the compaction_top exceeds the current threshold but not for an
+  // non-contiguous space.
+  cp->threshold =
+    cp->space->cross_threshold(compact_top - adjusted_size, compact_top);
+  return compact_top;
+}
+
+// A modified copy of OffsetTableContigSpace::cross_threshold() with _offsets -> _bt
+// and use of single_block instead of alloc_block.  The name here is not really
+// appropriate - maybe a more general name could be invented for both the
+// contiguous and noncontiguous spaces.
+
+HeapWord* CompactibleFreeListSpace::cross_threshold(HeapWord* start, HeapWord* the_end) {
+  _bt.single_block(start, the_end);
+  return end();
+}
+
+// Initialize them to NULL.
+void CompactibleFreeListSpace::initializeIndexedFreeListArray() {
+  for (size_t i = 0; i < IndexSetSize; i++) {
+    // Note that on platforms where objects are double word aligned,
+    // the odd array elements are not used.  It is convenient, however,
+    // to map directly from the object size to the array element.
+    _indexedFreeList[i].reset(IndexSetSize);
+    _indexedFreeList[i].set_size(i);
+    assert(_indexedFreeList[i].count() == 0, "reset check failed");
+    assert(_indexedFreeList[i].head() == NULL, "reset check failed");
+    assert(_indexedFreeList[i].tail() == NULL, "reset check failed");
+    assert(_indexedFreeList[i].hint() == IndexSetSize, "reset check failed");
+  }
+}
+
+size_t CompactibleFreeListSpace::obj_size(const HeapWord* addr) const {
+  return adjustObjectSize(oop(addr)->size());
+}
+
+void CompactibleFreeListSpace::resetIndexedFreeListArray() {
+  for (size_t i = 1; i < IndexSetSize; i++) {
+    assert(_indexedFreeList[i].size() == (size_t) i,
+      "Indexed free list sizes are incorrect");
+    _indexedFreeList[i].reset(IndexSetSize);
+    assert(_indexedFreeList[i].count() == 0, "reset check failed");
+    assert(_indexedFreeList[i].head() == NULL, "reset check failed");
+    assert(_indexedFreeList[i].tail() == NULL, "reset check failed");
+    assert(_indexedFreeList[i].hint() == IndexSetSize, "reset check failed");
+  }
+}
+
+void CompactibleFreeListSpace::reset(MemRegion mr) {
+  resetIndexedFreeListArray();
+  dictionary()->reset();
+  if (BlockOffsetArrayUseUnallocatedBlock) {
+    assert(end() == mr.end(), "We are compacting to the bottom of CMS gen");
+    // Everything's allocated until proven otherwise.
+    _bt.set_unallocated_block(end());
+  }
+  if (!mr.is_empty()) {
+    assert(mr.word_size() >= MinChunkSize, "Chunk size is too small");
+    _bt.single_block(mr.start(), mr.word_size());
+    FreeChunk* fc = (FreeChunk*) mr.start();
+    fc->set_size(mr.word_size());
+    if (mr.word_size() >= IndexSetSize ) {
+      returnChunkToDictionary(fc);
+    } else {
+      _bt.verify_not_unallocated((HeapWord*)fc, fc->size());
+      _indexedFreeList[mr.word_size()].return_chunk_at_head(fc);
+    }
+    coalBirth(mr.word_size());
+  }
+  _promoInfo.reset();
+  _smallLinearAllocBlock._ptr = NULL;
+  _smallLinearAllocBlock._word_size = 0;
+}
+
+void CompactibleFreeListSpace::reset_after_compaction() {
+  // Reset the space to the new reality - one free chunk.
+  MemRegion mr(compaction_top(), end());
+  reset(mr);
+  // Now refill the linear allocation block(s) if possible.
+  refillLinearAllocBlocksIfNeeded();
+}
+
+// Walks the entire dictionary, returning a coterminal
+// chunk, if it exists. Use with caution since it involves
+// a potentially complete walk of a potentially large tree.
+FreeChunk* CompactibleFreeListSpace::find_chunk_at_end() {
+
+  assert_lock_strong(&_freelistLock);
+
+  return dictionary()->find_chunk_ends_at(end());
+}
+
+
+#ifndef PRODUCT
+void CompactibleFreeListSpace::initializeIndexedFreeListArrayReturnedBytes() {
+  for (size_t i = IndexSetStart; i < IndexSetSize; i += IndexSetStride) {
+    _indexedFreeList[i].allocation_stats()->set_returned_bytes(0);
+  }
+}
+
+size_t CompactibleFreeListSpace::sumIndexedFreeListArrayReturnedBytes() {
+  size_t sum = 0;
+  for (size_t i = IndexSetStart; i < IndexSetSize; i += IndexSetStride) {
+    sum += _indexedFreeList[i].allocation_stats()->returned_bytes();
+  }
+  return sum;
+}
+
+size_t CompactibleFreeListSpace::totalCountInIndexedFreeLists() const {
+  size_t count = 0;
+  for (size_t i = IndexSetStart; i < IndexSetSize; i++) {
+    debug_only(
+      ssize_t total_list_count = 0;
+      for (FreeChunk* fc = _indexedFreeList[i].head(); fc != NULL;
+         fc = fc->next()) {
+        total_list_count++;
+      }
+      assert(total_list_count ==  _indexedFreeList[i].count(),
+        "Count in list is incorrect");
+    )
+    count += _indexedFreeList[i].count();
+  }
+  return count;
+}
+
+size_t CompactibleFreeListSpace::totalCount() {
+  size_t num = totalCountInIndexedFreeLists();
+  num +=  dictionary()->total_count();
+  if (_smallLinearAllocBlock._word_size != 0) {
+    num++;
+  }
+  return num;
+}
+#endif
+
+bool CompactibleFreeListSpace::is_free_block(const HeapWord* p) const {
+  FreeChunk* fc = (FreeChunk*) p;
+  return fc->is_free();
+}
+
+size_t CompactibleFreeListSpace::used() const {
+  return capacity() - free();
+}
+
+size_t CompactibleFreeListSpace::used_stable() const {
+  return _used_stable;
+}
+
+void CompactibleFreeListSpace::recalculate_used_stable() {
+  _used_stable = used();
+}
+
+size_t CompactibleFreeListSpace::free() const {
+  // "MT-safe, but not MT-precise"(TM), if you will: i.e.
+  // if you do this while the structures are in flux you
+  // may get an approximate answer only; for instance
+  // because there is concurrent allocation either
+  // directly by mutators or for promotion during a GC.
+  // It's "MT-safe", however, in the sense that you are guaranteed
+  // not to crash and burn, for instance, because of walking
+  // pointers that could disappear as you were walking them.
+  // The approximation is because the various components
+  // that are read below are not read atomically (and
+  // further the computation of totalSizeInIndexedFreeLists()
+  // is itself a non-atomic computation. The normal use of
+  // this is during a resize operation at the end of GC
+  // and at that time you are guaranteed to get the
+  // correct actual value. However, for instance, this is
+  // also read completely asynchronously by the "perf-sampler"
+  // that supports jvmstat, and you are apt to see the values
+  // flicker in such cases.
+  assert(_dictionary != NULL, "No _dictionary?");
+  return (_dictionary->total_chunk_size(DEBUG_ONLY(freelistLock())) +
+          totalSizeInIndexedFreeLists() +
+          _smallLinearAllocBlock._word_size) * HeapWordSize;
+}
+
+size_t CompactibleFreeListSpace::max_alloc_in_words() const {
+  assert(_dictionary != NULL, "No _dictionary?");
+  assert_locked();
+  size_t res = _dictionary->max_chunk_size();
+  res = MAX2(res, MIN2(_smallLinearAllocBlock._word_size,
+                       (size_t) SmallForLinearAlloc - 1));
+  // XXX the following could potentially be pretty slow;
+  // should one, pessimistically for the rare cases when res
+  // calculated above is less than IndexSetSize,
+  // just return res calculated above? My reasoning was that
+  // those cases will be so rare that the extra time spent doesn't
+  // really matter....
+  // Note: do not change the loop test i >= res + IndexSetStride
+  // to i > res below, because i is unsigned and res may be zero.
+  for (size_t i = IndexSetSize - 1; i >= res + IndexSetStride;
+       i -= IndexSetStride) {
+    if (_indexedFreeList[i].head() != NULL) {
+      assert(_indexedFreeList[i].count() != 0, "Inconsistent FreeList");
+      return i;
+    }
+  }
+  return res;
+}
+
+void LinearAllocBlock::print_on(outputStream* st) const {
+  st->print_cr(" LinearAllocBlock: ptr = " PTR_FORMAT ", word_size = " SIZE_FORMAT
+            ", refillsize = " SIZE_FORMAT ", allocation_size_limit = " SIZE_FORMAT,
+            p2i(_ptr), _word_size, _refillSize, _allocation_size_limit);
+}
+
+void CompactibleFreeListSpace::print_on(outputStream* st) const {
+  st->print_cr("COMPACTIBLE FREELIST SPACE");
+  st->print_cr(" Space:");
+  Space::print_on(st);
+
+  st->print_cr("promoInfo:");
+  _promoInfo.print_on(st);
+
+  st->print_cr("_smallLinearAllocBlock");
+  _smallLinearAllocBlock.print_on(st);
+
+  // dump_memory_block(_smallLinearAllocBlock->_ptr, 128);
+
+  st->print_cr(" _fitStrategy = %s", BOOL_TO_STR(_fitStrategy));
+}
+
+void CompactibleFreeListSpace::print_indexed_free_lists(outputStream* st)
+const {
+  reportIndexedFreeListStatistics(st);
+  st->print_cr("Layout of Indexed Freelists");
+  st->print_cr("---------------------------");
+  AdaptiveFreeList<FreeChunk>::print_labels_on(st, "size");
+  for (size_t i = IndexSetStart; i < IndexSetSize; i += IndexSetStride) {
+    _indexedFreeList[i].print_on(st);
+    for (FreeChunk* fc = _indexedFreeList[i].head(); fc != NULL; fc = fc->next()) {
+      st->print_cr("\t[" PTR_FORMAT "," PTR_FORMAT ")  %s",
+                   p2i(fc), p2i((HeapWord*)fc + i),
+                   fc->cantCoalesce() ? "\t CC" : "");
+    }
+  }
+}
+
+void CompactibleFreeListSpace::print_promo_info_blocks(outputStream* st)
+const {
+  _promoInfo.print_on(st);
+}
+
+void CompactibleFreeListSpace::print_dictionary_free_lists(outputStream* st)
+const {
+  _dictionary->report_statistics(st);
+  st->print_cr("Layout of Freelists in Tree");
+  st->print_cr("---------------------------");
+  _dictionary->print_free_lists(st);
+}
+
+class BlkPrintingClosure: public BlkClosure {
+  const CMSCollector*             _collector;
+  const CompactibleFreeListSpace* _sp;
+  const CMSBitMap*                _live_bit_map;
+  const bool                      _post_remark;
+  outputStream*                   _st;
+public:
+  BlkPrintingClosure(const CMSCollector* collector,
+                     const CompactibleFreeListSpace* sp,
+                     const CMSBitMap* live_bit_map,
+                     outputStream* st):
+    _collector(collector),
+    _sp(sp),
+    _live_bit_map(live_bit_map),
+    _post_remark(collector->abstract_state() > CMSCollector::FinalMarking),
+    _st(st) { }
+  size_t do_blk(HeapWord* addr);
+};
+
+size_t BlkPrintingClosure::do_blk(HeapWord* addr) {
+  size_t sz = _sp->block_size_no_stall(addr, _collector);
+  assert(sz != 0, "Should always be able to compute a size");
+  if (_sp->block_is_obj(addr)) {
+    const bool dead = _post_remark && !_live_bit_map->isMarked(addr);
+    _st->print_cr(PTR_FORMAT ": %s object of size " SIZE_FORMAT "%s",
+      p2i(addr),
+      dead ? "dead" : "live",
+      sz,
+      (!dead && CMSPrintObjectsInDump) ? ":" : ".");
+    if (CMSPrintObjectsInDump && !dead) {
+      oop(addr)->print_on(_st);
+      _st->print_cr("--------------------------------------");
+    }
+  } else { // free block
+    _st->print_cr(PTR_FORMAT ": free block of size " SIZE_FORMAT "%s",
+      p2i(addr), sz, CMSPrintChunksInDump ? ":" : ".");
+    if (CMSPrintChunksInDump) {
+      ((FreeChunk*)addr)->print_on(_st);
+      _st->print_cr("--------------------------------------");
+    }
+  }
+  return sz;
+}
+
+void CompactibleFreeListSpace::dump_at_safepoint_with_locks(CMSCollector* c, outputStream* st) {
+  st->print_cr("=========================");
+  st->print_cr("Block layout in CMS Heap:");
+  st->print_cr("=========================");
+  BlkPrintingClosure  bpcl(c, this, c->markBitMap(), st);
+  blk_iterate(&bpcl);
+
+  st->print_cr("=======================================");
+  st->print_cr("Order & Layout of Promotion Info Blocks");
+  st->print_cr("=======================================");
+  print_promo_info_blocks(st);
+
+  st->print_cr("===========================");
+  st->print_cr("Order of Indexed Free Lists");
+  st->print_cr("=========================");
+  print_indexed_free_lists(st);
+
+  st->print_cr("=================================");
+  st->print_cr("Order of Free Lists in Dictionary");
+  st->print_cr("=================================");
+  print_dictionary_free_lists(st);
+}
+
+
+void CompactibleFreeListSpace::reportFreeListStatistics(const char* title) const {
+  assert_lock_strong(&_freelistLock);
+  Log(gc, freelist, stats) log;
+  if (!log.is_debug()) {
+    return;
+  }
+  log.debug("%s", title);
+
+  LogStream out(log.debug());
+  _dictionary->report_statistics(&out);
+
+  if (log.is_trace()) {
+    LogStream trace_out(log.trace());
+    reportIndexedFreeListStatistics(&trace_out);
+    size_t total_size = totalSizeInIndexedFreeLists() +
+                       _dictionary->total_chunk_size(DEBUG_ONLY(freelistLock()));
+    log.trace(" free=" SIZE_FORMAT " frag=%1.4f", total_size, flsFrag());
+  }
+}
+
+void CompactibleFreeListSpace::reportIndexedFreeListStatistics(outputStream* st) const {
+  assert_lock_strong(&_freelistLock);
+  st->print_cr("Statistics for IndexedFreeLists:");
+  st->print_cr("--------------------------------");
+  size_t total_size = totalSizeInIndexedFreeLists();
+  size_t free_blocks = numFreeBlocksInIndexedFreeLists();
+  st->print_cr("Total Free Space: " SIZE_FORMAT, total_size);
+  st->print_cr("Max   Chunk Size: " SIZE_FORMAT, maxChunkSizeInIndexedFreeLists());
+  st->print_cr("Number of Blocks: " SIZE_FORMAT, free_blocks);
+  if (free_blocks != 0) {
+    st->print_cr("Av.  Block  Size: " SIZE_FORMAT, total_size/free_blocks);
+  }
+}
+
+size_t CompactibleFreeListSpace::numFreeBlocksInIndexedFreeLists() const {
+  size_t res = 0;
+  for (size_t i = IndexSetStart; i < IndexSetSize; i += IndexSetStride) {
+    debug_only(
+      ssize_t recount = 0;
+      for (FreeChunk* fc = _indexedFreeList[i].head(); fc != NULL;
+         fc = fc->next()) {
+        recount += 1;
+      }
+      assert(recount == _indexedFreeList[i].count(),
+        "Incorrect count in list");
+    )
+    res += _indexedFreeList[i].count();
+  }
+  return res;
+}
+
+size_t CompactibleFreeListSpace::maxChunkSizeInIndexedFreeLists() const {
+  for (size_t i = IndexSetSize - 1; i != 0; i -= IndexSetStride) {
+    if (_indexedFreeList[i].head() != NULL) {
+      assert(_indexedFreeList[i].count() != 0, "Inconsistent FreeList");
+      return (size_t)i;
+    }
+  }
+  return 0;
+}
+
+void CompactibleFreeListSpace::set_end(HeapWord* value) {
+  HeapWord* prevEnd = end();
+  assert(prevEnd != value, "unnecessary set_end call");
+  assert(prevEnd == NULL || !BlockOffsetArrayUseUnallocatedBlock || value >= unallocated_block(),
+        "New end is below unallocated block");
+  _end = value;
+  if (prevEnd != NULL) {
+    // Resize the underlying block offset table.
+    _bt.resize(pointer_delta(value, bottom()));
+    if (value <= prevEnd) {
+      assert(!BlockOffsetArrayUseUnallocatedBlock || value >= unallocated_block(),
+             "New end is below unallocated block");
+    } else {
+      // Now, take this new chunk and add it to the free blocks.
+      // Note that the BOT has not yet been updated for this block.
+      size_t newFcSize = pointer_delta(value, prevEnd);
+      // Add the block to the free lists, if possible coalescing it
+      // with the last free block, and update the BOT and census data.
+      addChunkToFreeListsAtEndRecordingStats(prevEnd, newFcSize);
+    }
+  }
+}
+
+class FreeListSpaceDCTOC : public FilteringDCTOC {
+  CompactibleFreeListSpace* _cfls;
+  CMSCollector* _collector;
+  bool _parallel;
+protected:
+  // Override.
+#define walk_mem_region_with_cl_DECL(ClosureType)                       \
+  virtual void walk_mem_region_with_cl(MemRegion mr,                    \
+                                       HeapWord* bottom, HeapWord* top, \
+                                       ClosureType* cl);                \
+      void walk_mem_region_with_cl_par(MemRegion mr,                    \
+                                       HeapWord* bottom, HeapWord* top, \
+                                       ClosureType* cl);                \
+    void walk_mem_region_with_cl_nopar(MemRegion mr,                    \
+                                       HeapWord* bottom, HeapWord* top, \
+                                       ClosureType* cl)
+  walk_mem_region_with_cl_DECL(OopIterateClosure);
+  walk_mem_region_with_cl_DECL(FilteringClosure);
+
+public:
+  FreeListSpaceDCTOC(CompactibleFreeListSpace* sp,
+                     CMSCollector* collector,
+                     OopIterateClosure* cl,
+                     CardTable::PrecisionStyle precision,
+                     HeapWord* boundary,
+                     bool parallel) :
+    FilteringDCTOC(sp, cl, precision, boundary),
+    _cfls(sp), _collector(collector), _parallel(parallel) {}
+};
+
+// We de-virtualize the block-related calls below, since we know that our
+// space is a CompactibleFreeListSpace.
+
+#define FreeListSpaceDCTOC__walk_mem_region_with_cl_DEFN(ClosureType)           \
+void FreeListSpaceDCTOC::walk_mem_region_with_cl(MemRegion mr,                  \
+                                                 HeapWord* bottom,              \
+                                                 HeapWord* top,                 \
+                                                 ClosureType* cl) {             \
+   if (_parallel) {                                                             \
+     walk_mem_region_with_cl_par(mr, bottom, top, cl);                          \
+   } else {                                                                     \
+     walk_mem_region_with_cl_nopar(mr, bottom, top, cl);                        \
+   }                                                                            \
+}                                                                               \
+void FreeListSpaceDCTOC::walk_mem_region_with_cl_par(MemRegion mr,              \
+                                                     HeapWord* bottom,          \
+                                                     HeapWord* top,             \
+                                                     ClosureType* cl) {         \
+  /* Skip parts that are before "mr", in case "block_start" sent us             \
+     back too far. */                                                           \
+  HeapWord* mr_start = mr.start();                                              \
+  size_t bot_size = _cfls->CompactibleFreeListSpace::block_size(bottom);        \
+  HeapWord* next = bottom + bot_size;                                           \
+  while (next < mr_start) {                                                     \
+    bottom = next;                                                              \
+    bot_size = _cfls->CompactibleFreeListSpace::block_size(bottom);             \
+    next = bottom + bot_size;                                                   \
+  }                                                                             \
+                                                                                \
+  while (bottom < top) {                                                        \
+    if (_cfls->CompactibleFreeListSpace::block_is_obj(bottom) &&                \
+        !_cfls->CompactibleFreeListSpace::obj_allocated_since_save_marks(       \
+                    oop(bottom)) &&                                             \
+        !_collector->CMSCollector::is_dead_obj(oop(bottom))) {                  \
+      size_t word_sz = oop(bottom)->oop_iterate_size(cl, mr);                   \
+      bottom += _cfls->adjustObjectSize(word_sz);                               \
+    } else {                                                                    \
+      bottom += _cfls->CompactibleFreeListSpace::block_size(bottom);            \
+    }                                                                           \
+  }                                                                             \
+}                                                                               \
+void FreeListSpaceDCTOC::walk_mem_region_with_cl_nopar(MemRegion mr,            \
+                                                       HeapWord* bottom,        \
+                                                       HeapWord* top,           \
+                                                       ClosureType* cl) {       \
+  /* Skip parts that are before "mr", in case "block_start" sent us             \
+     back too far. */                                                           \
+  HeapWord* mr_start = mr.start();                                              \
+  size_t bot_size = _cfls->CompactibleFreeListSpace::block_size_nopar(bottom);  \
+  HeapWord* next = bottom + bot_size;                                           \
+  while (next < mr_start) {                                                     \
+    bottom = next;                                                              \
+    bot_size = _cfls->CompactibleFreeListSpace::block_size_nopar(bottom);       \
+    next = bottom + bot_size;                                                   \
+  }                                                                             \
+                                                                                \
+  while (bottom < top) {                                                        \
+    if (_cfls->CompactibleFreeListSpace::block_is_obj_nopar(bottom) &&          \
+        !_cfls->CompactibleFreeListSpace::obj_allocated_since_save_marks(       \
+                    oop(bottom)) &&                                             \
+        !_collector->CMSCollector::is_dead_obj(oop(bottom))) {                  \
+      size_t word_sz = oop(bottom)->oop_iterate_size(cl, mr);                   \
+      bottom += _cfls->adjustObjectSize(word_sz);                               \
+    } else {                                                                    \
+      bottom += _cfls->CompactibleFreeListSpace::block_size_nopar(bottom);      \
+    }                                                                           \
+  }                                                                             \
+}
+
+// (There are only two of these, rather than N, because the split is due
+// only to the introduction of the FilteringClosure, a local part of the
+// impl of this abstraction.)
+FreeListSpaceDCTOC__walk_mem_region_with_cl_DEFN(OopIterateClosure)
+FreeListSpaceDCTOC__walk_mem_region_with_cl_DEFN(FilteringClosure)
+
+DirtyCardToOopClosure*
+CompactibleFreeListSpace::new_dcto_cl(OopIterateClosure* cl,
+                                      CardTable::PrecisionStyle precision,
+                                      HeapWord* boundary,
+                                      bool parallel) {
+  return new FreeListSpaceDCTOC(this, _collector, cl, precision, boundary, parallel);
+}
+
+
+// Note on locking for the space iteration functions:
+// since the collector's iteration activities are concurrent with
+// allocation activities by mutators, absent a suitable mutual exclusion
+// mechanism the iterators may go awry. For instance a block being iterated
+// may suddenly be allocated or divided up and part of it allocated and
+// so on.
+
+// Apply the given closure to each block in the space.
+void CompactibleFreeListSpace::blk_iterate_careful(BlkClosureCareful* cl) {
+  assert_lock_strong(freelistLock());
+  HeapWord *cur, *limit;
+  for (cur = bottom(), limit = end(); cur < limit;
+       cur += cl->do_blk_careful(cur));
+}
+
+// Apply the given closure to each block in the space.
+void CompactibleFreeListSpace::blk_iterate(BlkClosure* cl) {
+  assert_lock_strong(freelistLock());
+  HeapWord *cur, *limit;
+  for (cur = bottom(), limit = end(); cur < limit;
+       cur += cl->do_blk(cur));
+}
+
+// Apply the given closure to each oop in the space.
+void CompactibleFreeListSpace::oop_iterate(OopIterateClosure* cl) {
+  assert_lock_strong(freelistLock());
+  HeapWord *cur, *limit;
+  size_t curSize;
+  for (cur = bottom(), limit = end(); cur < limit;
+       cur += curSize) {
+    curSize = block_size(cur);
+    if (block_is_obj(cur)) {
+      oop(cur)->oop_iterate(cl);
+    }
+  }
+}
+
+// NOTE: In the following methods, in order to safely be able to
+// apply the closure to an object, we need to be sure that the
+// object has been initialized. We are guaranteed that an object
+// is initialized if we are holding the Heap_lock with the
+// world stopped.
+void CompactibleFreeListSpace::verify_objects_initialized() const {
+  if (is_init_completed()) {
+    assert_locked_or_safepoint(Heap_lock);
+    if (Universe::is_fully_initialized()) {
+      guarantee(SafepointSynchronize::is_at_safepoint(),
+                "Required for objects to be initialized");
+    }
+  } // else make a concession at vm start-up
+}
+
+// Apply the given closure to each object in the space
+void CompactibleFreeListSpace::object_iterate(ObjectClosure* blk) {
+  assert_lock_strong(freelistLock());
+  NOT_PRODUCT(verify_objects_initialized());
+  HeapWord *cur, *limit;
+  size_t curSize;
+  for (cur = bottom(), limit = end(); cur < limit;
+       cur += curSize) {
+    curSize = block_size(cur);
+    if (block_is_obj(cur)) {
+      blk->do_object(oop(cur));
+    }
+  }
+}
+
+// Apply the given closure to each live object in the space
+//   The usage of CompactibleFreeListSpace
+// by the ConcurrentMarkSweepGeneration for concurrent GC's allows
+// objects in the space with references to objects that are no longer
+// valid.  For example, an object may reference another object
+// that has already been sweep up (collected).  This method uses
+// obj_is_alive() to determine whether it is safe to apply the closure to
+// an object.  See obj_is_alive() for details on how liveness of an
+// object is decided.
+
+void CompactibleFreeListSpace::safe_object_iterate(ObjectClosure* blk) {
+  assert_lock_strong(freelistLock());
+  NOT_PRODUCT(verify_objects_initialized());
+  HeapWord *cur, *limit;
+  size_t curSize;
+  for (cur = bottom(), limit = end(); cur < limit;
+       cur += curSize) {
+    curSize = block_size(cur);
+    if (block_is_obj(cur) && obj_is_alive(cur)) {
+      blk->do_object(oop(cur));
+    }
+  }
+}
+
+void CompactibleFreeListSpace::object_iterate_mem(MemRegion mr,
+                                                  UpwardsObjectClosure* cl) {
+  assert_locked(freelistLock());
+  NOT_PRODUCT(verify_objects_initialized());
+  assert(!mr.is_empty(), "Should be non-empty");
+  // We use MemRegion(bottom(), end()) rather than used_region() below
+  // because the two are not necessarily equal for some kinds of
+  // spaces, in particular, certain kinds of free list spaces.
+  // We could use the more complicated but more precise:
+  // MemRegion(used_region().start(), align_up(used_region().end(), CardSize))
+  // but the slight imprecision seems acceptable in the assertion check.
+  assert(MemRegion(bottom(), end()).contains(mr),
+         "Should be within used space");
+  HeapWord* prev = cl->previous();   // max address from last time
+  if (prev >= mr.end()) { // nothing to do
+    return;
+  }
+  // This assert will not work when we go from cms space to perm
+  // space, and use same closure. Easy fix deferred for later. XXX YSR
+  // assert(prev == NULL || contains(prev), "Should be within space");
+
+  bool last_was_obj_array = false;
+  HeapWord *blk_start_addr, *region_start_addr;
+  if (prev > mr.start()) {
+    region_start_addr = prev;
+    blk_start_addr    = prev;
+    // The previous invocation may have pushed "prev" beyond the
+    // last allocated block yet there may be still be blocks
+    // in this region due to a particular coalescing policy.
+    // Relax the assertion so that the case where the unallocated
+    // block is maintained and "prev" is beyond the unallocated
+    // block does not cause the assertion to fire.
+    assert((BlockOffsetArrayUseUnallocatedBlock &&
+            (!is_in(prev))) ||
+           (blk_start_addr == block_start(region_start_addr)), "invariant");
+  } else {
+    region_start_addr = mr.start();
+    blk_start_addr    = block_start(region_start_addr);
+  }
+  HeapWord* region_end_addr = mr.end();
+  MemRegion derived_mr(region_start_addr, region_end_addr);
+  while (blk_start_addr < region_end_addr) {
+    const size_t size = block_size(blk_start_addr);
+    if (block_is_obj(blk_start_addr)) {
+      last_was_obj_array = cl->do_object_bm(oop(blk_start_addr), derived_mr);
+    } else {
+      last_was_obj_array = false;
+    }
+    blk_start_addr += size;
+  }
+  if (!last_was_obj_array) {
+    assert((bottom() <= blk_start_addr) && (blk_start_addr <= end()),
+           "Should be within (closed) used space");
+    assert(blk_start_addr > prev, "Invariant");
+    cl->set_previous(blk_start_addr); // min address for next time
+  }
+}
+
+// Callers of this iterator beware: The closure application should
+// be robust in the face of uninitialized objects and should (always)
+// return a correct size so that the next addr + size below gives us a
+// valid block boundary. [See for instance,
+// ScanMarkedObjectsAgainCarefullyClosure::do_object_careful()
+// in ConcurrentMarkSweepGeneration.cpp.]
+HeapWord*
+CompactibleFreeListSpace::object_iterate_careful_m(MemRegion mr,
+  ObjectClosureCareful* cl) {
+  assert_lock_strong(freelistLock());
+  // Can't use used_region() below because it may not necessarily
+  // be the same as [bottom(),end()); although we could
+  // use [used_region().start(),align_up(used_region().end(),CardSize)),
+  // that appears too cumbersome, so we just do the simpler check
+  // in the assertion below.
+  assert(!mr.is_empty() && MemRegion(bottom(),end()).contains(mr),
+         "mr should be non-empty and within used space");
+  HeapWord *addr, *end;
+  size_t size;
+  for (addr = block_start_careful(mr.start()), end  = mr.end();
+       addr < end; addr += size) {
+    FreeChunk* fc = (FreeChunk*)addr;
+    if (fc->is_free()) {
+      // Since we hold the free list lock, which protects direct
+      // allocation in this generation by mutators, a free object
+      // will remain free throughout this iteration code.
+      size = fc->size();
+    } else {
+      // Note that the object need not necessarily be initialized,
+      // because (for instance) the free list lock does NOT protect
+      // object initialization. The closure application below must
+      // therefore be correct in the face of uninitialized objects.
+      size = cl->do_object_careful_m(oop(addr), mr);
+      if (size == 0) {
+        // An unparsable object found. Signal early termination.
+        return addr;
+      }
+    }
+  }
+  return NULL;
+}
+
+
+HeapWord* CompactibleFreeListSpace::block_start_const(const void* p) const {
+  NOT_PRODUCT(verify_objects_initialized());
+  return _bt.block_start(p);
+}
+
+HeapWord* CompactibleFreeListSpace::block_start_careful(const void* p) const {
+  return _bt.block_start_careful(p);
+}
+
+size_t CompactibleFreeListSpace::block_size(const HeapWord* p) const {
+  NOT_PRODUCT(verify_objects_initialized());
+  // This must be volatile, or else there is a danger that the compiler
+  // will compile the code below into a sometimes-infinite loop, by keeping
+  // the value read the first time in a register.
+  while (true) {
+    // We must do this until we get a consistent view of the object.
+    if (FreeChunk::indicatesFreeChunk(p)) {
+      volatile FreeChunk* fc = (volatile FreeChunk*)p;
+      size_t res = fc->size();
+
+      // Bugfix for systems with weak memory model (PPC64/IA64). The
+      // block's free bit was set and we have read the size of the
+      // block. Acquire and check the free bit again. If the block is
+      // still free, the read size is correct.
+      OrderAccess::acquire();
+
+      // If the object is still a free chunk, return the size, else it
+      // has been allocated so try again.
+      if (FreeChunk::indicatesFreeChunk(p)) {
+        assert(res != 0, "Block size should not be 0");
+        return res;
+      }
+    } else {
+      // Ensure klass read before size.
+      Klass* k = oop(p)->klass_or_null_acquire();
+      if (k != NULL) {
+        assert(k->is_klass(), "Should really be klass oop.");
+        oop o = (oop)p;
+        assert(oopDesc::is_oop(o, true /* ignore mark word */), "Should be an oop.");
+
+        size_t res = o->size_given_klass(k);
+        res = adjustObjectSize(res);
+        assert(res != 0, "Block size should not be 0");
+        return res;
+      }
+    }
+  }
+}
+
+// TODO: Now that is_parsable is gone, we should combine these two functions.
+// A variant of the above that uses the Printezis bits for
+// unparsable but allocated objects. This avoids any possible
+// stalls waiting for mutators to initialize objects, and is
+// thus potentially faster than the variant above. However,
+// this variant may return a zero size for a block that is
+// under mutation and for which a consistent size cannot be
+// inferred without stalling; see CMSCollector::block_size_if_printezis_bits().
+size_t CompactibleFreeListSpace::block_size_no_stall(HeapWord* p,
+                                                     const CMSCollector* c)
+const {
+  assert(MemRegion(bottom(), end()).contains(p), "p not in space");
+  // This must be volatile, or else there is a danger that the compiler
+  // will compile the code below into a sometimes-infinite loop, by keeping
+  // the value read the first time in a register.
+  DEBUG_ONLY(uint loops = 0;)
+  while (true) {
+    // We must do this until we get a consistent view of the object.
+    if (FreeChunk::indicatesFreeChunk(p)) {
+      volatile FreeChunk* fc = (volatile FreeChunk*)p;
+      size_t res = fc->size();
+
+      // Bugfix for systems with weak memory model (PPC64/IA64). The
+      // free bit of the block was set and we have read the size of
+      // the block. Acquire and check the free bit again. If the
+      // block is still free, the read size is correct.
+      OrderAccess::acquire();
+
+      if (FreeChunk::indicatesFreeChunk(p)) {
+        assert(res != 0, "Block size should not be 0");
+        assert(loops == 0, "Should be 0");
+        return res;
+      }
+    } else {
+      // Ensure klass read before size.
+      Klass* k = oop(p)->klass_or_null_acquire();
+      if (k != NULL) {
+        assert(k->is_klass(), "Should really be klass oop.");
+        oop o = (oop)p;
+        assert(oopDesc::is_oop(o), "Should be an oop");
+
+        size_t res = o->size_given_klass(k);
+        res = adjustObjectSize(res);
+        assert(res != 0, "Block size should not be 0");
+        return res;
+      } else {
+        // May return 0 if P-bits not present.
+        return c->block_size_if_printezis_bits(p);
+      }
+    }
+    assert(loops == 0, "Can loop at most once");
+    DEBUG_ONLY(loops++;)
+  }
+}
+
+size_t CompactibleFreeListSpace::block_size_nopar(const HeapWord* p) const {
+  NOT_PRODUCT(verify_objects_initialized());
+  assert(MemRegion(bottom(), end()).contains(p), "p not in space");
+  FreeChunk* fc = (FreeChunk*)p;
+  if (fc->is_free()) {
+    return fc->size();
+  } else {
+    // Ignore mark word because this may be a recently promoted
+    // object whose mark word is used to chain together grey
+    // objects (the last one would have a null value).
+    assert(oopDesc::is_oop(oop(p), true), "Should be an oop");
+    return adjustObjectSize(oop(p)->size());
+  }
+}
+
+// This implementation assumes that the property of "being an object" is
+// stable.  But being a free chunk may not be (because of parallel
+// promotion.)
+bool CompactibleFreeListSpace::block_is_obj(const HeapWord* p) const {
+  FreeChunk* fc = (FreeChunk*)p;
+  assert(is_in_reserved(p), "Should be in space");
+  if (FreeChunk::indicatesFreeChunk(p)) return false;
+  Klass* k = oop(p)->klass_or_null_acquire();
+  if (k != NULL) {
+    // Ignore mark word because it may have been used to
+    // chain together promoted objects (the last one
+    // would have a null value).
+    assert(oopDesc::is_oop(oop(p), true), "Should be an oop");
+    return true;
+  } else {
+    return false;  // Was not an object at the start of collection.
+  }
+}
+
+// Check if the object is alive. This fact is checked either by consulting
+// the main marking bitmap in the sweeping phase or, if it's a permanent
+// generation and we're not in the sweeping phase, by checking the
+// perm_gen_verify_bit_map where we store the "deadness" information if
+// we did not sweep the perm gen in the most recent previous GC cycle.
+bool CompactibleFreeListSpace::obj_is_alive(const HeapWord* p) const {
+  assert(SafepointSynchronize::is_at_safepoint() || !is_init_completed(),
+         "Else races are possible");
+  assert(block_is_obj(p), "The address should point to an object");
+
+  // If we're sweeping, we use object liveness information from the main bit map
+  // for both perm gen and old gen.
+  // We don't need to lock the bitmap (live_map or dead_map below), because
+  // EITHER we are in the middle of the sweeping phase, and the
+  // main marking bit map (live_map below) is locked,
+  // OR we're in other phases and perm_gen_verify_bit_map (dead_map below)
+  // is stable, because it's mutated only in the sweeping phase.
+  // NOTE: This method is also used by jmap where, if class unloading is
+  // off, the results can return "false" for legitimate perm objects,
+  // when we are not in the midst of a sweeping phase, which can result
+  // in jmap not reporting certain perm gen objects. This will be moot
+  // if/when the perm gen goes away in the future.
+  if (_collector->abstract_state() == CMSCollector::Sweeping) {
+    CMSBitMap* live_map = _collector->markBitMap();
+    return live_map->par_isMarked((HeapWord*) p);
+  }
+  return true;
+}
+
+bool CompactibleFreeListSpace::block_is_obj_nopar(const HeapWord* p) const {
+  FreeChunk* fc = (FreeChunk*)p;
+  assert(is_in_reserved(p), "Should be in space");
+  assert(_bt.block_start(p) == p, "Should be a block boundary");
+  if (!fc->is_free()) {
+    // Ignore mark word because it may have been used to
+    // chain together promoted objects (the last one
+    // would have a null value).
+    assert(oopDesc::is_oop(oop(p), true), "Should be an oop");
+    return true;
+  }
+  return false;
+}
+
+// "MT-safe but not guaranteed MT-precise" (TM); you may get an
+// approximate answer if you don't hold the freelistlock when you call this.
+size_t CompactibleFreeListSpace::totalSizeInIndexedFreeLists() const {
+  size_t size = 0;
+  for (size_t i = IndexSetStart; i < IndexSetSize; i += IndexSetStride) {
+    debug_only(
+      // We may be calling here without the lock in which case we
+      // won't do this modest sanity check.
+      if (freelistLock()->owned_by_self()) {
+        size_t total_list_size = 0;
+        for (FreeChunk* fc = _indexedFreeList[i].head(); fc != NULL;
+          fc = fc->next()) {
+          total_list_size += i;
+        }
+        assert(total_list_size == i * _indexedFreeList[i].count(),
+               "Count in list is incorrect");
+      }
+    )
+    size += i * _indexedFreeList[i].count();
+  }
+  return size;
+}
+
+HeapWord* CompactibleFreeListSpace::par_allocate(size_t size) {
+  MutexLockerEx x(freelistLock(), Mutex::_no_safepoint_check_flag);
+  return allocate(size);
+}
+
+HeapWord*
+CompactibleFreeListSpace::getChunkFromSmallLinearAllocBlockRemainder(size_t size) {
+  return getChunkFromLinearAllocBlockRemainder(&_smallLinearAllocBlock, size);
+}
+
+HeapWord* CompactibleFreeListSpace::allocate(size_t size) {
+  assert_lock_strong(freelistLock());
+  HeapWord* res = NULL;
+  assert(size == adjustObjectSize(size),
+         "use adjustObjectSize() before calling into allocate()");
+
+  res = allocate_adaptive_freelists(size);
+
+  if (res != NULL) {
+    // check that res does lie in this space!
+    assert(is_in_reserved(res), "Not in this space!");
+    assert(is_aligned((void*)res), "alignment check");
+
+    FreeChunk* fc = (FreeChunk*)res;
+    fc->markNotFree();
+    assert(!fc->is_free(), "shouldn't be marked free");
+    assert(oop(fc)->klass_or_null() == NULL, "should look uninitialized");
+    // Verify that the block offset table shows this to
+    // be a single block, but not one which is unallocated.
+    _bt.verify_single_block(res, size);
+    _bt.verify_not_unallocated(res, size);
+    // mangle a just allocated object with a distinct pattern.
+    debug_only(fc->mangleAllocated(size));
+  }
+
+  // During GC we do not need to recalculate the stable used value for
+  // every allocation in old gen. It is done once at the end of GC instead
+  // for performance reasons.
+  if (!CMSHeap::heap()->is_gc_active()) {
+    recalculate_used_stable();
+  }
+
+  return res;
+}
+
+HeapWord* CompactibleFreeListSpace::allocate_adaptive_freelists(size_t size) {
+  assert_lock_strong(freelistLock());
+  HeapWord* res = NULL;
+  assert(size == adjustObjectSize(size),
+         "use adjustObjectSize() before calling into allocate()");
+
+  // Strategy
+  //   if small
+  //     exact size from small object indexed list if small
+  //     small or large linear allocation block (linAB) as appropriate
+  //     take from lists of greater sized chunks
+  //   else
+  //     dictionary
+  //     small or large linear allocation block if it has the space
+  // Try allocating exact size from indexTable first
+  if (size < IndexSetSize) {
+    res = (HeapWord*) getChunkFromIndexedFreeList(size);
+    if(res != NULL) {
+      assert(res != (HeapWord*)_indexedFreeList[size].head(),
+        "Not removed from free list");
+      // no block offset table adjustment is necessary on blocks in
+      // the indexed lists.
+
+    // Try allocating from the small LinAB
+    } else if (size < _smallLinearAllocBlock._allocation_size_limit &&
+        (res = getChunkFromSmallLinearAllocBlock(size)) != NULL) {
+        // if successful, the above also adjusts block offset table
+        // Note that this call will refill the LinAB to
+        // satisfy the request.  This is different that
+        // evm.
+        // Don't record chunk off a LinAB?  smallSplitBirth(size);
+    } else {
+      // Raid the exact free lists larger than size, even if they are not
+      // overpopulated.
+      res = (HeapWord*) getChunkFromGreater(size);
+    }
+  } else {
+    // Big objects get allocated directly from the dictionary.
+    res = (HeapWord*) getChunkFromDictionaryExact(size);
+    if (res == NULL) {
+      // Try hard not to fail since an allocation failure will likely
+      // trigger a synchronous GC.  Try to get the space from the
+      // allocation blocks.
+      res = getChunkFromSmallLinearAllocBlockRemainder(size);
+    }
+  }
+
+  return res;
+}
+
+// A worst-case estimate of the space required (in HeapWords) to expand the heap
+// when promoting obj.
+size_t CompactibleFreeListSpace::expansionSpaceRequired(size_t obj_size) const {
+  // Depending on the object size, expansion may require refilling either a
+  // bigLAB or a smallLAB plus refilling a PromotionInfo object.  MinChunkSize
+  // is added because the dictionary may over-allocate to avoid fragmentation.
+  size_t space = obj_size;
+  space += _promoInfo.refillSize() + 2 * MinChunkSize;
+  return space;
+}
+
+FreeChunk* CompactibleFreeListSpace::getChunkFromGreater(size_t numWords) {
+  FreeChunk* ret;
+
+  assert(numWords >= MinChunkSize, "Size is less than minimum");
+  assert(linearAllocationWouldFail() || bestFitFirst(),
+    "Should not be here");
+
+  size_t i;
+  size_t currSize = numWords + MinChunkSize;
+  assert(is_object_aligned(currSize), "currSize should be aligned");
+  for (i = currSize; i < IndexSetSize; i += IndexSetStride) {
+    AdaptiveFreeList<FreeChunk>* fl = &_indexedFreeList[i];
+    if (fl->head()) {
+      ret = getFromListGreater(fl, numWords);
+      assert(ret == NULL || ret->is_free(), "Should be returning a free chunk");
+      return ret;
+    }
+  }
+
+  currSize = MAX2((size_t)SmallForDictionary,
+                  (size_t)(numWords + MinChunkSize));
+
+  /* Try to get a chunk that satisfies request, while avoiding
+     fragmentation that can't be handled. */
+  {
+    ret =  dictionary()->get_chunk(currSize);
+    if (ret != NULL) {
+      assert(ret->size() - numWords >= MinChunkSize,
+             "Chunk is too small");
+      _bt.allocated((HeapWord*)ret, ret->size());
+      /* Carve returned chunk. */
+      (void) splitChunkAndReturnRemainder(ret, numWords);
+      /* Label this as no longer a free chunk. */
+      assert(ret->is_free(), "This chunk should be free");
+      ret->link_prev(NULL);
+    }
+    assert(ret == NULL || ret->is_free(), "Should be returning a free chunk");
+    return ret;
+  }
+  ShouldNotReachHere();
+}
+
+bool CompactibleFreeListSpace::verifyChunkInIndexedFreeLists(FreeChunk* fc) const {
+  assert(fc->size() < IndexSetSize, "Size of chunk is too large");
+  return _indexedFreeList[fc->size()].verify_chunk_in_free_list(fc);
+}
+
+bool CompactibleFreeListSpace::verify_chunk_is_linear_alloc_block(FreeChunk* fc) const {
+  assert((_smallLinearAllocBlock._ptr != (HeapWord*)fc) ||
+         (_smallLinearAllocBlock._word_size == fc->size()),
+         "Linear allocation block shows incorrect size");
+  return ((_smallLinearAllocBlock._ptr == (HeapWord*)fc) &&
+          (_smallLinearAllocBlock._word_size == fc->size()));
+}
+
+// Check if the purported free chunk is present either as a linear
+// allocation block, the size-indexed table of (smaller) free blocks,
+// or the larger free blocks kept in the binary tree dictionary.
+bool CompactibleFreeListSpace::verify_chunk_in_free_list(FreeChunk* fc) const {
+  if (verify_chunk_is_linear_alloc_block(fc)) {
+    return true;
+  } else if (fc->size() < IndexSetSize) {
+    return verifyChunkInIndexedFreeLists(fc);
+  } else {
+    return dictionary()->verify_chunk_in_free_list(fc);
+  }
+}
+
+#ifndef PRODUCT
+void CompactibleFreeListSpace::assert_locked() const {
+  CMSLockVerifier::assert_locked(freelistLock(), parDictionaryAllocLock());
+}
+
+void CompactibleFreeListSpace::assert_locked(const Mutex* lock) const {
+  CMSLockVerifier::assert_locked(lock);
+}
+#endif
+
+FreeChunk* CompactibleFreeListSpace::allocateScratch(size_t size) {
+  // In the parallel case, the main thread holds the free list lock
+  // on behalf the parallel threads.
+  FreeChunk* fc;
+  {
+    // If GC is parallel, this might be called by several threads.
+    // This should be rare enough that the locking overhead won't affect
+    // the sequential code.
+    MutexLockerEx x(parDictionaryAllocLock(),
+                    Mutex::_no_safepoint_check_flag);
+    fc = getChunkFromDictionary(size);
+  }
+  if (fc != NULL) {
+    fc->dontCoalesce();
+    assert(fc->is_free(), "Should be free, but not coalescable");
+    // Verify that the block offset table shows this to
+    // be a single block, but not one which is unallocated.
+    _bt.verify_single_block((HeapWord*)fc, fc->size());
+    _bt.verify_not_unallocated((HeapWord*)fc, fc->size());
+  }
+  return fc;
+}
+
+oop CompactibleFreeListSpace::promote(oop obj, size_t obj_size) {
+  assert(obj_size == (size_t)obj->size(), "bad obj_size passed in");
+  assert_locked();
+
+  // if we are tracking promotions, then first ensure space for
+  // promotion (including spooling space for saving header if necessary).
+  // then allocate and copy, then track promoted info if needed.
+  // When tracking (see PromotionInfo::track()), the mark word may
+  // be displaced and in this case restoration of the mark word
+  // occurs in the (oop_since_save_marks_)iterate phase.
+  if (_promoInfo.tracking() && !_promoInfo.ensure_spooling_space()) {
+    return NULL;
+  }
+  // Call the allocate(size_t, bool) form directly to avoid the
+  // additional call through the allocate(size_t) form.  Having
+  // the compile inline the call is problematic because allocate(size_t)
+  // is a virtual method.
+  HeapWord* res = allocate(adjustObjectSize(obj_size));
+  if (res != NULL) {
+    Copy::aligned_disjoint_words((HeapWord*)obj, res, obj_size);
+    // if we should be tracking promotions, do so.
+    if (_promoInfo.tracking()) {
+        _promoInfo.track((PromotedObject*)res);
+    }
+  }
+  return oop(res);
+}
+
+HeapWord*
+CompactibleFreeListSpace::getChunkFromSmallLinearAllocBlock(size_t size) {
+  assert_locked();
+  assert(size >= MinChunkSize, "minimum chunk size");
+  assert(size <  _smallLinearAllocBlock._allocation_size_limit,
+    "maximum from smallLinearAllocBlock");
+  return getChunkFromLinearAllocBlock(&_smallLinearAllocBlock, size);
+}
+
+HeapWord*
+CompactibleFreeListSpace::getChunkFromLinearAllocBlock(LinearAllocBlock *blk,
+                                                       size_t size) {
+  assert_locked();
+  assert(size >= MinChunkSize, "too small");
+  HeapWord* res = NULL;
+  // Try to do linear allocation from blk, making sure that
+  if (blk->_word_size == 0) {
+    // We have probably been unable to fill this either in the prologue or
+    // when it was exhausted at the last linear allocation. Bail out until
+    // next time.
+    assert(blk->_ptr == NULL, "consistency check");
+    return NULL;
+  }
+  assert(blk->_word_size != 0 && blk->_ptr != NULL, "consistency check");
+  res = getChunkFromLinearAllocBlockRemainder(blk, size);
+  if (res != NULL) return res;
+
+  // about to exhaust this linear allocation block
+  if (blk->_word_size == size) { // exactly satisfied
+    res = blk->_ptr;
+    _bt.allocated(res, blk->_word_size);
+  } else if (size + MinChunkSize <= blk->_refillSize) {
+    size_t sz = blk->_word_size;
+    // Update _unallocated_block if the size is such that chunk would be
+    // returned to the indexed free list.  All other chunks in the indexed
+    // free lists are allocated from the dictionary so that _unallocated_block
+    // has already been adjusted for them.  Do it here so that the cost
+    // for all chunks added back to the indexed free lists.
+    if (sz < SmallForDictionary) {
+      _bt.allocated(blk->_ptr, sz);
+    }
+    // Return the chunk that isn't big enough, and then refill below.
+    addChunkToFreeLists(blk->_ptr, sz);
+    split_birth(sz);
+    // Don't keep statistics on adding back chunk from a LinAB.
+  } else {
+    // A refilled block would not satisfy the request.
+    return NULL;
+  }
+
+  blk->_ptr = NULL; blk->_word_size = 0;
+  refillLinearAllocBlock(blk);
+  assert(blk->_ptr == NULL || blk->_word_size >= size + MinChunkSize,
+         "block was replenished");
+  if (res != NULL) {
+    split_birth(size);
+    repairLinearAllocBlock(blk);
+  } else if (blk->_ptr != NULL) {
+    res = blk->_ptr;
+    size_t blk_size = blk->_word_size;
+    blk->_word_size -= size;
+    blk->_ptr  += size;
+    split_birth(size);
+    repairLinearAllocBlock(blk);
+    // Update BOT last so that other (parallel) GC threads see a consistent
+    // view of the BOT and free blocks.
+    // Above must occur before BOT is updated below.
+    OrderAccess::storestore();
+    _bt.split_block(res, blk_size, size);  // adjust block offset table
+  }
+  return res;
+}
+
+HeapWord*  CompactibleFreeListSpace::getChunkFromLinearAllocBlockRemainder(
+                                        LinearAllocBlock* blk,
+                                        size_t size) {
+  assert_locked();
+  assert(size >= MinChunkSize, "too small");
+
+  HeapWord* res = NULL;
+  // This is the common case.  Keep it simple.
+  if (blk->_word_size >= size + MinChunkSize) {
+    assert(blk->_ptr != NULL, "consistency check");
+    res = blk->_ptr;
+    // Note that the BOT is up-to-date for the linAB before allocation.  It
+    // indicates the start of the linAB.  The split_block() updates the
+    // BOT for the linAB after the allocation (indicates the start of the
+    // next chunk to be allocated).
+    size_t blk_size = blk->_word_size;
+    blk->_word_size -= size;
+    blk->_ptr  += size;
+    split_birth(size);
+    repairLinearAllocBlock(blk);
+    // Update BOT last so that other (parallel) GC threads see a consistent
+    // view of the BOT and free blocks.
+    // Above must occur before BOT is updated below.
+    OrderAccess::storestore();
+    _bt.split_block(res, blk_size, size);  // adjust block offset table
+    _bt.allocated(res, size);
+  }
+  return res;
+}
+
+FreeChunk*
+CompactibleFreeListSpace::getChunkFromIndexedFreeList(size_t size) {
+  assert_locked();
+  assert(size < SmallForDictionary, "just checking");
+  FreeChunk* res;
+  res = _indexedFreeList[size].get_chunk_at_head();
+  if (res == NULL) {
+    res = getChunkFromIndexedFreeListHelper(size);
+  }
+  _bt.verify_not_unallocated((HeapWord*) res, size);
+  assert(res == NULL || res->size() == size, "Incorrect block size");
+  return res;
+}
+
+FreeChunk*
+CompactibleFreeListSpace::getChunkFromIndexedFreeListHelper(size_t size,
+  bool replenish) {
+  assert_locked();
+  FreeChunk* fc = NULL;
+  if (size < SmallForDictionary) {
+    assert(_indexedFreeList[size].head() == NULL ||
+      _indexedFreeList[size].surplus() <= 0,
+      "List for this size should be empty or under populated");
+    // Try best fit in exact lists before replenishing the list
+    if (!bestFitFirst() || (fc = bestFitSmall(size)) == NULL) {
+      // Replenish list.
+      //
+      // Things tried that failed.
+      //   Tried allocating out of the two LinAB's first before
+      // replenishing lists.
+      //   Tried small linAB of size 256 (size in indexed list)
+      // and replenishing indexed lists from the small linAB.
+      //
+      FreeChunk* newFc = NULL;
+      const size_t replenish_size = CMSIndexedFreeListReplenish * size;
+      if (replenish_size < SmallForDictionary) {
+        // Do not replenish from an underpopulated size.
+        if (_indexedFreeList[replenish_size].surplus() > 0 &&
+            _indexedFreeList[replenish_size].head() != NULL) {
+          newFc = _indexedFreeList[replenish_size].get_chunk_at_head();
+        } else if (bestFitFirst()) {
+          newFc = bestFitSmall(replenish_size);
+        }
+      }
+      if (newFc == NULL && replenish_size > size) {
+        assert(CMSIndexedFreeListReplenish > 1, "ctl pt invariant");
+        newFc = getChunkFromIndexedFreeListHelper(replenish_size, false);
+      }
+      // Note: The stats update re split-death of block obtained above
+      // will be recorded below precisely when we know we are going to
+      // be actually splitting it into more than one pieces below.
+      if (newFc != NULL) {
+        if  (replenish || CMSReplenishIntermediate) {
+          // Replenish this list and return one block to caller.
+          size_t i;
+          FreeChunk *curFc, *nextFc;
+          size_t num_blk = newFc->size() / size;
+          assert(num_blk >= 1, "Smaller than requested?");
+          assert(newFc->size() % size == 0, "Should be integral multiple of request");
+          if (num_blk > 1) {
+            // we are sure we will be splitting the block just obtained
+            // into multiple pieces; record the split-death of the original
+            splitDeath(replenish_size);
+          }
+          // carve up and link blocks 0, ..., num_blk - 2
+          // The last chunk is not added to the lists but is returned as the
+          // free chunk.
+          for (curFc = newFc, nextFc = (FreeChunk*)((HeapWord*)curFc + size),
+               i = 0;
+               i < (num_blk - 1);
+               curFc = nextFc, nextFc = (FreeChunk*)((HeapWord*)nextFc + size),
+               i++) {
+            curFc->set_size(size);
+            // Don't record this as a return in order to try and
+            // determine the "returns" from a GC.
+            _bt.verify_not_unallocated((HeapWord*) fc, size);
+            _indexedFreeList[size].return_chunk_at_tail(curFc, false);
+            _bt.mark_block((HeapWord*)curFc, size);
+            split_birth(size);
+            // Don't record the initial population of the indexed list
+            // as a split birth.
+          }
+
+          // check that the arithmetic was OK above
+          assert((HeapWord*)nextFc == (HeapWord*)newFc + num_blk*size,
+            "inconsistency in carving newFc");
+          curFc->set_size(size);
+          _bt.mark_block((HeapWord*)curFc, size);
+          split_birth(size);
+          fc = curFc;
+        } else {
+          // Return entire block to caller
+          fc = newFc;
+        }
+      }
+    }
+  } else {
+    // Get a free chunk from the free chunk dictionary to be returned to
+    // replenish the indexed free list.
+    fc = getChunkFromDictionaryExact(size);
+  }
+  // assert(fc == NULL || fc->is_free(), "Should be returning a free chunk");
+  return fc;
+}
+
+FreeChunk*
+CompactibleFreeListSpace::getChunkFromDictionary(size_t size) {
+  assert_locked();
+  FreeChunk* fc = _dictionary->get_chunk(size);
+  if (fc == NULL) {
+    return NULL;
+  }
+  _bt.allocated((HeapWord*)fc, fc->size());
+  if (fc->size() >= size + MinChunkSize) {
+    fc = splitChunkAndReturnRemainder(fc, size);
+  }
+  assert(fc->size() >= size, "chunk too small");
+  assert(fc->size() < size + MinChunkSize, "chunk too big");
+  _bt.verify_single_block((HeapWord*)fc, fc->size());
+  return fc;
+}
+
+FreeChunk*
+CompactibleFreeListSpace::getChunkFromDictionaryExact(size_t size) {
+  assert_locked();
+  FreeChunk* fc = _dictionary->get_chunk(size);
+  if (fc == NULL) {
+    return fc;
+  }
+  _bt.allocated((HeapWord*)fc, fc->size());
+  if (fc->size() == size) {
+    _bt.verify_single_block((HeapWord*)fc, size);
+    return fc;
+  }
+  assert(fc->size() > size, "get_chunk() guarantee");
+  if (fc->size() < size + MinChunkSize) {
+    // Return the chunk to the dictionary and go get a bigger one.
+    returnChunkToDictionary(fc);
+    fc = _dictionary->get_chunk(size + MinChunkSize);
+    if (fc == NULL) {
+      return NULL;
+    }
+    _bt.allocated((HeapWord*)fc, fc->size());
+  }
+  assert(fc->size() >= size + MinChunkSize, "tautology");
+  fc = splitChunkAndReturnRemainder(fc, size);
+  assert(fc->size() == size, "chunk is wrong size");
+  _bt.verify_single_block((HeapWord*)fc, size);
+  return fc;
+}
+
+void
+CompactibleFreeListSpace::returnChunkToDictionary(FreeChunk* chunk) {
+  assert_locked();
+
+  size_t size = chunk->size();
+  _bt.verify_single_block((HeapWord*)chunk, size);
+  // adjust _unallocated_block downward, as necessary
+  _bt.freed((HeapWord*)chunk, size);
+  _dictionary->return_chunk(chunk);
+#ifndef PRODUCT
+  if (CMSCollector::abstract_state() != CMSCollector::Sweeping) {
+    TreeChunk<FreeChunk, AdaptiveFreeList<FreeChunk> >* tc = TreeChunk<FreeChunk, AdaptiveFreeList<FreeChunk> >::as_TreeChunk(chunk);
+    TreeList<FreeChunk, AdaptiveFreeList<FreeChunk> >* tl = tc->list();
+    tl->verify_stats();
+  }
+#endif // PRODUCT
+}
+
+void
+CompactibleFreeListSpace::returnChunkToFreeList(FreeChunk* fc) {
+  assert_locked();
+  size_t size = fc->size();
+  _bt.verify_single_block((HeapWord*) fc, size);
+  _bt.verify_not_unallocated((HeapWord*) fc, size);
+  _indexedFreeList[size].return_chunk_at_tail(fc);
+#ifndef PRODUCT
+  if (CMSCollector::abstract_state() != CMSCollector::Sweeping) {
+     _indexedFreeList[size].verify_stats();
+  }
+#endif // PRODUCT
+}
+
+// Add chunk to end of last block -- if it's the largest
+// block -- and update BOT and census data. We would
+// of course have preferred to coalesce it with the
+// last block, but it's currently less expensive to find the
+// largest block than it is to find the last.
+void
+CompactibleFreeListSpace::addChunkToFreeListsAtEndRecordingStats(
+  HeapWord* chunk, size_t     size) {
+  // check that the chunk does lie in this space!
+  assert(chunk != NULL && is_in_reserved(chunk), "Not in this space!");
+  // One of the parallel gc task threads may be here
+  // whilst others are allocating.
+  Mutex* lock = &_parDictionaryAllocLock;
+  FreeChunk* ec;
+  {
+    MutexLockerEx x(lock, Mutex::_no_safepoint_check_flag);
+    ec = dictionary()->find_largest_dict();  // get largest block
+    if (ec != NULL && ec->end() == (uintptr_t*) chunk) {
+      // It's a coterminal block - we can coalesce.
+      size_t old_size = ec->size();
+      coalDeath(old_size);
+      removeChunkFromDictionary(ec);
+      size += old_size;
+    } else {
+      ec = (FreeChunk*)chunk;
+    }
+  }
+  ec->set_size(size);
+  debug_only(ec->mangleFreed(size));
+  if (size < SmallForDictionary) {
+    lock = _indexedFreeListParLocks[size];
+  }
+  MutexLockerEx x(lock, Mutex::_no_safepoint_check_flag);
+  addChunkAndRepairOffsetTable((HeapWord*)ec, size, true);
+  // record the birth under the lock since the recording involves
+  // manipulation of the list on which the chunk lives and
+  // if the chunk is allocated and is the last on the list,
+  // the list can go away.
+  coalBirth(size);
+}
+
+void
+CompactibleFreeListSpace::addChunkToFreeLists(HeapWord* chunk,
+                                              size_t     size) {
+  // check that the chunk does lie in this space!
+  assert(chunk != NULL && is_in_reserved(chunk), "Not in this space!");
+  assert_locked();
+  _bt.verify_single_block(chunk, size);
+
+  FreeChunk* fc = (FreeChunk*) chunk;
+  fc->set_size(size);
+  debug_only(fc->mangleFreed(size));
+  if (size < SmallForDictionary) {
+    returnChunkToFreeList(fc);
+  } else {
+    returnChunkToDictionary(fc);
+  }
+}
+
+void
+CompactibleFreeListSpace::addChunkAndRepairOffsetTable(HeapWord* chunk,
+  size_t size, bool coalesced) {
+  assert_locked();
+  assert(chunk != NULL, "null chunk");
+  if (coalesced) {
+    // repair BOT
+    _bt.single_block(chunk, size);
+  }
+  addChunkToFreeLists(chunk, size);
+}
+
+// We _must_ find the purported chunk on our free lists;
+// we assert if we don't.
+void
+CompactibleFreeListSpace::removeFreeChunkFromFreeLists(FreeChunk* fc) {
+  size_t size = fc->size();
+  assert_locked();
+  debug_only(verifyFreeLists());
+  if (size < SmallForDictionary) {
+    removeChunkFromIndexedFreeList(fc);
+  } else {
+    removeChunkFromDictionary(fc);
+  }
+  _bt.verify_single_block((HeapWord*)fc, size);
+  debug_only(verifyFreeLists());
+}
+
+void
+CompactibleFreeListSpace::removeChunkFromDictionary(FreeChunk* fc) {
+  size_t size = fc->size();
+  assert_locked();
+  assert(fc != NULL, "null chunk");
+  _bt.verify_single_block((HeapWord*)fc, size);
+  _dictionary->remove_chunk(fc);
+  // adjust _unallocated_block upward, as necessary
+  _bt.allocated((HeapWord*)fc, size);
+}
+
+void
+CompactibleFreeListSpace::removeChunkFromIndexedFreeList(FreeChunk* fc) {
+  assert_locked();
+  size_t size = fc->size();
+  _bt.verify_single_block((HeapWord*)fc, size);
+  NOT_PRODUCT(
+    if (FLSVerifyIndexTable) {
+      verifyIndexedFreeList(size);
+    }
+  )
+  _indexedFreeList[size].remove_chunk(fc);
+  NOT_PRODUCT(
+    if (FLSVerifyIndexTable) {
+      verifyIndexedFreeList(size);
+    }
+  )
+}
+
+FreeChunk* CompactibleFreeListSpace::bestFitSmall(size_t numWords) {
+  /* A hint is the next larger size that has a surplus.
+     Start search at a size large enough to guarantee that
+     the excess is >= MIN_CHUNK. */
+  size_t start = align_object_size(numWords + MinChunkSize);
+  if (start < IndexSetSize) {
+    AdaptiveFreeList<FreeChunk>* it   = _indexedFreeList;
+    size_t    hint = _indexedFreeList[start].hint();
+    while (hint < IndexSetSize) {
+      assert(is_object_aligned(hint), "hint should be aligned");
+      AdaptiveFreeList<FreeChunk> *fl = &_indexedFreeList[hint];
+      if (fl->surplus() > 0 && fl->head() != NULL) {
+        // Found a list with surplus, reset original hint
+        // and split out a free chunk which is returned.
+        _indexedFreeList[start].set_hint(hint);
+        FreeChunk* res = getFromListGreater(fl, numWords);
+        assert(res == NULL || res->is_free(),
+          "Should be returning a free chunk");
+        return res;
+      }
+      hint = fl->hint(); /* keep looking */
+    }
+    /* None found. */
+    it[start].set_hint(IndexSetSize);
+  }
+  return NULL;
+}
+
+/* Requires fl->size >= numWords + MinChunkSize */
+FreeChunk* CompactibleFreeListSpace::getFromListGreater(AdaptiveFreeList<FreeChunk>* fl,
+  size_t numWords) {
+  FreeChunk *curr = fl->head();
+  size_t oldNumWords = curr->size();
+  assert(numWords >= MinChunkSize, "Word size is too small");
+  assert(curr != NULL, "List is empty");
+  assert(oldNumWords >= numWords + MinChunkSize,
+        "Size of chunks in the list is too small");
+
+  fl->remove_chunk(curr);
+  // recorded indirectly by splitChunkAndReturnRemainder -
+  // smallSplit(oldNumWords, numWords);
+  FreeChunk* new_chunk = splitChunkAndReturnRemainder(curr, numWords);
+  // Does anything have to be done for the remainder in terms of
+  // fixing the card table?
+  assert(new_chunk == NULL || new_chunk->is_free(),
+    "Should be returning a free chunk");
+  return new_chunk;
+}
+
+FreeChunk*
+CompactibleFreeListSpace::splitChunkAndReturnRemainder(FreeChunk* chunk,
+  size_t new_size) {
+  assert_locked();
+  size_t size = chunk->size();
+  assert(size > new_size, "Split from a smaller block?");
+  assert(is_aligned(chunk), "alignment problem");
+  assert(size == adjustObjectSize(size), "alignment problem");
+  size_t rem_sz = size - new_size;
+  assert(rem_sz == adjustObjectSize(rem_sz), "alignment problem");
+  assert(rem_sz >= MinChunkSize, "Free chunk smaller than minimum");
+  FreeChunk* ffc = (FreeChunk*)((HeapWord*)chunk + new_size);
+  assert(is_aligned(ffc), "alignment problem");
+  ffc->set_size(rem_sz);
+  ffc->link_next(NULL);
+  ffc->link_prev(NULL); // Mark as a free block for other (parallel) GC threads.
+  // Above must occur before BOT is updated below.
+  // adjust block offset table
+  OrderAccess::storestore();
+  assert(chunk->is_free() && ffc->is_free(), "Error");
+  _bt.split_block((HeapWord*)chunk, chunk->size(), new_size);
+  if (rem_sz < SmallForDictionary) {
+    // The freeList lock is held, but multiple GC task threads might be executing in parallel.
+    bool is_par = Thread::current()->is_GC_task_thread();
+    if (is_par) _indexedFreeListParLocks[rem_sz]->lock();
+    returnChunkToFreeList(ffc);
+    split(size, rem_sz);
+    if (is_par) _indexedFreeListParLocks[rem_sz]->unlock();
+  } else {
+    returnChunkToDictionary(ffc);
+    split(size, rem_sz);
+  }
+  chunk->set_size(new_size);
+  return chunk;
+}
+
+void
+CompactibleFreeListSpace::sweep_completed() {
+  // Now that space is probably plentiful, refill linear
+  // allocation blocks as needed.
+  refillLinearAllocBlocksIfNeeded();
+}
+
+void
+CompactibleFreeListSpace::gc_prologue() {
+  assert_locked();
+  reportFreeListStatistics("Before GC:");
+  refillLinearAllocBlocksIfNeeded();
+}
+
+void
+CompactibleFreeListSpace::gc_epilogue() {
+  assert_locked();
+  assert(_promoInfo.noPromotions(), "_promoInfo inconsistency");
+  _promoInfo.stopTrackingPromotions();
+  repairLinearAllocationBlocks();
+  reportFreeListStatistics("After GC:");
+}
+
+// Iteration support, mostly delegated from a CMS generation
+
+void CompactibleFreeListSpace::save_marks() {
+  assert(Thread::current()->is_VM_thread(),
+         "Global variable should only be set when single-threaded");
+  // Mark the "end" of the used space at the time of this call;
+  // note, however, that promoted objects from this point
+  // on are tracked in the _promoInfo below.
+  set_saved_mark_word(unallocated_block());
+#ifdef ASSERT
+  // Check the sanity of save_marks() etc.
+  MemRegion ur    = used_region();
+  MemRegion urasm = used_region_at_save_marks();
+  assert(ur.contains(urasm),
+         " Error at save_marks(): [" PTR_FORMAT "," PTR_FORMAT ")"
+         " should contain [" PTR_FORMAT "," PTR_FORMAT ")",
+         p2i(ur.start()), p2i(ur.end()), p2i(urasm.start()), p2i(urasm.end()));
+#endif
+  // inform allocator that promotions should be tracked.
+  assert(_promoInfo.noPromotions(), "_promoInfo inconsistency");
+  _promoInfo.startTrackingPromotions();
+}
+
+bool CompactibleFreeListSpace::no_allocs_since_save_marks() {
+  assert(_promoInfo.tracking(), "No preceding save_marks?");
+  return _promoInfo.noPromotions();
+}
+
+bool CompactibleFreeListSpace::linearAllocationWouldFail() const {
+  return _smallLinearAllocBlock._word_size == 0;
+}
+
+void CompactibleFreeListSpace::repairLinearAllocationBlocks() {
+  // Fix up linear allocation blocks to look like free blocks
+  repairLinearAllocBlock(&_smallLinearAllocBlock);
+}
+
+void CompactibleFreeListSpace::repairLinearAllocBlock(LinearAllocBlock* blk) {
+  assert_locked();
+  if (blk->_ptr != NULL) {
+    assert(blk->_word_size != 0 && blk->_word_size >= MinChunkSize,
+           "Minimum block size requirement");
+    FreeChunk* fc = (FreeChunk*)(blk->_ptr);
+    fc->set_size(blk->_word_size);
+    fc->link_prev(NULL);   // mark as free
+    fc->dontCoalesce();
+    assert(fc->is_free(), "just marked it free");
+    assert(fc->cantCoalesce(), "just marked it uncoalescable");
+  }
+}
+
+void CompactibleFreeListSpace::refillLinearAllocBlocksIfNeeded() {
+  assert_locked();
+  if (_smallLinearAllocBlock._ptr == NULL) {
+    assert(_smallLinearAllocBlock._word_size == 0,
+      "Size of linAB should be zero if the ptr is NULL");
+    // Reset the linAB refill and allocation size limit.
+    _smallLinearAllocBlock.set(0, 0, 1024*SmallForLinearAlloc, SmallForLinearAlloc);
+  }
+  refillLinearAllocBlockIfNeeded(&_smallLinearAllocBlock);
+}
+
+void
+CompactibleFreeListSpace::refillLinearAllocBlockIfNeeded(LinearAllocBlock* blk) {
+  assert_locked();
+  assert((blk->_ptr == NULL && blk->_word_size == 0) ||
+         (blk->_ptr != NULL && blk->_word_size >= MinChunkSize),
+         "blk invariant");
+  if (blk->_ptr == NULL) {
+    refillLinearAllocBlock(blk);
+  }
+}
+
+void
+CompactibleFreeListSpace::refillLinearAllocBlock(LinearAllocBlock* blk) {
+  assert_locked();
+  assert(blk->_word_size == 0 && blk->_ptr == NULL,
+         "linear allocation block should be empty");
+  FreeChunk* fc;
+  if (blk->_refillSize < SmallForDictionary &&
+      (fc = getChunkFromIndexedFreeList(blk->_refillSize)) != NULL) {
+    // A linAB's strategy might be to use small sizes to reduce
+    // fragmentation but still get the benefits of allocation from a
+    // linAB.
+  } else {
+    fc = getChunkFromDictionary(blk->_refillSize);
+  }
+  if (fc != NULL) {
+    blk->_ptr  = (HeapWord*)fc;
+    blk->_word_size = fc->size();
+    fc->dontCoalesce();   // to prevent sweeper from sweeping us up
+  }
+}
+
+// Support for compaction
+void CompactibleFreeListSpace::prepare_for_compaction(CompactPoint* cp) {
+  scan_and_forward(this, cp, false);
+   // of the free lists doesn't work after.
+  // Prepare_for_compaction() uses the space between live objects
+  // so that later phase can skip dead space quickly.  So verification
+  // of the free lists doesn't work after.
+}
+
+void CompactibleFreeListSpace::adjust_pointers() {
+  // In other versions of adjust_pointers(), a bail out
+  // based on the amount of live data in the generation
+  // (i.e., if 0, bail out) may be used.
+  // Cannot test used() == 0 here because the free lists have already
+  // been mangled by the compaction.
+
+  scan_and_adjust_pointers(this);
+  // See note about verification in prepare_for_compaction().
+}
+
+void CompactibleFreeListSpace::compact() {
+  scan_and_compact(this, false);
+}
+
+// Fragmentation metric = 1 - [sum of (fbs**2) / (sum of fbs)**2]
+// where fbs is free block sizes
+double CompactibleFreeListSpace::flsFrag() const {
+  size_t itabFree = totalSizeInIndexedFreeLists();
+  double frag = 0.0;
+  size_t i;
+
+  for (i = IndexSetStart; i < IndexSetSize; i += IndexSetStride) {
+    double sz  = i;
+    frag      += _indexedFreeList[i].count() * (sz * sz);
+  }
+
+  double totFree = itabFree +
+                   _dictionary->total_chunk_size(DEBUG_ONLY(freelistLock()));
+  if (totFree > 0) {
+    frag = ((frag + _dictionary->sum_of_squared_block_sizes()) /
+            (totFree * totFree));
+    frag = (double)1.0  - frag;
+  } else {
+    assert(frag == 0.0, "Follows from totFree == 0");
+  }
+  return frag;
+}
+
+void CompactibleFreeListSpace::beginSweepFLCensus(
+  float inter_sweep_current,
+  float inter_sweep_estimate,
+  float intra_sweep_estimate) {
+  assert_locked();
+  size_t i;
+  for (i = IndexSetStart; i < IndexSetSize; i += IndexSetStride) {
+    AdaptiveFreeList<FreeChunk>* fl    = &_indexedFreeList[i];
+    log_trace(gc, freelist)("size[" SIZE_FORMAT "] : ", i);
+    fl->compute_desired(inter_sweep_current, inter_sweep_estimate, intra_sweep_estimate);
+    fl->set_coal_desired((ssize_t)((double)fl->desired() * CMSSmallCoalSurplusPercent));
+    fl->set_before_sweep(fl->count());
+    fl->set_bfr_surp(fl->surplus());
+  }
+  _dictionary->begin_sweep_dict_census(CMSLargeCoalSurplusPercent,
+                                    inter_sweep_current,
+                                    inter_sweep_estimate,
+                                    intra_sweep_estimate);
+}
+
+void CompactibleFreeListSpace::setFLSurplus() {
+  assert_locked();
+  size_t i;
+  for (i = IndexSetStart; i < IndexSetSize; i += IndexSetStride) {
+    AdaptiveFreeList<FreeChunk> *fl = &_indexedFreeList[i];
+    fl->set_surplus(fl->count() -
+                    (ssize_t)((double)fl->desired() * CMSSmallSplitSurplusPercent));
+  }
+}
+
+void CompactibleFreeListSpace::setFLHints() {
+  assert_locked();
+  size_t i;
+  size_t h = IndexSetSize;
+  for (i = IndexSetSize - 1; i != 0; i -= IndexSetStride) {
+    AdaptiveFreeList<FreeChunk> *fl = &_indexedFreeList[i];
+    fl->set_hint(h);
+    if (fl->surplus() > 0) {
+      h = i;
+    }
+  }
+}
+
+void CompactibleFreeListSpace::clearFLCensus() {
+  assert_locked();
+  size_t i;
+  for (i = IndexSetStart; i < IndexSetSize; i += IndexSetStride) {
+    AdaptiveFreeList<FreeChunk> *fl = &_indexedFreeList[i];
+    fl->set_prev_sweep(fl->count());
+    fl->set_coal_births(0);
+    fl->set_coal_deaths(0);
+    fl->set_split_births(0);
+    fl->set_split_deaths(0);
+  }
+}
+
+void CompactibleFreeListSpace::endSweepFLCensus(size_t sweep_count) {
+  log_debug(gc, freelist)("CMS: Large block " PTR_FORMAT, p2i(dictionary()->find_largest_dict()));
+  setFLSurplus();
+  setFLHints();
+  printFLCensus(sweep_count);
+  clearFLCensus();
+  assert_locked();
+  _dictionary->end_sweep_dict_census(CMSLargeSplitSurplusPercent);
+}
+
+bool CompactibleFreeListSpace::coalOverPopulated(size_t size) {
+  if (size < SmallForDictionary) {
+    AdaptiveFreeList<FreeChunk> *fl = &_indexedFreeList[size];
+    return (fl->coal_desired() < 0) ||
+           ((int)fl->count() > fl->coal_desired());
+  } else {
+    return dictionary()->coal_dict_over_populated(size);
+  }
+}
+
+void CompactibleFreeListSpace::smallCoalBirth(size_t size) {
+  assert(size < SmallForDictionary, "Size too large for indexed list");
+  AdaptiveFreeList<FreeChunk> *fl = &_indexedFreeList[size];
+  fl->increment_coal_births();
+  fl->increment_surplus();
+}
+
+void CompactibleFreeListSpace::smallCoalDeath(size_t size) {
+  assert(size < SmallForDictionary, "Size too large for indexed list");
+  AdaptiveFreeList<FreeChunk> *fl = &_indexedFreeList[size];
+  fl->increment_coal_deaths();
+  fl->decrement_surplus();
+}
+
+void CompactibleFreeListSpace::coalBirth(size_t size) {
+  if (size  < SmallForDictionary) {
+    smallCoalBirth(size);
+  } else {
+    dictionary()->dict_census_update(size,
+                                   false /* split */,
+                                   true /* birth */);
+  }
+}
+
+void CompactibleFreeListSpace::coalDeath(size_t size) {
+  if(size  < SmallForDictionary) {
+    smallCoalDeath(size);
+  } else {
+    dictionary()->dict_census_update(size,
+                                   false /* split */,
+                                   false /* birth */);
+  }
+}
+
+void CompactibleFreeListSpace::smallSplitBirth(size_t size) {
+  assert(size < SmallForDictionary, "Size too large for indexed list");
+  AdaptiveFreeList<FreeChunk> *fl = &_indexedFreeList[size];
+  fl->increment_split_births();
+  fl->increment_surplus();
+}
+
+void CompactibleFreeListSpace::smallSplitDeath(size_t size) {
+  assert(size < SmallForDictionary, "Size too large for indexed list");
+  AdaptiveFreeList<FreeChunk> *fl = &_indexedFreeList[size];
+  fl->increment_split_deaths();
+  fl->decrement_surplus();
+}
+
+void CompactibleFreeListSpace::split_birth(size_t size) {
+  if (size  < SmallForDictionary) {
+    smallSplitBirth(size);
+  } else {
+    dictionary()->dict_census_update(size,
+                                   true /* split */,
+                                   true /* birth */);
+  }
+}
+
+void CompactibleFreeListSpace::splitDeath(size_t size) {
+  if (size  < SmallForDictionary) {
+    smallSplitDeath(size);
+  } else {
+    dictionary()->dict_census_update(size,
+                                   true /* split */,
+                                   false /* birth */);
+  }
+}
+
+void CompactibleFreeListSpace::split(size_t from, size_t to1) {
+  size_t to2 = from - to1;
+  splitDeath(from);
+  split_birth(to1);
+  split_birth(to2);
+}
+
+void CompactibleFreeListSpace::print() const {
+  print_on(tty);
+}
+
+void CompactibleFreeListSpace::prepare_for_verify() {
+  assert_locked();
+  repairLinearAllocationBlocks();
+  // Verify that the SpoolBlocks look like free blocks of
+  // appropriate sizes... To be done ...
+}
+
+class VerifyAllBlksClosure: public BlkClosure {
+ private:
+  const CompactibleFreeListSpace* _sp;
+  const MemRegion                 _span;
+  HeapWord*                       _last_addr;
+  size_t                          _last_size;
+  bool                            _last_was_obj;
+  bool                            _last_was_live;
+
+ public:
+  VerifyAllBlksClosure(const CompactibleFreeListSpace* sp,
+    MemRegion span) :  _sp(sp), _span(span),
+                       _last_addr(NULL), _last_size(0),
+                       _last_was_obj(false), _last_was_live(false) { }
+
+  virtual size_t do_blk(HeapWord* addr) {
+    size_t res;
+    bool   was_obj  = false;
+    bool   was_live = false;
+    if (_sp->block_is_obj(addr)) {
+      was_obj = true;
+      oop p = oop(addr);
+      guarantee(oopDesc::is_oop(p), "Should be an oop");
+      res = _sp->adjustObjectSize(p->size());
+      if (_sp->obj_is_alive(addr)) {
+        was_live = true;
+        p->verify();
+      }
+    } else {
+      FreeChunk* fc = (FreeChunk*)addr;
+      res = fc->size();
+      if (FLSVerifyLists && !fc->cantCoalesce()) {
+        guarantee(_sp->verify_chunk_in_free_list(fc),
+                  "Chunk should be on a free list");
+      }
+    }
+    if (res == 0) {
+      Log(gc, verify) log;
+      log.error("Livelock: no rank reduction!");
+      log.error(" Current:  addr = " PTR_FORMAT ", size = " SIZE_FORMAT ", obj = %s, live = %s \n"
+                " Previous: addr = " PTR_FORMAT ", size = " SIZE_FORMAT ", obj = %s, live = %s \n",
+        p2i(addr),       res,        was_obj      ?"true":"false", was_live      ?"true":"false",
+        p2i(_last_addr), _last_size, _last_was_obj?"true":"false", _last_was_live?"true":"false");
+      LogStream ls(log.error());
+      _sp->print_on(&ls);
+      guarantee(false, "Verification failed.");
+    }
+    _last_addr = addr;
+    _last_size = res;
+    _last_was_obj  = was_obj;
+    _last_was_live = was_live;
+    return res;
+  }
+};
+
+class VerifyAllOopsClosure: public BasicOopIterateClosure {
+ private:
+  const CMSCollector*             _collector;
+  const CompactibleFreeListSpace* _sp;
+  const MemRegion                 _span;
+  const bool                      _past_remark;
+  const CMSBitMap*                _bit_map;
+
+ protected:
+  void do_oop(void* p, oop obj) {
+    if (_span.contains(obj)) { // the interior oop points into CMS heap
+      if (!_span.contains(p)) { // reference from outside CMS heap
+        // Should be a valid object; the first disjunct below allows
+        // us to sidestep an assertion in block_is_obj() that insists
+        // that p be in _sp. Note that several generations (and spaces)
+        // are spanned by _span (CMS heap) above.
+        guarantee(!_sp->is_in_reserved(obj) ||
+                  _sp->block_is_obj((HeapWord*)obj),
+                  "Should be an object");
+        guarantee(oopDesc::is_oop(obj), "Should be an oop");
+        obj->verify();
+        if (_past_remark) {
+          // Remark has been completed, the object should be marked
+          _bit_map->isMarked((HeapWord*)obj);
+        }
+      } else { // reference within CMS heap
+        if (_past_remark) {
+          // Remark has been completed -- so the referent should have
+          // been marked, if referring object is.
+          if (_bit_map->isMarked(_collector->block_start(p))) {
+            guarantee(_bit_map->isMarked((HeapWord*)obj), "Marking error?");
+          }
+        }
+      }
+    } else if (_sp->is_in_reserved(p)) {
+      // the reference is from FLS, and points out of FLS
+      guarantee(oopDesc::is_oop(obj), "Should be an oop");
+      obj->verify();
+    }
+  }
+
+  template <class T> void do_oop_work(T* p) {
+    T heap_oop = RawAccess<>::oop_load(p);
+    if (!CompressedOops::is_null(heap_oop)) {
+      oop obj = CompressedOops::decode_not_null(heap_oop);
+      do_oop(p, obj);
+    }
+  }
+
+ public:
+  VerifyAllOopsClosure(const CMSCollector* collector,
+    const CompactibleFreeListSpace* sp, MemRegion span,
+    bool past_remark, CMSBitMap* bit_map) :
+    _collector(collector), _sp(sp), _span(span),
+    _past_remark(past_remark), _bit_map(bit_map) { }
+
+  virtual void do_oop(oop* p)       { VerifyAllOopsClosure::do_oop_work(p); }
+  virtual void do_oop(narrowOop* p) { VerifyAllOopsClosure::do_oop_work(p); }
+};
+
+void CompactibleFreeListSpace::verify() const {
+  assert_lock_strong(&_freelistLock);
+  verify_objects_initialized();
+  MemRegion span = _collector->_span;
+  bool past_remark = (_collector->abstract_state() ==
+                      CMSCollector::Sweeping);
+
+  ResourceMark rm;
+  HandleMark  hm;
+
+  // Check integrity of CFL data structures
+  _promoInfo.verify();
+  _dictionary->verify();
+  if (FLSVerifyIndexTable) {
+    verifyIndexedFreeLists();
+  }
+  // Check integrity of all objects and free blocks in space
+  {
+    VerifyAllBlksClosure cl(this, span);
+    ((CompactibleFreeListSpace*)this)->blk_iterate(&cl);  // cast off const
+  }
+  // Check that all references in the heap to FLS
+  // are to valid objects in FLS or that references in
+  // FLS are to valid objects elsewhere in the heap
+  if (FLSVerifyAllHeapReferences)
+  {
+    VerifyAllOopsClosure cl(_collector, this, span, past_remark,
+      _collector->markBitMap());
+
+    // Iterate over all oops in the heap.
+    CMSHeap::heap()->oop_iterate(&cl);
+  }
+
+  if (VerifyObjectStartArray) {
+    // Verify the block offset table
+    _bt.verify();
+  }
+}
+
+#ifndef PRODUCT
+void CompactibleFreeListSpace::verifyFreeLists() const {
+  if (FLSVerifyLists) {
+    _dictionary->verify();
+    verifyIndexedFreeLists();
+  } else {
+    if (FLSVerifyDictionary) {
+      _dictionary->verify();
+    }
+    if (FLSVerifyIndexTable) {
+      verifyIndexedFreeLists();
+    }
+  }
+}
+#endif
+
+void CompactibleFreeListSpace::verifyIndexedFreeLists() const {
+  size_t i = 0;
+  for (; i < IndexSetStart; i++) {
+    guarantee(_indexedFreeList[i].head() == NULL, "should be NULL");
+  }
+  for (; i < IndexSetSize; i++) {
+    verifyIndexedFreeList(i);
+  }
+}
+
+void CompactibleFreeListSpace::verifyIndexedFreeList(size_t size) const {
+  FreeChunk* fc   =  _indexedFreeList[size].head();
+  FreeChunk* tail =  _indexedFreeList[size].tail();
+  size_t    num = _indexedFreeList[size].count();
+  size_t      n = 0;
+  guarantee(((size >= IndexSetStart) && (size % IndexSetStride == 0)) || fc == NULL,
+            "Slot should have been empty");
+  for (; fc != NULL; fc = fc->next(), n++) {
+    guarantee(fc->size() == size, "Size inconsistency");
+    guarantee(fc->is_free(), "!free?");
+    guarantee(fc->next() == NULL || fc->next()->prev() == fc, "Broken list");
+    guarantee((fc->next() == NULL) == (fc == tail), "Incorrect tail");
+  }
+  guarantee(n == num, "Incorrect count");
+}
+
+#ifndef PRODUCT
+void CompactibleFreeListSpace::check_free_list_consistency() const {
+  assert((TreeChunk<FreeChunk, AdaptiveFreeList<FreeChunk> >::min_size() <= IndexSetSize),
+    "Some sizes can't be allocated without recourse to"
+    " linear allocation buffers");
+  assert((TreeChunk<FreeChunk, AdaptiveFreeList<FreeChunk> >::min_size()*HeapWordSize == sizeof(TreeChunk<FreeChunk, AdaptiveFreeList<FreeChunk> >)),
+    "else MIN_TREE_CHUNK_SIZE is wrong");
+  assert(IndexSetStart != 0, "IndexSetStart not initialized");
+  assert(IndexSetStride != 0, "IndexSetStride not initialized");
+}
+#endif
+
+void CompactibleFreeListSpace::printFLCensus(size_t sweep_count) const {
+  assert_lock_strong(&_freelistLock);
+  LogTarget(Debug, gc, freelist, census) log;
+  if (!log.is_enabled()) {
+    return;
+  }
+  AdaptiveFreeList<FreeChunk> total;
+  log.print("end sweep# " SIZE_FORMAT, sweep_count);
+  ResourceMark rm;
+  LogStream ls(log);
+  outputStream* out = &ls;
+  AdaptiveFreeList<FreeChunk>::print_labels_on(out, "size");
+  size_t total_free = 0;
+  for (size_t i = IndexSetStart; i < IndexSetSize; i += IndexSetStride) {
+    const AdaptiveFreeList<FreeChunk> *fl = &_indexedFreeList[i];
+    total_free += fl->count() * fl->size();
+    if (i % (40*IndexSetStride) == 0) {
+      AdaptiveFreeList<FreeChunk>::print_labels_on(out, "size");
+    }
+    fl->print_on(out);
+    total.set_bfr_surp(    total.bfr_surp()     + fl->bfr_surp()    );
+    total.set_surplus(    total.surplus()     + fl->surplus()    );
+    total.set_desired(    total.desired()     + fl->desired()    );
+    total.set_prev_sweep(  total.prev_sweep()   + fl->prev_sweep()  );
+    total.set_before_sweep(total.before_sweep() + fl->before_sweep());
+    total.set_count(      total.count()       + fl->count()      );
+    total.set_coal_births( total.coal_births()  + fl->coal_births() );
+    total.set_coal_deaths( total.coal_deaths()  + fl->coal_deaths() );
+    total.set_split_births(total.split_births() + fl->split_births());
+    total.set_split_deaths(total.split_deaths() + fl->split_deaths());
+  }
+  total.print_on(out, "TOTAL");
+  log.print("Total free in indexed lists " SIZE_FORMAT " words", total_free);
+  log.print("growth: %8.5f  deficit: %8.5f",
+            (double)(total.split_births()+total.coal_births()-total.split_deaths()-total.coal_deaths())/
+                    (total.prev_sweep() != 0 ? (double)total.prev_sweep() : 1.0),
+            (double)(total.desired() - total.count())/(total.desired() != 0 ? (double)total.desired() : 1.0));
+  _dictionary->print_dict_census(out);
+}
+
+///////////////////////////////////////////////////////////////////////////
+// CompactibleFreeListSpaceLAB
+///////////////////////////////////////////////////////////////////////////
+
+#define VECTOR_257(x)                                                                                  \
+  /* 1  2  3  4  5  6  7  8  9 1x 11 12 13 14 15 16 17 18 19 2x 21 22 23 24 25 26 27 28 29 3x 31 32 */ \
+  {  x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,   \
+     x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,   \
+     x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,   \
+     x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,   \
+     x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,   \
+     x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,   \
+     x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,   \
+     x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,   \
+     x }
+
+// Initialize with default setting for CMS, _not_
+// generic OldPLABSize, whose static default is different; if overridden at the
+// command-line, this will get reinitialized via a call to
+// modify_initialization() below.
+AdaptiveWeightedAverage CompactibleFreeListSpaceLAB::_blocks_to_claim[]    =
+  VECTOR_257(AdaptiveWeightedAverage(OldPLABWeight, (float)CompactibleFreeListSpaceLAB::_default_dynamic_old_plab_size));
+size_t CompactibleFreeListSpaceLAB::_global_num_blocks[]  = VECTOR_257(0);
+uint   CompactibleFreeListSpaceLAB::_global_num_workers[] = VECTOR_257(0);
+
+CompactibleFreeListSpaceLAB::CompactibleFreeListSpaceLAB(CompactibleFreeListSpace* cfls) :
+  _cfls(cfls)
+{
+  assert(CompactibleFreeListSpace::IndexSetSize == 257, "Modify VECTOR_257() macro above");
+  for (size_t i = CompactibleFreeListSpace::IndexSetStart;
+       i < CompactibleFreeListSpace::IndexSetSize;
+       i += CompactibleFreeListSpace::IndexSetStride) {
+    _indexedFreeList[i].set_size(i);
+    _num_blocks[i] = 0;
+  }
+}
+
+static bool _CFLS_LAB_modified = false;
+
+void CompactibleFreeListSpaceLAB::modify_initialization(size_t n, unsigned wt) {
+  assert(!_CFLS_LAB_modified, "Call only once");
+  _CFLS_LAB_modified = true;
+  for (size_t i = CompactibleFreeListSpace::IndexSetStart;
+       i < CompactibleFreeListSpace::IndexSetSize;
+       i += CompactibleFreeListSpace::IndexSetStride) {
+    _blocks_to_claim[i].modify(n, wt, true /* force */);
+  }
+}
+
+HeapWord* CompactibleFreeListSpaceLAB::alloc(size_t word_sz) {
+  FreeChunk* res;
+  assert(word_sz == _cfls->adjustObjectSize(word_sz), "Error");
+  if (word_sz >=  CompactibleFreeListSpace::IndexSetSize) {
+    // This locking manages sync with other large object allocations.
+    MutexLockerEx x(_cfls->parDictionaryAllocLock(),
+                    Mutex::_no_safepoint_check_flag);
+    res = _cfls->getChunkFromDictionaryExact(word_sz);
+    if (res == NULL) return NULL;
+  } else {
+    AdaptiveFreeList<FreeChunk>* fl = &_indexedFreeList[word_sz];
+    if (fl->count() == 0) {
+      // Attempt to refill this local free list.
+      get_from_global_pool(word_sz, fl);
+      // If it didn't work, give up.
+      if (fl->count() == 0) return NULL;
+    }
+    res = fl->get_chunk_at_head();
+    assert(res != NULL, "Why was count non-zero?");
+  }
+  res->markNotFree();
+  assert(!res->is_free(), "shouldn't be marked free");
+  assert(oop(res)->klass_or_null() == NULL, "should look uninitialized");
+  // mangle a just allocated object with a distinct pattern.
+  debug_only(res->mangleAllocated(word_sz));
+  return (HeapWord*)res;
+}
+
+// Get a chunk of blocks of the right size and update related
+// book-keeping stats
+void CompactibleFreeListSpaceLAB::get_from_global_pool(size_t word_sz, AdaptiveFreeList<FreeChunk>* fl) {
+  // Get the #blocks we want to claim
+  size_t n_blks = (size_t)_blocks_to_claim[word_sz].average();
+  assert(n_blks > 0, "Error");
+  assert(ResizeOldPLAB || n_blks == OldPLABSize, "Error");
+  // In some cases, when the application has a phase change,
+  // there may be a sudden and sharp shift in the object survival
+  // profile, and updating the counts at the end of a scavenge
+  // may not be quick enough, giving rise to large scavenge pauses
+  // during these phase changes. It is beneficial to detect such
+  // changes on-the-fly during a scavenge and avoid such a phase-change
+  // pothole. The following code is a heuristic attempt to do that.
+  // It is protected by a product flag until we have gained
+  // enough experience with this heuristic and fine-tuned its behavior.
+  // WARNING: This might increase fragmentation if we overreact to
+  // small spikes, so some kind of historical smoothing based on
+  // previous experience with the greater reactivity might be useful.
+  // Lacking sufficient experience, CMSOldPLABResizeQuicker is disabled by
+  // default.
+  if (ResizeOldPLAB && CMSOldPLABResizeQuicker) {
+    //
+    // On a 32-bit VM, the denominator can become zero because of integer overflow,
+    // which is why there is a cast to double.
+    //
+    size_t multiple = (size_t) (_num_blocks[word_sz]/(((double)CMSOldPLABToleranceFactor)*CMSOldPLABNumRefills*n_blks));
+    n_blks +=  CMSOldPLABReactivityFactor*multiple*n_blks;
+    n_blks = MIN2(n_blks, CMSOldPLABMax);
+  }
+  assert(n_blks > 0, "Error");
+  _cfls->par_get_chunk_of_blocks(word_sz, n_blks, fl);
+  // Update stats table entry for this block size
+  _num_blocks[word_sz] += fl->count();
+}
+
+void CompactibleFreeListSpaceLAB::compute_desired_plab_size() {
+  for (size_t i =  CompactibleFreeListSpace::IndexSetStart;
+       i < CompactibleFreeListSpace::IndexSetSize;
+       i += CompactibleFreeListSpace::IndexSetStride) {
+    assert((_global_num_workers[i] == 0) == (_global_num_blocks[i] == 0),
+           "Counter inconsistency");
+    if (_global_num_workers[i] > 0) {
+      // Need to smooth wrt historical average
+      if (ResizeOldPLAB) {
+        _blocks_to_claim[i].sample(
+          MAX2(CMSOldPLABMin,
+          MIN2(CMSOldPLABMax,
+               _global_num_blocks[i]/_global_num_workers[i]/CMSOldPLABNumRefills)));
+      }
+      // Reset counters for next round
+      _global_num_workers[i] = 0;
+      _global_num_blocks[i] = 0;
+      log_trace(gc, plab)("[" SIZE_FORMAT "]: " SIZE_FORMAT, i, (size_t)_blocks_to_claim[i].average());
+    }
+  }
+}
+
+// If this is changed in the future to allow parallel
+// access, one would need to take the FL locks and,
+// depending on how it is used, stagger access from
+// parallel threads to reduce contention.
+void CompactibleFreeListSpaceLAB::retire(int tid) {
+  // We run this single threaded with the world stopped;
+  // so no need for locks and such.
+  NOT_PRODUCT(Thread* t = Thread::current();)
+  assert(Thread::current()->is_VM_thread(), "Error");
+  for (size_t i =  CompactibleFreeListSpace::IndexSetStart;
+       i < CompactibleFreeListSpace::IndexSetSize;
+       i += CompactibleFreeListSpace::IndexSetStride) {
+    assert(_num_blocks[i] >= (size_t)_indexedFreeList[i].count(),
+           "Can't retire more than what we obtained");
+    if (_num_blocks[i] > 0) {
+      size_t num_retire =  _indexedFreeList[i].count();
+      assert(_num_blocks[i] > num_retire, "Should have used at least one");
+      {
+        // MutexLockerEx x(_cfls->_indexedFreeListParLocks[i],
+        //                Mutex::_no_safepoint_check_flag);
+
+        // Update globals stats for num_blocks used
+        _global_num_blocks[i] += (_num_blocks[i] - num_retire);
+        _global_num_workers[i]++;
+        assert(_global_num_workers[i] <= ParallelGCThreads, "Too big");
+        if (num_retire > 0) {
+          _cfls->_indexedFreeList[i].prepend(&_indexedFreeList[i]);
+          // Reset this list.
+          _indexedFreeList[i] = AdaptiveFreeList<FreeChunk>();
+          _indexedFreeList[i].set_size(i);
+        }
+      }
+      log_trace(gc, plab)("%d[" SIZE_FORMAT "]: " SIZE_FORMAT "/" SIZE_FORMAT "/" SIZE_FORMAT,
+                          tid, i, num_retire, _num_blocks[i], (size_t)_blocks_to_claim[i].average());
+      // Reset stats for next round
+      _num_blocks[i]         = 0;
+    }
+  }
+}
+
+// Used by par_get_chunk_of_blocks() for the chunks from the
+// indexed_free_lists.  Looks for a chunk with size that is a multiple
+// of "word_sz" and if found, splits it into "word_sz" chunks and add
+// to the free list "fl".  "n" is the maximum number of chunks to
+// be added to "fl".
+bool CompactibleFreeListSpace:: par_get_chunk_of_blocks_IFL(size_t word_sz, size_t n, AdaptiveFreeList<FreeChunk>* fl) {
+
+  // We'll try all multiples of word_sz in the indexed set, starting with
+  // word_sz itself and, if CMSSplitIndexedFreeListBlocks, try larger multiples,
+  // then try getting a big chunk and splitting it.
+  {
+    bool found;
+    int  k;
+    size_t cur_sz;
+    for (k = 1, cur_sz = k * word_sz, found = false;
+         (cur_sz < CompactibleFreeListSpace::IndexSetSize) &&
+         (CMSSplitIndexedFreeListBlocks || k <= 1);
+         k++, cur_sz = k * word_sz) {
+      AdaptiveFreeList<FreeChunk> fl_for_cur_sz;  // Empty.
+      fl_for_cur_sz.set_size(cur_sz);
+      {
+        MutexLockerEx x(_indexedFreeListParLocks[cur_sz],
+                        Mutex::_no_safepoint_check_flag);
+        AdaptiveFreeList<FreeChunk>* gfl = &_indexedFreeList[cur_sz];
+        if (gfl->count() != 0) {
+          // nn is the number of chunks of size cur_sz that
+          // we'd need to split k-ways each, in order to create
+          // "n" chunks of size word_sz each.
+          const size_t nn = MAX2(n/k, (size_t)1);
+          gfl->getFirstNChunksFromList(nn, &fl_for_cur_sz);
+          found = true;
+          if (k > 1) {
+            // Update split death stats for the cur_sz-size blocks list:
+            // we increment the split death count by the number of blocks
+            // we just took from the cur_sz-size blocks list and which
+            // we will be splitting below.
+            ssize_t deaths = gfl->split_deaths() +
+                             fl_for_cur_sz.count();
+            gfl->set_split_deaths(deaths);
+          }
+        }
+      }
+      // Now transfer fl_for_cur_sz to fl.  Common case, we hope, is k = 1.
+      if (found) {
+        if (k == 1) {
+          fl->prepend(&fl_for_cur_sz);
+        } else {
+          // Divide each block on fl_for_cur_sz up k ways.
+          FreeChunk* fc;
+          while ((fc = fl_for_cur_sz.get_chunk_at_head()) != NULL) {
+            // Must do this in reverse order, so that anybody attempting to
+            // access the main chunk sees it as a single free block until we
+            // change it.
+            size_t fc_size = fc->size();
+            assert(fc->is_free(), "Error");
+            for (int i = k-1; i >= 0; i--) {
+              FreeChunk* ffc = (FreeChunk*)((HeapWord*)fc + i * word_sz);
+              assert((i != 0) ||
+                        ((fc == ffc) && ffc->is_free() &&
+                         (ffc->size() == k*word_sz) && (fc_size == word_sz)),
+                        "Counting error");
+              ffc->set_size(word_sz);
+              ffc->link_prev(NULL); // Mark as a free block for other (parallel) GC threads.
+              ffc->link_next(NULL);
+              // Above must occur before BOT is updated below.
+              OrderAccess::storestore();
+              // splitting from the right, fc_size == i * word_sz
+              _bt.mark_block((HeapWord*)ffc, word_sz, true /* reducing */);
+              fc_size -= word_sz;
+              assert(fc_size == i*word_sz, "Error");
+              _bt.verify_not_unallocated((HeapWord*)ffc, word_sz);
+              _bt.verify_single_block((HeapWord*)fc, fc_size);
+              _bt.verify_single_block((HeapWord*)ffc, word_sz);
+              // Push this on "fl".
+              fl->return_chunk_at_head(ffc);
+            }
+            // TRAP
+            assert(fl->tail()->next() == NULL, "List invariant.");
+          }
+        }
+        // Update birth stats for this block size.
+        size_t num = fl->count();
+        MutexLockerEx x(_indexedFreeListParLocks[word_sz],
+                        Mutex::_no_safepoint_check_flag);
+        ssize_t births = _indexedFreeList[word_sz].split_births() + num;
+        _indexedFreeList[word_sz].set_split_births(births);
+        return true;
+      }
+    }
+    return found;
+  }
+}
+
+FreeChunk* CompactibleFreeListSpace::get_n_way_chunk_to_split(size_t word_sz, size_t n) {
+
+  FreeChunk* fc = NULL;
+  FreeChunk* rem_fc = NULL;
+  size_t rem;
+  {
+    MutexLockerEx x(parDictionaryAllocLock(),
+                    Mutex::_no_safepoint_check_flag);
+    while (n > 0) {
+      fc = dictionary()->get_chunk(MAX2(n * word_sz, _dictionary->min_size()));
+      if (fc != NULL) {
+        break;
+      } else {
+        n--;
+      }
+    }
+    if (fc == NULL) return NULL;
+    // Otherwise, split up that block.
+    assert((ssize_t)n >= 1, "Control point invariant");
+    assert(fc->is_free(), "Error: should be a free block");
+    _bt.verify_single_block((HeapWord*)fc, fc->size());
+    const size_t nn = fc->size() / word_sz;
+    n = MIN2(nn, n);
+    assert((ssize_t)n >= 1, "Control point invariant");
+    rem = fc->size() - n * word_sz;
+    // If there is a remainder, and it's too small, allocate one fewer.
+    if (rem > 0 && rem < MinChunkSize) {
+      n--; rem += word_sz;
+    }
+    // Note that at this point we may have n == 0.
+    assert((ssize_t)n >= 0, "Control point invariant");
+
+    // If n is 0, the chunk fc that was found is not large
+    // enough to leave a viable remainder.  We are unable to
+    // allocate even one block.  Return fc to the
+    // dictionary and return, leaving "fl" empty.
+    if (n == 0) {
+      returnChunkToDictionary(fc);
+      return NULL;
+    }
+
+    _bt.allocated((HeapWord*)fc, fc->size(), true /* reducing */);  // update _unallocated_blk
+    dictionary()->dict_census_update(fc->size(),
+                                     true /*split*/,
+                                     false /*birth*/);
+
+    // First return the remainder, if any.
+    // Note that we hold the lock until we decide if we're going to give
+    // back the remainder to the dictionary, since a concurrent allocation
+    // may otherwise see the heap as empty.  (We're willing to take that
+    // hit if the block is a small block.)
+    if (rem > 0) {
+      size_t prefix_size = n * word_sz;
+      rem_fc = (FreeChunk*)((HeapWord*)fc + prefix_size);
+      rem_fc->set_size(rem);
+      rem_fc->link_prev(NULL); // Mark as a free block for other (parallel) GC threads.
+      rem_fc->link_next(NULL);
+      // Above must occur before BOT is updated below.
+      assert((ssize_t)n > 0 && prefix_size > 0 && rem_fc > fc, "Error");
+      OrderAccess::storestore();
+      _bt.split_block((HeapWord*)fc, fc->size(), prefix_size);
+      assert(fc->is_free(), "Error");
+      fc->set_size(prefix_size);
+      if (rem >= IndexSetSize) {
+        returnChunkToDictionary(rem_fc);
+        dictionary()->dict_census_update(rem, true /*split*/, true /*birth*/);
+        rem_fc = NULL;
+      }
+      // Otherwise, return it to the small list below.
+    }
+  }
+  if (rem_fc != NULL) {
+    MutexLockerEx x(_indexedFreeListParLocks[rem],
+                    Mutex::_no_safepoint_check_flag);
+    _bt.verify_not_unallocated((HeapWord*)rem_fc, rem_fc->size());
+    _indexedFreeList[rem].return_chunk_at_head(rem_fc);
+    smallSplitBirth(rem);
+  }
+  assert(n * word_sz == fc->size(),
+         "Chunk size " SIZE_FORMAT " is not exactly splittable by "
+         SIZE_FORMAT " sized chunks of size " SIZE_FORMAT,
+         fc->size(), n, word_sz);
+  return fc;
+}
+
+void CompactibleFreeListSpace:: par_get_chunk_of_blocks_dictionary(size_t word_sz, size_t targetted_number_of_chunks, AdaptiveFreeList<FreeChunk>* fl) {
+
+  FreeChunk* fc = get_n_way_chunk_to_split(word_sz, targetted_number_of_chunks);
+
+  if (fc == NULL) {
+    return;
+  }
+
+  size_t n = fc->size() / word_sz;
+
+  assert((ssize_t)n > 0, "Consistency");
+  // Now do the splitting up.
+  // Must do this in reverse order, so that anybody attempting to
+  // access the main chunk sees it as a single free block until we
+  // change it.
+  size_t fc_size = n * word_sz;
+  // All but first chunk in this loop
+  for (ssize_t i = n-1; i > 0; i--) {
+    FreeChunk* ffc = (FreeChunk*)((HeapWord*)fc + i * word_sz);
+    ffc->set_size(word_sz);
+    ffc->link_prev(NULL); // Mark as a free block for other (parallel) GC threads.
+    ffc->link_next(NULL);
+    // Above must occur before BOT is updated below.
+    OrderAccess::storestore();
+    // splitting from the right, fc_size == (n - i + 1) * wordsize
+    _bt.mark_block((HeapWord*)ffc, word_sz, true /* reducing */);
+    fc_size -= word_sz;
+    _bt.verify_not_unallocated((HeapWord*)ffc, ffc->size());
+    _bt.verify_single_block((HeapWord*)ffc, ffc->size());
+    _bt.verify_single_block((HeapWord*)fc, fc_size);
+    // Push this on "fl".
+    fl->return_chunk_at_head(ffc);
+  }
+  // First chunk
+  assert(fc->is_free() && fc->size() == n*word_sz, "Error: should still be a free block");
+  // The blocks above should show their new sizes before the first block below
+  fc->set_size(word_sz);
+  fc->link_prev(NULL);    // idempotent wrt free-ness, see assert above
+  fc->link_next(NULL);
+  _bt.verify_not_unallocated((HeapWord*)fc, fc->size());
+  _bt.verify_single_block((HeapWord*)fc, fc->size());
+  fl->return_chunk_at_head(fc);
+
+  assert((ssize_t)n > 0 && (ssize_t)n == fl->count(), "Incorrect number of blocks");
+  {
+    // Update the stats for this block size.
+    MutexLockerEx x(_indexedFreeListParLocks[word_sz],
+                    Mutex::_no_safepoint_check_flag);
+    const ssize_t births = _indexedFreeList[word_sz].split_births() + n;
+    _indexedFreeList[word_sz].set_split_births(births);
+    // ssize_t new_surplus = _indexedFreeList[word_sz].surplus() + n;
+    // _indexedFreeList[word_sz].set_surplus(new_surplus);
+  }
+
+  // TRAP
+  assert(fl->tail()->next() == NULL, "List invariant.");
+}
+
+void CompactibleFreeListSpace:: par_get_chunk_of_blocks(size_t word_sz, size_t n, AdaptiveFreeList<FreeChunk>* fl) {
+  assert(fl->count() == 0, "Precondition.");
+  assert(word_sz < CompactibleFreeListSpace::IndexSetSize,
+         "Precondition");
+
+  if (par_get_chunk_of_blocks_IFL(word_sz, n, fl)) {
+    // Got it
+    return;
+  }
+
+  // Otherwise, we'll split a block from the dictionary.
+  par_get_chunk_of_blocks_dictionary(word_sz, n, fl);
+}
+
+const size_t CompactibleFreeListSpace::max_flag_size_for_task_size() const {
+  const size_t ergo_max = _old_gen->reserved().word_size() / (CardTable::card_size_in_words * BitsPerWord);
+  return ergo_max;
+}
+
+// Set up the space's par_seq_tasks structure for work claiming
+// for parallel rescan. See CMSParRemarkTask where this is currently used.
+// XXX Need to suitably abstract and generalize this and the next
+// method into one.
+void
+CompactibleFreeListSpace::
+initialize_sequential_subtasks_for_rescan(int n_threads) {
+  // The "size" of each task is fixed according to rescan_task_size.
+  assert(n_threads > 0, "Unexpected n_threads argument");
+  const size_t task_size = rescan_task_size();
+  size_t n_tasks = (used_region().word_size() + task_size - 1)/task_size;
+  assert((n_tasks == 0) == used_region().is_empty(), "n_tasks incorrect");
+  assert(n_tasks == 0 ||
+         ((used_region().start() + (n_tasks - 1)*task_size < used_region().end()) &&
+          (used_region().start() + n_tasks*task_size >= used_region().end())),
+         "n_tasks calculation incorrect");
+  SequentialSubTasksDone* pst = conc_par_seq_tasks();
+  assert(!pst->valid(), "Clobbering existing data?");
+  // Sets the condition for completion of the subtask (how many threads
+  // need to finish in order to be done).
+  pst->set_n_threads(n_threads);
+  pst->set_n_tasks((int)n_tasks);
+}
+
+// Set up the space's par_seq_tasks structure for work claiming
+// for parallel concurrent marking. See CMSConcMarkTask where this is currently used.
+void
+CompactibleFreeListSpace::
+initialize_sequential_subtasks_for_marking(int n_threads,
+                                           HeapWord* low) {
+  // The "size" of each task is fixed according to rescan_task_size.
+  assert(n_threads > 0, "Unexpected n_threads argument");
+  const size_t task_size = marking_task_size();
+  assert(task_size > CardTable::card_size_in_words &&
+         (task_size %  CardTable::card_size_in_words == 0),
+         "Otherwise arithmetic below would be incorrect");
+  MemRegion span = _old_gen->reserved();
+  if (low != NULL) {
+    if (span.contains(low)) {
+      // Align low down to  a card boundary so that
+      // we can use block_offset_careful() on span boundaries.
+      HeapWord* aligned_low = align_down(low, CardTable::card_size);
+      // Clip span prefix at aligned_low
+      span = span.intersection(MemRegion(aligned_low, span.end()));
+    } else if (low > span.end()) {
+      span = MemRegion(low, low);  // Null region
+    } // else use entire span
+  }
+  assert(span.is_empty() ||
+         ((uintptr_t)span.start() %  CardTable::card_size == 0),
+        "span should start at a card boundary");
+  size_t n_tasks = (span.word_size() + task_size - 1)/task_size;
+  assert((n_tasks == 0) == span.is_empty(), "Inconsistency");
+  assert(n_tasks == 0 ||
+         ((span.start() + (n_tasks - 1)*task_size < span.end()) &&
+          (span.start() + n_tasks*task_size >= span.end())),
+         "n_tasks calculation incorrect");
+  SequentialSubTasksDone* pst = conc_par_seq_tasks();
+  assert(!pst->valid(), "Clobbering existing data?");
+  // Sets the condition for completion of the subtask (how many threads
+  // need to finish in order to be done).
+  pst->set_n_threads(n_threads);
+  pst->set_n_tasks((int)n_tasks);
+}
diff --git a/src/hotspot/share/gc/cms/compactibleFreeListSpace.hpp b/src/hotspot/share/gc/cms/compactibleFreeListSpace.hpp
new file mode 100644
index 00000000000..9fd2ea58320
--- /dev/null
+++ b/src/hotspot/share/gc/cms/compactibleFreeListSpace.hpp
@@ -0,0 +1,759 @@
+/*
+ * Copyright (c) 2001, 2018, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef SHARE_VM_GC_CMS_COMPACTIBLEFREELISTSPACE_HPP
+#define SHARE_VM_GC_CMS_COMPACTIBLEFREELISTSPACE_HPP
+
+#include "gc/cms/adaptiveFreeList.hpp"
+#include "gc/cms/promotionInfo.hpp"
+#include "gc/shared/blockOffsetTable.hpp"
+#include "gc/shared/cardTable.hpp"
+#include "gc/shared/space.hpp"
+#include "logging/log.hpp"
+#include "memory/binaryTreeDictionary.hpp"
+#include "memory/freeList.hpp"
+
+// Classes in support of keeping track of promotions into a non-Contiguous
+// space, in this case a CompactibleFreeListSpace.
+
+// Forward declarations
+class CMSCollector;
+class CompactibleFreeListSpace;
+class ConcurrentMarkSweepGeneration;
+class BlkClosure;
+class BlkClosureCareful;
+class FreeChunk;
+class UpwardsObjectClosure;
+class ObjectClosureCareful;
+class Klass;
+
+class AFLBinaryTreeDictionary : public BinaryTreeDictionary<FreeChunk, AdaptiveFreeList<FreeChunk> > {
+ public:
+  AFLBinaryTreeDictionary(MemRegion mr)
+      : BinaryTreeDictionary<FreeChunk, AdaptiveFreeList<FreeChunk> >(mr) {}
+
+  // Find the list with size "size" in the binary tree and update
+  // the statistics in the list according to "split" (chunk was
+  // split or coalesce) and "birth" (chunk was added or removed).
+  void       dict_census_update(size_t size, bool split, bool birth);
+  // Return true if the dictionary is overpopulated (more chunks of
+  // this size than desired) for size "size".
+  bool       coal_dict_over_populated(size_t size);
+  // Methods called at the beginning of a sweep to prepare the
+  // statistics for the sweep.
+  void       begin_sweep_dict_census(double coalSurplusPercent,
+                                     float inter_sweep_current,
+                                     float inter_sweep_estimate,
+                                     float intra_sweep_estimate);
+  // Methods called after the end of a sweep to modify the
+  // statistics for the sweep.
+  void       end_sweep_dict_census(double splitSurplusPercent);
+  // Accessors for statistics
+  void       set_tree_surplus(double splitSurplusPercent);
+  void       set_tree_hints(void);
+  // Reset statistics for all the lists in the tree.
+  void       clear_tree_census(void);
+  // Print the statistics for all the lists in the tree.  Also may
+  // print out summaries.
+  void       print_dict_census(outputStream* st) const;
+};
+
+class LinearAllocBlock {
+ public:
+  LinearAllocBlock() : _ptr(0), _word_size(0), _refillSize(0),
+    _allocation_size_limit(0) {}
+  void set(HeapWord* ptr, size_t word_size, size_t refill_size,
+    size_t allocation_size_limit) {
+    _ptr = ptr;
+    _word_size = word_size;
+    _refillSize = refill_size;
+    _allocation_size_limit = allocation_size_limit;
+  }
+  HeapWord* _ptr;
+  size_t    _word_size;
+  size_t    _refillSize;
+  size_t    _allocation_size_limit;  // Largest size that will be allocated
+
+  void print_on(outputStream* st) const;
+};
+
+// Concrete subclass of CompactibleSpace that implements
+// a free list space, such as used in the concurrent mark sweep
+// generation.
+
+class CompactibleFreeListSpace: public CompactibleSpace {
+  friend class VMStructs;
+  friend class ConcurrentMarkSweepGeneration;
+  friend class CMSCollector;
+  // Local alloc buffer for promotion into this space.
+  friend class CompactibleFreeListSpaceLAB;
+  // Allow scan_and_* functions to call (private) overrides of the auxiliary functions on this class
+  template <typename SpaceType>
+  friend void CompactibleSpace::scan_and_adjust_pointers(SpaceType* space);
+  template <typename SpaceType>
+  friend void CompactibleSpace::scan_and_compact(SpaceType* space, bool redefinition_run);
+  template <typename SpaceType>
+  friend void CompactibleSpace::verify_up_to_first_dead(SpaceType* space);
+  template <typename SpaceType>
+  friend void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp, bool redefinition_run);
+
+  // "Size" of chunks of work (executed during parallel remark phases
+  // of CMS collection); this probably belongs in CMSCollector, although
+  // it's cached here because it's used in
+  // initialize_sequential_subtasks_for_rescan() which modifies
+  // par_seq_tasks which also lives in Space. XXX
+  const size_t _rescan_task_size;
+  const size_t _marking_task_size;
+
+  // Yet another sequential tasks done structure. This supports
+  // CMS GC, where we have threads dynamically
+  // claiming sub-tasks from a larger parallel task.
+  SequentialSubTasksDone _conc_par_seq_tasks;
+
+  BlockOffsetArrayNonContigSpace _bt;
+
+  CMSCollector* _collector;
+  ConcurrentMarkSweepGeneration* _old_gen;
+
+  // Data structures for free blocks (used during allocation/sweeping)
+
+  // Allocation is done linearly from two different blocks depending on
+  // whether the request is small or large, in an effort to reduce
+  // fragmentation. We assume that any locking for allocation is done
+  // by the containing generation. Thus, none of the methods in this
+  // space are re-entrant.
+  enum SomeConstants {
+    SmallForLinearAlloc = 16,        // size < this then use _sLAB
+    SmallForDictionary  = 257,       // size < this then use _indexedFreeList
+    IndexSetSize        = SmallForDictionary  // keep this odd-sized
+  };
+  static size_t IndexSetStart;
+  static size_t IndexSetStride;
+  static size_t _min_chunk_size_in_bytes;
+
+ private:
+  enum FitStrategyOptions {
+    FreeBlockStrategyNone = 0,
+    FreeBlockBestFitFirst
+  };
+
+  PromotionInfo _promoInfo;
+
+  // Helps to impose a global total order on freelistLock ranks;
+  // assumes that CFLSpace's are allocated in global total order
+  static int   _lockRank;
+
+  // A lock protecting the free lists and free blocks;
+  // mutable because of ubiquity of locking even for otherwise const methods
+  mutable Mutex _freelistLock;
+
+  // Locking verifier convenience function
+  void assert_locked() const PRODUCT_RETURN;
+  void assert_locked(const Mutex* lock) const PRODUCT_RETURN;
+
+  // Linear allocation blocks
+  LinearAllocBlock _smallLinearAllocBlock;
+
+  AFLBinaryTreeDictionary* _dictionary;    // Pointer to dictionary for large size blocks
+
+  // Indexed array for small size blocks
+  AdaptiveFreeList<FreeChunk> _indexedFreeList[IndexSetSize];
+
+  // Allocation strategy
+  bool _fitStrategy;  // Use best fit strategy
+
+  // This is an address close to the largest free chunk in the heap.
+  // It is currently assumed to be at the end of the heap.  Free
+  // chunks with addresses greater than nearLargestChunk are coalesced
+  // in an effort to maintain a large chunk at the end of the heap.
+  HeapWord*  _nearLargestChunk;
+
+  // Used to keep track of limit of sweep for the space
+  HeapWord* _sweep_limit;
+
+  // Stable value of used().
+  size_t _used_stable;
+
+  // Used to make the young collector update the mod union table
+  MemRegionClosure* _preconsumptionDirtyCardClosure;
+
+  // Support for compacting cms
+  HeapWord* cross_threshold(HeapWord* start, HeapWord* end);
+  HeapWord* forward_compact_top(size_t size, CompactPoint* cp, HeapWord* compact_top);
+  HeapWord* forward(oop q, size_t size, CompactPoint* cp, HeapWord* compact_top);
+
+  // Initialization helpers.
+  void initializeIndexedFreeListArray();
+
+  // Extra stuff to manage promotion parallelism.
+
+  // A lock protecting the dictionary during par promotion allocation.
+  mutable Mutex _parDictionaryAllocLock;
+  Mutex* parDictionaryAllocLock() const { return &_parDictionaryAllocLock; }
+
+  // Locks protecting the exact lists during par promotion allocation.
+  Mutex* _indexedFreeListParLocks[IndexSetSize];
+
+  // Attempt to obtain up to "n" blocks of the size "word_sz" (which is
+  // required to be smaller than "IndexSetSize".)  If successful,
+  // adds them to "fl", which is required to be an empty free list.
+  // If the count of "fl" is negative, it's absolute value indicates a
+  // number of free chunks that had been previously "borrowed" from global
+  // list of size "word_sz", and must now be decremented.
+  void par_get_chunk_of_blocks(size_t word_sz, size_t n, AdaptiveFreeList<FreeChunk>* fl);
+
+  // Used by par_get_chunk_of_blocks() for the chunks from the
+  // indexed_free_lists.
+  bool par_get_chunk_of_blocks_IFL(size_t word_sz, size_t n, AdaptiveFreeList<FreeChunk>* fl);
+
+  // Used by par_get_chunk_of_blocks_dictionary() to get a chunk
+  // evenly splittable into "n" "word_sz" chunks.  Returns that
+  // evenly splittable chunk.  May split a larger chunk to get the
+  // evenly splittable chunk.
+  FreeChunk* get_n_way_chunk_to_split(size_t word_sz, size_t n);
+
+  // Used by par_get_chunk_of_blocks() for the chunks from the
+  // dictionary.
+  void par_get_chunk_of_blocks_dictionary(size_t word_sz, size_t n, AdaptiveFreeList<FreeChunk>* fl);
+
+  // Allocation helper functions
+  // Allocate using a strategy that takes from the indexed free lists
+  // first.  This allocation strategy assumes a companion sweeping
+  // strategy that attempts to keep the needed number of chunks in each
+  // indexed free lists.
+  HeapWord* allocate_adaptive_freelists(size_t size);
+
+  // Gets a chunk from the linear allocation block (LinAB).  If there
+  // is not enough space in the LinAB, refills it.
+  HeapWord*  getChunkFromLinearAllocBlock(LinearAllocBlock* blk, size_t size);
+  HeapWord*  getChunkFromSmallLinearAllocBlock(size_t size);
+  // Get a chunk from the space remaining in the linear allocation block.  Do
+  // not attempt to refill if the space is not available, return NULL.  Do the
+  // repairs on the linear allocation block as appropriate.
+  HeapWord*  getChunkFromLinearAllocBlockRemainder(LinearAllocBlock* blk, size_t size);
+  inline HeapWord*  getChunkFromSmallLinearAllocBlockRemainder(size_t size);
+
+  // Helper function for getChunkFromIndexedFreeList.
+  // Replenish the indexed free list for this "size".  Do not take from an
+  // underpopulated size.
+  FreeChunk*  getChunkFromIndexedFreeListHelper(size_t size, bool replenish = true);
+
+  // Get a chunk from the indexed free list.  If the indexed free list
+  // does not have a free chunk, try to replenish the indexed free list
+  // then get the free chunk from the replenished indexed free list.
+  inline FreeChunk* getChunkFromIndexedFreeList(size_t size);
+
+  // The returned chunk may be larger than requested (or null).
+  FreeChunk* getChunkFromDictionary(size_t size);
+  // The returned chunk is the exact size requested (or null).
+  FreeChunk* getChunkFromDictionaryExact(size_t size);
+
+  // Find a chunk in the indexed free list that is the best
+  // fit for size "numWords".
+  FreeChunk* bestFitSmall(size_t numWords);
+  // For free list "fl" of chunks of size > numWords,
+  // remove a chunk, split off a chunk of size numWords
+  // and return it.  The split off remainder is returned to
+  // the free lists.  The old name for getFromListGreater
+  // was lookInListGreater.
+  FreeChunk* getFromListGreater(AdaptiveFreeList<FreeChunk>* fl, size_t numWords);
+  // Get a chunk in the indexed free list or dictionary,
+  // by considering a larger chunk and splitting it.
+  FreeChunk* getChunkFromGreater(size_t numWords);
+  //  Verify that the given chunk is in the indexed free lists.
+  bool verifyChunkInIndexedFreeLists(FreeChunk* fc) const;
+  // Remove the specified chunk from the indexed free lists.
+  void       removeChunkFromIndexedFreeList(FreeChunk* fc);
+  // Remove the specified chunk from the dictionary.
+  void       removeChunkFromDictionary(FreeChunk* fc);
+  // Split a free chunk into a smaller free chunk of size "new_size".
+  // Return the smaller free chunk and return the remainder to the
+  // free lists.
+  FreeChunk* splitChunkAndReturnRemainder(FreeChunk* chunk, size_t new_size);
+  // Add a chunk to the free lists.
+  void       addChunkToFreeLists(HeapWord* chunk, size_t size);
+  // Add a chunk to the free lists, preferring to suffix it
+  // to the last free chunk at end of space if possible, and
+  // updating the block census stats as well as block offset table.
+  // Take any locks as appropriate if we are multithreaded.
+  void       addChunkToFreeListsAtEndRecordingStats(HeapWord* chunk, size_t size);
+  // Add a free chunk to the indexed free lists.
+  void       returnChunkToFreeList(FreeChunk* chunk);
+  // Add a free chunk to the dictionary.
+  void       returnChunkToDictionary(FreeChunk* chunk);
+
+  // Functions for maintaining the linear allocation buffers (LinAB).
+  // Repairing a linear allocation block refers to operations
+  // performed on the remainder of a LinAB after an allocation
+  // has been made from it.
+  void       repairLinearAllocationBlocks();
+  void       repairLinearAllocBlock(LinearAllocBlock* blk);
+  void       refillLinearAllocBlock(LinearAllocBlock* blk);
+  void       refillLinearAllocBlockIfNeeded(LinearAllocBlock* blk);
+  void       refillLinearAllocBlocksIfNeeded();
+
+  void       verify_objects_initialized() const;
+
+  // Statistics reporting helper functions
+  void       reportFreeListStatistics(const char* title) const;
+  void       reportIndexedFreeListStatistics(outputStream* st) const;
+  size_t     maxChunkSizeInIndexedFreeLists() const;
+  size_t     numFreeBlocksInIndexedFreeLists() const;
+  // Accessor
+  HeapWord* unallocated_block() const {
+    if (BlockOffsetArrayUseUnallocatedBlock) {
+      HeapWord* ub = _bt.unallocated_block();
+      assert(ub >= bottom() &&
+             ub <= end(), "space invariant");
+      return ub;
+    } else {
+      return end();
+    }
+  }
+  void freed(HeapWord* start, size_t size) {
+    _bt.freed(start, size);
+  }
+
+  // Auxiliary functions for scan_and_{forward,adjust_pointers,compact} support.
+  // See comments for CompactibleSpace for more information.
+  inline HeapWord* scan_limit() const {
+    return end();
+  }
+
+  inline bool scanned_block_is_obj(const HeapWord* addr) const {
+    return CompactibleFreeListSpace::block_is_obj(addr); // Avoid virtual call
+  }
+
+  inline size_t scanned_block_size(const HeapWord* addr) const {
+    return CompactibleFreeListSpace::block_size(addr); // Avoid virtual call
+  }
+
+  inline size_t adjust_obj_size(size_t size) const {
+    return adjustObjectSize(size);
+  }
+
+  inline size_t obj_size(const HeapWord* addr) const;
+
+ protected:
+  // Reset the indexed free list to its initial empty condition.
+  void resetIndexedFreeListArray();
+  // Reset to an initial state with a single free block described
+  // by the MemRegion parameter.
+  void reset(MemRegion mr);
+  // Return the total number of words in the indexed free lists.
+  size_t     totalSizeInIndexedFreeLists() const;
+
+ public:
+  // Constructor
+  CompactibleFreeListSpace(BlockOffsetSharedArray* bs, MemRegion mr);
+  // Accessors
+  bool bestFitFirst() { return _fitStrategy == FreeBlockBestFitFirst; }
+  AFLBinaryTreeDictionary* dictionary() const { return _dictionary; }
+  HeapWord* nearLargestChunk() const { return _nearLargestChunk; }
+  void set_nearLargestChunk(HeapWord* v) { _nearLargestChunk = v; }
+
+  // Set CMS global values.
+  static void set_cms_values();
+
+  // Return the free chunk at the end of the space.  If no such
+  // chunk exists, return NULL.
+  FreeChunk* find_chunk_at_end();
+
+  void set_collector(CMSCollector* collector) { _collector = collector; }
+
+  // Support for parallelization of rescan and marking.
+  const size_t rescan_task_size()  const { return _rescan_task_size;  }
+  const size_t marking_task_size() const { return _marking_task_size; }
+  // Return ergonomic max size for CMSRescanMultiple and CMSConcMarkMultiple.
+  const size_t max_flag_size_for_task_size() const;
+  SequentialSubTasksDone* conc_par_seq_tasks() {return &_conc_par_seq_tasks; }
+  void initialize_sequential_subtasks_for_rescan(int n_threads);
+  void initialize_sequential_subtasks_for_marking(int n_threads,
+         HeapWord* low = NULL);
+
+  virtual MemRegionClosure* preconsumptionDirtyCardClosure() const {
+    return _preconsumptionDirtyCardClosure;
+  }
+
+  void setPreconsumptionDirtyCardClosure(MemRegionClosure* cl) {
+    _preconsumptionDirtyCardClosure = cl;
+  }
+
+  // Space enquiries
+  size_t used() const;
+  size_t free() const;
+  size_t max_alloc_in_words() const;
+  // XXX: should have a less conservative used_region() than that of
+  // Space; we could consider keeping track of highest allocated
+  // address and correcting that at each sweep, as the sweeper
+  // goes through the entire allocated part of the generation. We
+  // could also use that information to keep the sweeper from
+  // sweeping more than is necessary. The allocator and sweeper will
+  // of course need to synchronize on this, since the sweeper will
+  // try to bump down the address and the allocator will try to bump it up.
+  // For now, however, we'll just use the default used_region()
+  // which overestimates the region by returning the entire
+  // committed region (this is safe, but inefficient).
+
+  // Returns monotonically increasing stable used space bytes for CMS.
+  // This is required for jstat and other memory monitoring tools
+  // that might otherwise see inconsistent used space values during a garbage
+  // collection, promotion or allocation into compactibleFreeListSpace.
+  // The value returned by this function might be smaller than the
+  // actual value.
+  size_t used_stable() const;
+  // Recalculate and cache the current stable used() value. Only to be called
+  // in places where we can be sure that the result is stable.
+  void recalculate_used_stable();
+
+  // Returns a subregion of the space containing all the objects in
+  // the space.
+  MemRegion used_region() const {
+    return MemRegion(bottom(),
+                     BlockOffsetArrayUseUnallocatedBlock ?
+                     unallocated_block() : end());
+  }
+
+  virtual bool is_free_block(const HeapWord* p) const;
+
+  // Resizing support
+  void set_end(HeapWord* value);  // override
+
+  // Never mangle CompactibleFreeListSpace
+  void mangle_unused_area() {}
+  void mangle_unused_area_complete() {}
+
+  // Mutual exclusion support
+  Mutex* freelistLock() const { return &_freelistLock; }
+
+  // Iteration support
+  void oop_iterate(OopIterateClosure* cl);
+
+  void object_iterate(ObjectClosure* blk);
+  // Apply the closure to each object in the space whose references
+  // point to objects in the heap.  The usage of CompactibleFreeListSpace
+  // by the ConcurrentMarkSweepGeneration for concurrent GC's allows
+  // objects in the space with references to objects that are no longer
+  // valid.  For example, an object may reference another object
+  // that has already been sweep up (collected).  This method uses
+  // obj_is_alive() to determine whether it is safe to iterate of
+  // an object.
+  void safe_object_iterate(ObjectClosure* blk);
+
+  // Iterate over all objects that intersect with mr, calling "cl->do_object"
+  // on each.  There is an exception to this: if this closure has already
+  // been invoked on an object, it may skip such objects in some cases.  This is
+  // Most likely to happen in an "upwards" (ascending address) iteration of
+  // MemRegions.
+  void object_iterate_mem(MemRegion mr, UpwardsObjectClosure* cl);
+
+  // Requires that "mr" be entirely within the space.
+  // Apply "cl->do_object" to all objects that intersect with "mr".
+  // If the iteration encounters an unparseable portion of the region,
+  // terminate the iteration and return the address of the start of the
+  // subregion that isn't done.  Return of "NULL" indicates that the
+  // iteration completed.
+  HeapWord* object_iterate_careful_m(MemRegion mr,
+                                     ObjectClosureCareful* cl);
+
+  // Override: provides a DCTO_CL specific to this kind of space.
+  DirtyCardToOopClosure* new_dcto_cl(OopIterateClosure* cl,
+                                     CardTable::PrecisionStyle precision,
+                                     HeapWord* boundary,
+                                     bool parallel);
+
+  void blk_iterate(BlkClosure* cl);
+  void blk_iterate_careful(BlkClosureCareful* cl);
+  HeapWord* block_start_const(const void* p) const;
+  HeapWord* block_start_careful(const void* p) const;
+  size_t block_size(const HeapWord* p) const;
+  size_t block_size_no_stall(HeapWord* p, const CMSCollector* c) const;
+  bool block_is_obj(const HeapWord* p) const;
+  bool obj_is_alive(const HeapWord* p) const;
+  size_t block_size_nopar(const HeapWord* p) const;
+  bool block_is_obj_nopar(const HeapWord* p) const;
+
+  // Iteration support for promotion
+  void save_marks();
+  bool no_allocs_since_save_marks();
+
+  // Iteration support for sweeping
+  void save_sweep_limit() {
+    _sweep_limit = BlockOffsetArrayUseUnallocatedBlock ?
+                   unallocated_block() : end();
+    log_develop_trace(gc, sweep)(">>>>> Saving sweep limit " PTR_FORMAT
+                                 "  for space [" PTR_FORMAT "," PTR_FORMAT ") <<<<<<",
+                                 p2i(_sweep_limit), p2i(bottom()), p2i(end()));
+  }
+  NOT_PRODUCT(
+    void clear_sweep_limit() { _sweep_limit = NULL; }
+  )
+  HeapWord* sweep_limit() { return _sweep_limit; }
+
+  // Apply "blk->do_oop" to the addresses of all reference fields in objects
+  // promoted into this generation since the most recent save_marks() call.
+  // Fields in objects allocated by applications of the closure
+  // *are* included in the iteration. Thus, when the iteration completes
+  // there should be no further such objects remaining.
+  template <typename OopClosureType>
+  void oop_since_save_marks_iterate(OopClosureType* blk);
+
+  // Allocation support
+  HeapWord* allocate(size_t size);
+  HeapWord* par_allocate(size_t size);
+
+  oop       promote(oop obj, size_t obj_size);
+  void      gc_prologue();
+  void      gc_epilogue();
+
+  // This call is used by a containing CMS generation / collector
+  // to inform the CFLS space that a sweep has been completed
+  // and that the space can do any related house-keeping functions.
+  void      sweep_completed();
+
+  // For an object in this space, the mark-word's two
+  // LSB's having the value [11] indicates that it has been
+  // promoted since the most recent call to save_marks() on
+  // this generation and has not subsequently been iterated
+  // over (using oop_since_save_marks_iterate() above).
+  // This property holds only for single-threaded collections,
+  // and is typically used for Cheney scans; for MT scavenges,
+  // the property holds for all objects promoted during that
+  // scavenge for the duration of the scavenge and is used
+  // by card-scanning to avoid scanning objects (being) promoted
+  // during that scavenge.
+  bool obj_allocated_since_save_marks(const oop obj) const {
+    assert(is_in_reserved(obj), "Wrong space?");
+    return ((PromotedObject*)obj)->hasPromotedMark();
+  }
+
+  // A worst-case estimate of the space required (in HeapWords) to expand the
+  // heap when promoting an obj of size obj_size.
+  size_t expansionSpaceRequired(size_t obj_size) const;
+
+  FreeChunk* allocateScratch(size_t size);
+
+  // Returns true if either the small or large linear allocation buffer is empty.
+  bool       linearAllocationWouldFail() const;
+
+  // Adjust the chunk for the minimum size.  This version is called in
+  // most cases in CompactibleFreeListSpace methods.
+  inline static size_t adjustObjectSize(size_t size) {
+    return align_object_size(MAX2(size, (size_t)MinChunkSize));
+  }
+  // This is a virtual version of adjustObjectSize() that is called
+  // only occasionally when the compaction space changes and the type
+  // of the new compaction space is is only known to be CompactibleSpace.
+  size_t adjust_object_size_v(size_t size) const {
+    return adjustObjectSize(size);
+  }
+  // Minimum size of a free block.
+  virtual size_t minimum_free_block_size() const { return MinChunkSize; }
+  void      removeFreeChunkFromFreeLists(FreeChunk* chunk);
+  void      addChunkAndRepairOffsetTable(HeapWord* chunk, size_t size,
+              bool coalesced);
+
+  // Support for compaction.
+  void prepare_for_compaction(CompactPoint* cp);
+  void adjust_pointers();
+  void compact();
+  // Reset the space to reflect the fact that a compaction of the
+  // space has been done.
+  virtual void reset_after_compaction();
+
+  // Debugging support.
+  void print()                            const;
+  void print_on(outputStream* st)         const;
+  void prepare_for_verify();
+  void verify()                           const;
+  void verifyFreeLists()                  const PRODUCT_RETURN;
+  void verifyIndexedFreeLists()           const;
+  void verifyIndexedFreeList(size_t size) const;
+  // Verify that the given chunk is in the free lists:
+  // i.e. either the binary tree dictionary, the indexed free lists
+  // or the linear allocation block.
+  bool verify_chunk_in_free_list(FreeChunk* fc) const;
+  // Verify that the given chunk is the linear allocation block.
+  bool verify_chunk_is_linear_alloc_block(FreeChunk* fc) const;
+  // Do some basic checks on the the free lists.
+  void check_free_list_consistency()      const PRODUCT_RETURN;
+
+  // Printing support
+  void dump_at_safepoint_with_locks(CMSCollector* c, outputStream* st);
+  void print_indexed_free_lists(outputStream* st) const;
+  void print_dictionary_free_lists(outputStream* st) const;
+  void print_promo_info_blocks(outputStream* st) const;
+
+  NOT_PRODUCT (
+    void initializeIndexedFreeListArrayReturnedBytes();
+    size_t sumIndexedFreeListArrayReturnedBytes();
+    // Return the total number of chunks in the indexed free lists.
+    size_t totalCountInIndexedFreeLists() const;
+    // Return the total number of chunks in the space.
+    size_t totalCount();
+  )
+
+  // The census consists of counts of the quantities such as
+  // the current count of the free chunks, number of chunks
+  // created as a result of the split of a larger chunk or
+  // coalescing of smaller chucks, etc.  The counts in the
+  // census is used to make decisions on splitting and
+  // coalescing of chunks during the sweep of garbage.
+
+  // Print the statistics for the free lists.
+  void printFLCensus(size_t sweep_count) const;
+
+  // Statistics functions
+  // Initialize census for lists before the sweep.
+  void beginSweepFLCensus(float inter_sweep_current,
+                          float inter_sweep_estimate,
+                          float intra_sweep_estimate);
+  // Set the surplus for each of the free lists.
+  void setFLSurplus();
+  // Set the hint for each of the free lists.
+  void setFLHints();
+  // Clear the census for each of the free lists.
+  void clearFLCensus();
+  // Perform functions for the census after the end of the sweep.
+  void endSweepFLCensus(size_t sweep_count);
+  // Return true if the count of free chunks is greater
+  // than the desired number of free chunks.
+  bool coalOverPopulated(size_t size);
+
+// Record (for each size):
+//
+//   split-births = #chunks added due to splits in (prev-sweep-end,
+//      this-sweep-start)
+//   split-deaths = #chunks removed for splits in (prev-sweep-end,
+//      this-sweep-start)
+//   num-curr     = #chunks at start of this sweep
+//   num-prev     = #chunks at end of previous sweep
+//
+// The above are quantities that are measured. Now define:
+//
+//   num-desired := num-prev + split-births - split-deaths - num-curr
+//
+// Roughly, num-prev + split-births is the supply,
+// split-deaths is demand due to other sizes
+// and num-curr is what we have left.
+//
+// Thus, num-desired is roughly speaking the "legitimate demand"
+// for blocks of this size and what we are striving to reach at the
+// end of the current sweep.
+//
+// For a given list, let num-len be its current population.
+// Define, for a free list of a given size:
+//
+//   coal-overpopulated := num-len >= num-desired * coal-surplus
+// (coal-surplus is set to 1.05, i.e. we allow a little slop when
+// coalescing -- we do not coalesce unless we think that the current
+// supply has exceeded the estimated demand by more than 5%).
+//
+// For the set of sizes in the binary tree, which is neither dense nor
+// closed, it may be the case that for a particular size we have never
+// had, or do not now have, or did not have at the previous sweep,
+// chunks of that size. We need to extend the definition of
+// coal-overpopulated to such sizes as well:
+//
+//   For a chunk in/not in the binary tree, extend coal-overpopulated
+//   defined above to include all sizes as follows:
+//
+//   . a size that is non-existent is coal-overpopulated
+//   . a size that has a num-desired <= 0 as defined above is
+//     coal-overpopulated.
+//
+// Also define, for a chunk heap-offset C and mountain heap-offset M:
+//
+//   close-to-mountain := C >= 0.99 * M
+//
+// Now, the coalescing strategy is:
+//
+//    Coalesce left-hand chunk with right-hand chunk if and
+//    only if:
+//
+//      EITHER
+//        . left-hand chunk is of a size that is coal-overpopulated
+//      OR
+//        . right-hand chunk is close-to-mountain
+  void smallCoalBirth(size_t size);
+  void smallCoalDeath(size_t size);
+  void coalBirth(size_t size);
+  void coalDeath(size_t size);
+  void smallSplitBirth(size_t size);
+  void smallSplitDeath(size_t size);
+  void split_birth(size_t size);
+  void splitDeath(size_t size);
+  void split(size_t from, size_t to1);
+
+  double flsFrag() const;
+};
+
+// A parallel-GC-thread-local allocation buffer for allocation into a
+// CompactibleFreeListSpace.
+class CompactibleFreeListSpaceLAB : public CHeapObj<mtGC> {
+  // The space that this buffer allocates into.
+  CompactibleFreeListSpace* _cfls;
+
+  // Our local free lists.
+  AdaptiveFreeList<FreeChunk> _indexedFreeList[CompactibleFreeListSpace::IndexSetSize];
+
+  // Initialized from a command-line arg.
+
+  // Allocation statistics in support of dynamic adjustment of
+  // #blocks to claim per get_from_global_pool() call below.
+  static AdaptiveWeightedAverage
+                 _blocks_to_claim  [CompactibleFreeListSpace::IndexSetSize];
+  static size_t _global_num_blocks [CompactibleFreeListSpace::IndexSetSize];
+  static uint   _global_num_workers[CompactibleFreeListSpace::IndexSetSize];
+  size_t        _num_blocks        [CompactibleFreeListSpace::IndexSetSize];
+
+  // Internal work method
+  void get_from_global_pool(size_t word_sz, AdaptiveFreeList<FreeChunk>* fl);
+
+public:
+  static const int _default_dynamic_old_plab_size = 16;
+  static const int _default_static_old_plab_size  = 50;
+
+  CompactibleFreeListSpaceLAB(CompactibleFreeListSpace* cfls);
+
+  // Allocate and return a block of the given size, or else return NULL.
+  HeapWord* alloc(size_t word_sz);
+
+  // Return any unused portions of the buffer to the global pool.
+  void retire(int tid);
+
+  // Dynamic OldPLABSize sizing
+  static void compute_desired_plab_size();
+  // When the settings are modified from default static initialization
+  static void modify_initialization(size_t n, unsigned wt);
+};
+
+size_t PromotionInfo::refillSize() const {
+  const size_t CMSSpoolBlockSize = 256;
+  const size_t sz = heap_word_size(sizeof(SpoolBlock) + sizeof(markOop)
+                                   * CMSSpoolBlockSize);
+  return CompactibleFreeListSpace::adjustObjectSize(sz);
+}
+
+#endif // SHARE_VM_GC_CMS_COMPACTIBLEFREELISTSPACE_HPP
diff --git a/src/hotspot/share/gc/serial/genMarkSweep.cpp b/src/hotspot/share/gc/serial/genMarkSweep.cpp
index 28c5990c66e..72f571645a5 100644
--- a/src/hotspot/share/gc/serial/genMarkSweep.cpp
+++ b/src/hotspot/share/gc/serial/genMarkSweep.cpp
@@ -325,10 +325,14 @@ void GenMarkSweep::mark_sweep_phase4() {
   // in the same order in phase2, phase3 and phase4. We don't quite do that
   // here (perm_gen first rather than last), so we tell the validate code
   // to use a higher index (saved from phase2) when verifying perm_gen.
+  assert(_rescued_oops == NULL, "must be empty before processing");
   GenCollectedHeap* gch = GenCollectedHeap::heap();
 
   GCTraceTime(Info, gc, phases) tm("Phase 4: Move objects", _gc_timer);
 
+//  MarkSweep::copy_rescued_objects_back();
+
   GenCompactClosure blk;
   gch->generation_iterate(&blk, true);
+  MarkSweep::copy_rescued_objects_back();
 }
diff --git a/src/hotspot/share/gc/serial/markSweep.cpp b/src/hotspot/share/gc/serial/markSweep.cpp
index ebe0da25766..c7befd2f63d 100644
--- a/src/hotspot/share/gc/serial/markSweep.cpp
+++ b/src/hotspot/share/gc/serial/markSweep.cpp
@@ -57,6 +57,8 @@ ReferenceProcessor*     MarkSweep::_ref_processor   = NULL;
 STWGCTimer*             MarkSweep::_gc_timer        = NULL;
 SerialOldTracer*        MarkSweep::_gc_tracer       = NULL;
 
+GrowableArray<HeapWord*>*   MarkSweep::_rescued_oops       = NULL;
+
 MarkSweep::FollowRootClosure  MarkSweep::follow_root_closure;
 
 MarkAndPushClosure MarkSweep::mark_and_push_closure;
@@ -222,3 +224,100 @@ void MarkSweep::initialize() {
   MarkSweep::_gc_timer = new (ResourceObj::C_HEAP, mtGC) STWGCTimer();
   MarkSweep::_gc_tracer = new (ResourceObj::C_HEAP, mtGC) SerialOldTracer();
 }
+
+// (DCEVM) Copy the rescued objects to their destination address after compaction.
+void MarkSweep::copy_rescued_objects_back() {
+
+  if (_rescued_oops != NULL) {
+
+    for (int i=0; i<_rescued_oops->length(); i++) {
+      HeapWord* rescued_ptr = _rescued_oops->at(i);
+      oop rescued_obj = (oop) rescued_ptr;
+
+      int size = rescued_obj->size();
+      oop new_obj = rescued_obj->forwardee();
+
+      assert(rescued_obj->klass()->new_version() != NULL, "just checking");
+
+      if (rescued_obj->klass()->new_version()->update_information() != NULL) {
+        MarkSweep::update_fields(rescued_obj, new_obj);
+      } else {
+        rescued_obj->set_klass(rescued_obj->klass()->new_version());
+        Copy::aligned_disjoint_words((HeapWord*)rescued_obj, (HeapWord*)new_obj, size);
+      }
+
+      FREE_RESOURCE_ARRAY(HeapWord, rescued_ptr, size);
+
+      new_obj->init_mark();
+      assert(oopDesc::is_oop(new_obj), "must be a valid oop");
+    }
+    _rescued_oops->clear();
+    _rescued_oops = NULL;
+  }
+}
+
+// (DCEVM) Update instances of a class whose fields changed.
+void MarkSweep::update_fields(oop q, oop new_location) {
+
+  assert(q->klass()->new_version() != NULL, "class of old object must have new version");
+
+  Klass* old_klass_oop = q->klass();
+  Klass* new_klass_oop = q->klass()->new_version();
+
+  InstanceKlass *old_klass = InstanceKlass::cast(old_klass_oop);
+  InstanceKlass *new_klass = InstanceKlass::cast(new_klass_oop);
+
+  int size = q->size_given_klass(old_klass);
+  int new_size = q->size_given_klass(new_klass);
+
+  HeapWord* tmp = NULL;
+  oop tmp_obj = q;
+
+  // Save object somewhere, there is an overlap in fields
+  if (new_klass_oop->is_copying_backwards()) {
+    if (((HeapWord *)q >= (HeapWord *)new_location && (HeapWord *)q < (HeapWord *)new_location + new_size) ||
+        ((HeapWord *)new_location >= (HeapWord *)q && (HeapWord *)new_location < (HeapWord *)q + size)) {
+       tmp = NEW_RESOURCE_ARRAY(HeapWord, size);
+       q = (oop) tmp;
+       Copy::aligned_disjoint_words((HeapWord*)tmp_obj, (HeapWord*)q, size);
+    }
+  }
+
+  q->set_klass(new_klass_oop);
+  int *cur = new_klass_oop->update_information();
+  assert(cur != NULL, "just checking");
+  MarkSweep::update_fields(new_location, q, cur);
+
+  if (tmp != NULL) {
+    FREE_RESOURCE_ARRAY(HeapWord, tmp, size);
+  }
+}
+
+void MarkSweep::update_fields(oop new_location, oop tmp_obj, int *cur) {
+  assert(cur != NULL, "just checking");
+  char* to = (char*)(HeapWord*)new_location;
+  while (*cur != 0) {
+    int size = *cur;
+    if (size > 0) {
+      cur++;
+      int offset = *cur;
+      HeapWord* from = (HeapWord*)(((char *)(HeapWord*)tmp_obj) + offset);
+      if (size == HeapWordSize) {
+        *((HeapWord*)to) = *from;
+      } else if (size == HeapWordSize * 2) {
+        *((HeapWord*)to) = *from;
+        *(((HeapWord*)to) + 1) = *(from + 1);
+      } else {
+        Copy::conjoint_jbytes(from, to, size);
+      }
+      to += size;
+      cur++;
+    } else {
+      assert(size < 0, "");
+      int skip = -*cur;
+      Copy::fill_to_bytes(to, skip, 0);
+      to += skip;
+      cur++;
+    }
+  }
+}
diff --git a/src/hotspot/share/gc/serial/markSweep.hpp b/src/hotspot/share/gc/serial/markSweep.hpp
index e9588a527e1..e12ac327d90 100644
--- a/src/hotspot/share/gc/serial/markSweep.hpp
+++ b/src/hotspot/share/gc/serial/markSweep.hpp
@@ -87,6 +87,10 @@ class MarkSweep : AllStatic {
   friend class AdjustPointerClosure;
   friend class KeepAliveClosure;
   friend class VM_MarkSweep;
+  friend class GenMarkSweep;
+
+ public:
+  static GrowableArray<HeapWord*>*             _rescued_oops;
 
   //
   // Vars
@@ -144,6 +148,9 @@ class MarkSweep : AllStatic {
 
   static int adjust_pointers(oop obj);
 
+  static void copy_rescued_objects_back();
+  static void update_fields(oop q, oop new_location);
+  static void update_fields(oop new_location, oop tmp_obj, int *cur);
   static void follow_stack();   // Empty marking stack.
 
   static void follow_klass(Klass* klass);
diff --git a/src/hotspot/share/gc/shared/gcConfig.cpp b/src/hotspot/share/gc/shared/gcConfig.cpp
index 39d860b20a1..fdb1c806559 100644
--- a/src/hotspot/share/gc/shared/gcConfig.cpp
+++ b/src/hotspot/share/gc/shared/gcConfig.cpp
@@ -98,7 +98,10 @@ void GCConfig::fail_if_non_included_gc_is_selected() {
 }
 
 void GCConfig::select_gc_ergonomically() {
-  if (os::is_server_class_machine()) {
+  if (AllowEnhancedClassRedefinition) {
+    // Enhanced class redefinition only supports serial GC at the moment
+    FLAG_SET_ERGO(bool, UseSerialGC, true);
+  } else if (os::is_server_class_machine()) {
 #if INCLUDE_G1GC
     FLAG_SET_ERGO_IF_DEFAULT(UseG1GC, true);
 #elif INCLUDE_PARALLELGC
diff --git a/src/hotspot/share/gc/shared/space.cpp b/src/hotspot/share/gc/shared/space.cpp
index 6e215c8e6b7..0fefc5da478 100644
--- a/src/hotspot/share/gc/shared/space.cpp
+++ b/src/hotspot/share/gc/shared/space.cpp
@@ -350,9 +350,8 @@ void CompactibleSpace::clear(bool mangle_space) {
   _compaction_top = bottom();
 }
 
-HeapWord* CompactibleSpace::forward(oop q, size_t size,
-                                    CompactPoint* cp, HeapWord* compact_top) {
-  // q is alive
+// (DCEVM) Calculates the compact_top that will be used for placing the next object with the giving size on the heap.
+HeapWord* CompactibleSpace::forward_compact_top(size_t size, CompactPoint* cp, HeapWord* compact_top) {
   // First check if we should switch compaction space
   assert(this == cp->space, "'this' should be current compaction space.");
   size_t compaction_max_size = pointer_delta(end(), compact_top);
@@ -372,8 +371,15 @@ HeapWord* CompactibleSpace::forward(oop q, size_t size,
     compaction_max_size = pointer_delta(cp->space->end(), compact_top);
   }
 
+  return compact_top;
+}
+
+HeapWord* CompactibleSpace::forward(oop q, size_t size,
+                                    CompactPoint* cp, HeapWord* compact_top) {
+  compact_top = forward_compact_top(size, cp, compact_top);
+
   // store the forwarding pointer into the mark word
-  if (cast_from_oop<HeapWord*>(q) != compact_top) {
+  if (cast_from_oop<HeapWord*>(q) != compact_top || (size_t)q->size() != size) {
     q->forward_to(oop(compact_top));
     assert(q->is_gc_marked(), "encoding the pointer should preserve the mark");
   } else {
@@ -397,7 +403,132 @@ HeapWord* CompactibleSpace::forward(oop q, size_t size,
 #if INCLUDE_SERIALGC
 
 void ContiguousSpace::prepare_for_compaction(CompactPoint* cp) {
-  scan_and_forward(this, cp);
+  if (!Universe::is_redefining_gc_run()) {
+    scan_and_forward(this, cp, false);
+  } else {
+    // Redefinition run
+    scan_and_forward(this, cp, true);
+  }
+}
+
+
+#ifdef ASSERT
+
+int CompactibleSpace::space_index(oop obj) {
+  GenCollectedHeap* heap = GenCollectedHeap::heap();
+
+  //if (heap->is_in_permanent(obj)) {
+  //  return -1;
+  //}
+
+  int index = 0;
+  CompactibleSpace* space = heap->old_gen()->first_compaction_space();
+  while (space != NULL) {
+    if (space->is_in_reserved(obj)) {
+      return index;
+    }
+    space = space->next_compaction_space();
+    index++;
+  }
+
+  space = heap->young_gen()->first_compaction_space();
+  while (space != NULL) {
+    if (space->is_in_reserved(obj)) {
+      return index;
+    }
+    space = space->next_compaction_space();
+    index++;
+  }
+
+  tty->print_cr("could not compute space_index for %08xh", (HeapWord*)obj);
+  index = 0;
+
+  Generation* gen = heap->old_gen();
+  tty->print_cr("  generation %s: %08xh - %08xh", gen->name(), gen->reserved().start(), gen->reserved().end());
+
+  space = gen->first_compaction_space();
+  while (space != NULL) {
+    tty->print_cr("    %2d space %08xh - %08xh", index, space->bottom(), space->end());
+    space = space->next_compaction_space();
+    index++;
+  }
+
+  gen = heap->young_gen();
+  tty->print_cr("  generation %s: %08xh - %08xh", gen->name(), gen->reserved().start(), gen->reserved().end());
+
+  space = gen->first_compaction_space();
+  while (space != NULL) {
+    tty->print_cr("    %2d space %08xh - %08xh", index, space->bottom(), space->end());
+    space = space->next_compaction_space();
+    index++;
+  }
+
+  ShouldNotReachHere();
+  return 0;
+}
+#endif
+
+bool CompactibleSpace::must_rescue(oop old_obj, oop new_obj) {
+  // Only redefined objects can have the need to be rescued.
+  if (oop(old_obj)->klass()->new_version() == NULL) return false;
+
+  //if (old_obj->is_perm()) {
+  //  // This object is in perm gen: Always rescue to satisfy invariant obj->klass() <= obj.
+  //  return true;
+  //}
+
+  int new_size = old_obj->size_given_klass(oop(old_obj)->klass()->new_version());
+  int original_size = old_obj->size();
+  
+  Generation* tenured_gen = GenCollectedHeap::heap()->old_gen();
+  bool old_in_tenured = tenured_gen->is_in_reserved(old_obj);
+  bool new_in_tenured = tenured_gen->is_in_reserved(new_obj);
+  if (old_in_tenured == new_in_tenured) {
+    // Rescue if object may overlap with a higher memory address.
+    bool overlap = ((HeapWord*)old_obj + original_size < (HeapWord*)new_obj + new_size);
+    if (old_in_tenured) {
+      // Old and new address are in same space, so just compare the address.
+      // Must rescue if object moves towards the top of the space.
+      assert(space_index(old_obj) == space_index(new_obj), "old_obj and new_obj must be in same space");
+    } else {
+      // In the new generation, eden is located before the from space, so a
+      // simple pointer comparison is sufficient.
+      assert(GenCollectedHeap::heap()->young_gen()->is_in_reserved(old_obj), "old_obj must be in DefNewGeneration");
+      assert(GenCollectedHeap::heap()->young_gen()->is_in_reserved(new_obj), "new_obj must be in DefNewGeneration");
+      assert(overlap == (space_index(old_obj) < space_index(new_obj)), "slow and fast computation must yield same result");
+    }
+    return overlap;
+
+  } else {
+    assert(space_index(old_obj) != space_index(new_obj), "old_obj and new_obj must be in different spaces");
+    if (tenured_gen->is_in_reserved(new_obj)) {
+      // Must never rescue when moving from the new into the old generation.
+      assert(GenCollectedHeap::heap()->young_gen()->is_in_reserved(old_obj), "old_obj must be in DefNewGeneration");
+      assert(space_index(old_obj) > space_index(new_obj), "must be");
+      return false;
+
+    } else /* if (tenured_gen->is_in_reserved(old_obj)) */ {
+      // Must always rescue when moving from the old into the new generation.
+      assert(GenCollectedHeap::heap()->young_gen()->is_in_reserved(new_obj), "new_obj must be in DefNewGeneration");
+      assert(space_index(old_obj) < space_index(new_obj), "must be");
+      return true;
+    }
+  }
+}
+
+HeapWord* CompactibleSpace::rescue(HeapWord* old_obj) {
+  assert(must_rescue(oop(old_obj), oop(old_obj)->forwardee()), "do not call otherwise");
+
+  int size = oop(old_obj)->size();
+  HeapWord* rescued_obj = NEW_RESOURCE_ARRAY(HeapWord, size);
+  Copy::aligned_disjoint_words(old_obj, rescued_obj, size);
+
+  if (MarkSweep::_rescued_oops == NULL) {
+    MarkSweep::_rescued_oops = new GrowableArray<HeapWord*>(128);
+  }
+
+  MarkSweep::_rescued_oops->append(rescued_obj);
+  return rescued_obj;
 }
 
 void CompactibleSpace::adjust_pointers() {
@@ -410,7 +541,12 @@ void CompactibleSpace::adjust_pointers() {
 }
 
 void CompactibleSpace::compact() {
-  scan_and_compact(this);
+  if(!Universe::is_redefining_gc_run()) {
+    scan_and_compact(this, false);
+  } else {
+    // Redefinition run
+    scan_and_compact(this, true);
+  }
 }
 
 #endif // INCLUDE_SERIALGC
@@ -685,6 +821,58 @@ void OffsetTableContigSpace::verify() const {
   guarantee(p == top(), "end of last object must match end of space");
 }
 
+// Compute the forward sizes and leave out objects whose position could
+// possibly overlap other objects.
+HeapWord* CompactibleSpace::forward_with_rescue(HeapWord* q, size_t size,
+                                                CompactPoint* cp, HeapWord* compact_top) {
+  size_t forward_size = size;
+
+  // (DCEVM) There is a new version of the class of q => different size
+  if (oop(q)->klass()->new_version() != NULL && oop(q)->klass()->new_version()->update_information() != NULL) {
+
+    size_t new_size = oop(q)->size_given_klass(oop(q)->klass()->new_version());
+    assert(size != new_size, "instances without changed size have to be updated prior to GC run");
+    forward_size = new_size;
+  }
+
+  compact_top = forward_compact_top(forward_size, cp, compact_top);
+
+  if (must_rescue(oop(q), oop(compact_top))) {
+    if (MarkSweep::_rescued_oops == NULL) {
+      MarkSweep::_rescued_oops = new GrowableArray<HeapWord*>(128);
+    }
+    MarkSweep::_rescued_oops->append(q);
+    return compact_top;
+  }
+
+  return forward(oop(q), forward_size, cp, compact_top);
+}
+
+// Compute the forwarding addresses for the objects that need to be rescued.
+HeapWord* CompactibleSpace::forward_rescued(CompactPoint* cp, HeapWord* compact_top) {
+  // TODO: empty the _rescued_oops after ALL spaces are compacted!
+  if (MarkSweep::_rescued_oops != NULL) {
+    for (int i=0; i<MarkSweep::_rescued_oops->length(); i++) {
+      HeapWord* q = MarkSweep::_rescued_oops->at(i);
+
+      /* size_t size = oop(q)->size();  changing this for cms for perm gen */
+      size_t size = block_size(q);
+
+      // (DCEVM) There is a new version of the class of q => different size
+      if (oop(q)->klass()->new_version() != NULL) {
+        size_t new_size = oop(q)->size_given_klass(oop(q)->klass()->new_version());
+        assert(size != new_size, "instances without changed size have to be updated prior to GC run");
+        size = new_size;
+      }
+
+      compact_top = cp->space->forward(oop(q), size, cp, compact_top);
+      assert(compact_top <= end(), "must not write over end of space!");
+    }
+    MarkSweep::_rescued_oops->clear();
+    MarkSweep::_rescued_oops = NULL;
+  }
+  return compact_top;
+}
 
 size_t TenuredSpace::allowed_dead_ratio() const {
   return MarkSweepDeadRatio;
diff --git a/src/hotspot/share/gc/shared/space.hpp b/src/hotspot/share/gc/shared/space.hpp
index 0b760364bbc..c9bfc365f0f 100644
--- a/src/hotspot/share/gc/shared/space.hpp
+++ b/src/hotspot/share/gc/shared/space.hpp
@@ -404,6 +404,9 @@ public:
   // indicates when the next such action should be taken.
   virtual void prepare_for_compaction(CompactPoint* cp) = 0;
   // MarkSweep support phase3
+  DEBUG_ONLY(int space_index(oop obj));
+  bool must_rescue(oop old_obj, oop new_obj);
+  HeapWord* rescue(HeapWord* old_obj);
   virtual void adjust_pointers();
   // MarkSweep support phase4
   virtual void compact();
@@ -434,6 +437,15 @@ public:
   // accordingly".
   virtual HeapWord* forward(oop q, size_t size, CompactPoint* cp,
                     HeapWord* compact_top);
+  // (DCEVM) same as forwad, but can rescue objects. Invoked only during
+  // redefinition runs
+  HeapWord* forward_with_rescue(HeapWord* q, size_t size, CompactPoint* cp,
+                                HeapWord* compact_top);
+
+  HeapWord* forward_rescued(CompactPoint* cp, HeapWord* compact_top);
+
+  // (tw) Compute new compact top without actually forwarding the object.
+  virtual HeapWord* forward_compact_top(size_t size, CompactPoint* cp, HeapWord* compact_top);
 
   // Return a size with adjustments as required of the space.
   virtual size_t adjust_object_size_v(size_t size) const { return size; }
@@ -467,12 +479,12 @@ protected:
 
   // Frequently calls obj_size().
   template <class SpaceType>
-  static inline void scan_and_compact(SpaceType* space);
+  static inline void scan_and_compact(SpaceType* space, bool redefinition_run);
 
   // Frequently calls scanned_block_is_obj() and scanned_block_size().
   // Requires the scan_limit() function.
   template <class SpaceType>
-  static inline void scan_and_forward(SpaceType* space, CompactPoint* cp);
+  static inline void scan_and_forward(SpaceType* space, CompactPoint* cp, bool redefinition_run);
 };
 
 class GenSpaceMangler;
@@ -483,7 +495,7 @@ class ContiguousSpace: public CompactibleSpace {
   friend class VMStructs;
   // Allow scan_and_forward function to call (private) overrides for auxiliary functions on this class
   template <typename SpaceType>
-  friend void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp);
+  friend void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp, bool redefinition_run);
 
  private:
   // Auxiliary functions for scan_and_forward support.
diff --git a/src/hotspot/share/gc/shared/space.inline.hpp b/src/hotspot/share/gc/shared/space.inline.hpp
index 37ae2765e71..8e3722b01b1 100644
--- a/src/hotspot/share/gc/shared/space.inline.hpp
+++ b/src/hotspot/share/gc/shared/space.inline.hpp
@@ -133,7 +133,7 @@ public:
 };
 
 template <class SpaceType>
-inline void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp) {
+inline void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp, bool redefinition_run) {
   // Compute the new addresses for the live objects and store it in the mark
   // Used by universe::mark_sweep_phase2()
 
@@ -171,7 +171,18 @@ inline void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* c
       // prefetch beyond cur_obj
       Prefetch::write(cur_obj, interval);
       size_t size = space->scanned_block_size(cur_obj);
-      compact_top = cp->space->forward(oop(cur_obj), size, cp, compact_top);
+
+      if (redefinition_run) {
+        compact_top = cp->space->forward_with_rescue(cur_obj, size, cp, compact_top);
+        if (first_dead == NULL && oop(cur_obj)->is_gc_marked()) {
+          /* Was moved (otherwise, forward would reset mark),
+             set first_dead to here */
+          first_dead = cur_obj;
+        }
+      } else {
+        compact_top = cp->space->forward(oop(cur_obj), size, cp, compact_top);
+      }
+
       cur_obj += size;
       end_of_live = cur_obj;
     } else {
@@ -206,6 +217,10 @@ inline void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* c
     }
   }
 
+  if (redefinition_run) {
+    compact_top = space->forward_rescued(cp, compact_top);
+  }
+
   assert(cur_obj == scan_limit, "just checking");
   space->_end_of_live = end_of_live;
   if (first_dead != NULL) {
@@ -292,7 +307,7 @@ inline void CompactibleSpace::clear_empty_region(SpaceType* space) {
 }
 
 template <class SpaceType>
-inline void CompactibleSpace::scan_and_compact(SpaceType* space) {
+inline void CompactibleSpace::scan_and_compact(SpaceType* space, bool redefinition_run) {
   // Copy all live objects to their new location
   // Used by MarkSweep::mark_sweep_phase4()
 
@@ -316,7 +331,7 @@ inline void CompactibleSpace::scan_and_compact(SpaceType* space) {
   if (space->_first_dead > cur_obj && !oop(cur_obj)->is_gc_marked()) {
     // All object before _first_dead can be skipped. They should not be moved.
     // A pointer to the first live object is stored at the memory location for _first_dead.
-    cur_obj = *(HeapWord**)(space->_first_dead);
+    cur_obj = space->_first_dead;
   }
 
   debug_only(HeapWord* prev_obj = NULL);
@@ -334,11 +349,35 @@ inline void CompactibleSpace::scan_and_compact(SpaceType* space) {
       size_t size = space->obj_size(cur_obj);
       HeapWord* compaction_top = cast_from_oop<HeapWord*>(oop(cur_obj)->forwardee());
 
+      if (redefinition_run &&  space->must_rescue(oop(cur_obj), oop(cur_obj)->forwardee())) {
+         space->rescue(cur_obj);
+        debug_only(Copy::fill_to_words(cur_obj, size, 0));
+        cur_obj += size;
+        continue;
+      }
+
       // prefetch beyond compaction_top
       Prefetch::write(compaction_top, copy_interval);
 
       // copy object and reinit its mark
-      assert(cur_obj != compaction_top, "everything in this pass should be moving");
+      assert(cur_obj != compaction_top || oop(cur_obj)->klass()->new_version() != NULL,
+             "everything in this pass should be moving");
+      if (redefinition_run && oop(cur_obj)->klass()->new_version() != NULL) {
+        Klass* new_version = oop(cur_obj)->klass()->new_version();
+        if (new_version->update_information() == NULL) {
+          Copy::aligned_conjoint_words(cur_obj, compaction_top, size);
+          oop(compaction_top)->set_klass(new_version);
+        } else {
+          MarkSweep::update_fields(oop(cur_obj), oop(compaction_top));
+        }
+        oop(compaction_top)->init_mark();
+        assert(oop(compaction_top)->klass() != NULL, "should have a class");
+
+        debug_only(prev_obj = cur_obj);
+        cur_obj += size;
+        continue;
+      }
+
       Copy::aligned_conjoint_words(cur_obj, compaction_top, size);
       oop(compaction_top)->init_mark_raw();
       assert(oop(compaction_top)->klass() != NULL, "should have a class");
diff --git a/src/hotspot/share/interpreter/linkResolver.cpp b/src/hotspot/share/interpreter/linkResolver.cpp
index d8e01e80d2e..ed15db841c6 100644
--- a/src/hotspot/share/interpreter/linkResolver.cpp
+++ b/src/hotspot/share/interpreter/linkResolver.cpp
@@ -284,7 +284,7 @@ void LinkResolver::check_klass_accessibility(Klass* ref_klass, Klass* sel_klass,
   }
 
   Reflection::VerifyClassAccessResults vca_result =
-    Reflection::verify_class_access(ref_klass, InstanceKlass::cast(base_klass), true);
+    Reflection::verify_class_access(ref_klass->newest_version(), InstanceKlass::cast(base_klass->newest_version()), true);
   if (vca_result != Reflection::ACCESS_OK) {
     ResourceMark rm(THREAD);
     char* msg = Reflection::verify_class_access_msg(ref_klass,
@@ -566,7 +566,7 @@ void LinkResolver::check_method_accessability(Klass* ref_klass,
   // We'll check for the method name first, as that's most likely
   // to be false (so we'll short-circuit out of these tests).
   if (sel_method->name() == vmSymbols::clone_name() &&
-      sel_klass == SystemDictionary::Object_klass() &&
+      sel_klass->newest_version() == SystemDictionary::Object_klass()->newest_version() &&
       resolved_klass->is_array_klass()) {
     // We need to change "protected" to "public".
     assert(flags.is_protected(), "clone not protected?");
@@ -1011,7 +1011,7 @@ void LinkResolver::resolve_field(fieldDescriptor& fd,
     //     or by the <init> method (in case of an instance field).
     if (is_put && fd.access_flags().is_final()) {
 
-      if (sel_klass != current_klass) {
+      if (sel_klass != current_klass && sel_klass != current_klass->active_version()) {
         ResourceMark rm(THREAD);
         stringStream ss;
         ss.print("Update to %s final field %s.%s attempted from a different class (%s) than the field's declaring class",
@@ -1400,6 +1400,7 @@ void LinkResolver::runtime_resolve_virtual_method(CallInfo& result,
       assert(resolved_method->can_be_statically_bound(), "cannot override this method");
       selected_method = resolved_method;
     } else {
+      assert(recv_klass->is_subtype_of(resolved_method->method_holder()), "receiver and resolved method holder are inconsistent");
       selected_method = methodHandle(THREAD, recv_klass->method_at_vtable(vtable_index));
     }
   }
diff --git a/src/hotspot/share/jfr/instrumentation/jfrEventClassTransformer.cpp b/src/hotspot/share/jfr/instrumentation/jfrEventClassTransformer.cpp
index 793ee757879..96fc139bea3 100644
--- a/src/hotspot/share/jfr/instrumentation/jfrEventClassTransformer.cpp
+++ b/src/hotspot/share/jfr/instrumentation/jfrEventClassTransformer.cpp
@@ -1471,6 +1471,7 @@ static InstanceKlass* create_new_instance_klass(InstanceKlass* ik, ClassFileStre
                              cld,
                              &cl_info,
                              ClassFileParser::INTERNAL, // internal visibility
+							 false,
                              THREAD);
   if (HAS_PENDING_EXCEPTION) {
     log_pending_exception(PENDING_EXCEPTION);
diff --git a/src/hotspot/share/memory/universe.cpp b/src/hotspot/share/memory/universe.cpp
index dfb9edf6721..1f3f746b2b7 100644
--- a/src/hotspot/share/memory/universe.cpp
+++ b/src/hotspot/share/memory/universe.cpp
@@ -156,6 +156,7 @@ int             Universe::_base_vtable_size = 0;
 bool            Universe::_bootstrapping = false;
 bool            Universe::_module_initialized = false;
 bool            Universe::_fully_initialized = false;
+bool            Universe::_is_redefining_gc_run = false; // FIXME: review
 
 size_t          Universe::_heap_capacity_at_last_gc;
 size_t          Universe::_heap_used_at_last_gc = 0;
@@ -177,6 +178,44 @@ void Universe::basic_type_classes_do(KlassClosure *closure) {
 #define DO_PRIMITIVE_MIRROR(m) \
   f->do_oop((oop*) &m);
 
+// FIXME: This method should iterate all pointers that are not within heap objects.
+void Universe::root_oops_do(OopClosure *oopClosure) {
+
+  class AlwaysTrueClosure: public BoolObjectClosure {
+  public:
+    void do_object(oop p) { ShouldNotReachHere(); }
+    bool do_object_b(oop p) { return true; }
+  };
+  AlwaysTrueClosure always_true;
+
+  Universe::oops_do(oopClosure);
+//  ReferenceProcessor::oops_do(oopClosure); (tw) check why no longer there
+  JNIHandles::oops_do(oopClosure);   // Global (strong) JNI handles
+  Threads::oops_do(oopClosure, NULL);
+  ObjectSynchronizer::oops_do(oopClosure);
+  // TODO: review, flat profiler was removed in j10
+  // FlatProfiler::oops_do(oopClosure);
+  JvmtiExport::oops_do(oopClosure);
+
+  // Now adjust pointers in remaining weak roots.  (All of which should
+  // have been cleared if they pointed to non-surviving objects.)
+  // Global (weak) JNI handles
+  JNIHandles::weak_oops_do(&always_true, oopClosure);
+
+  CodeBlobToOopClosure blobClosure(oopClosure, CodeBlobToOopClosure::FixRelocations);
+  CodeCache::blobs_do(&blobClosure);
+  StringTable::oops_do(oopClosure);
+  
+  // (DCEVM) TODO: Check if this is correct?
+  //CodeCache::scavenge_root_nmethods_oops_do(oopClosure);
+  //Management::oops_do(oopClosure);
+  //ref_processor()->weak_oops_do(&oopClosure);
+  //PSScavenge::reference_processor()->weak_oops_do(&oopClosure);
+
+  // SO_AllClasses
+  SystemDictionary::oops_do(oopClosure);
+}
+
 void Universe::oops_do(OopClosure* f) {
   PRIMITIVE_MIRRORS_DO(DO_PRIMITIVE_MIRROR);
 
diff --git a/src/hotspot/share/memory/universe.hpp b/src/hotspot/share/memory/universe.hpp
index 5d528f522d4..f382a709d18 100644
--- a/src/hotspot/share/memory/universe.hpp
+++ b/src/hotspot/share/memory/universe.hpp
@@ -53,7 +53,13 @@ class LatestMethodCache : public CHeapObj<mtClass> {
   Klass*                _klass;
   int                   _method_idnum;
 
+  static bool _is_redefining_gc_run;
+
  public:
+
+   static bool is_redefining_gc_run()               { return _is_redefining_gc_run; }
+   static void set_redefining_gc_run(bool b)        { _is_redefining_gc_run = b;    }
+
   LatestMethodCache()   { _klass = NULL; _method_idnum = -1; }
   ~LatestMethodCache()  { _klass = NULL; _method_idnum = -1; }
 
@@ -211,10 +217,15 @@ class Universe: AllStatic {
 
   static uintptr_t _verify_oop_mask;
   static uintptr_t _verify_oop_bits;
+  static bool _is_redefining_gc_run;
 
  public:
   static void calculate_verify_data(HeapWord* low_boundary, HeapWord* high_boundary) PRODUCT_RETURN;
 
+  // Advanced class redefinition. FIXME: review?
+  static bool is_redefining_gc_run()               { return _is_redefining_gc_run; }
+  static void set_redefining_gc_run(bool b)        { _is_redefining_gc_run = b;    }
+
   // Known classes in the VM
   static Klass* boolArrayKlassObj()                 { return typeArrayKlassObj(T_BOOLEAN); }
   static Klass* byteArrayKlassObj()                 { return typeArrayKlassObj(T_BYTE); }
@@ -344,6 +355,7 @@ class Universe: AllStatic {
 
   // Iteration
 
+  static void root_oops_do(OopClosure *oopClosure); // FIXME: kill...
   // Apply "f" to the addresses of all the direct heap pointers maintained
   // as static fields of "Universe".
   static void oops_do(OopClosure* f);
diff --git a/src/hotspot/share/oops/cpCache.cpp b/src/hotspot/share/oops/cpCache.cpp
index d939880ed12..ac1c7d397e8 100644
--- a/src/hotspot/share/oops/cpCache.cpp
+++ b/src/hotspot/share/oops/cpCache.cpp
@@ -450,7 +450,8 @@ void ConstantPoolCacheEntry::set_method_handle_common(const constantPoolHandle&
   if (has_appendix) {
     const int appendix_index = f2_as_index();
     assert(appendix_index >= 0 && appendix_index < resolved_references->length(), "oob");
-    assert(resolved_references->obj_at(appendix_index) == NULL, "init just once");
+    // FIXME (DCEVM) relaxing for now...
+    //assert(resolved_references->obj_at(appendix_index) == NULL, "init just once");
     resolved_references->obj_at_put(appendix_index, appendix());
   }
 
@@ -640,6 +641,35 @@ Method* ConstantPoolCacheEntry::get_interesting_method_entry() {
   }
   return m;
 }
+
+// Enhanced RedefineClasses() API support (DCEVM):
+// Clear cached entry, let it be re-resolved
+void ConstantPoolCacheEntry::clear_entry() {
+  // Always clear for invokehandle/invokedynamic to re-resolve them
+  bool clearData = bytecode_1() == Bytecodes::_invokehandle || bytecode_1() == Bytecodes::_invokedynamic;
+  _indices = constant_pool_index();
+
+  if (clearData) {
+    if (!is_resolved_reference()) {
+      _f2 = 0;
+    }
+    // FIXME: (DCEVM) we want to clear flags, but parameter size is actually used
+    // after we return from the method, before entry is re-initialized. So let's
+    // keep parameter size the same.
+    // For example, it's used in TemplateInterpreterGenerator::generate_return_entry_for
+    // Also, we need to keep flag marking entry as one containing resolved_reference
+    _flags &= parameter_size_mask | (1 << is_resolved_ref_shift);
+    _f1 = NULL;
+  }
+}
+
+// Enhanced RedefineClasses() API support (DCEVM):
+// Clear all entries
+void ConstantPoolCache::clear_entries() {
+  for (int i = 0; i < length(); i++) {
+    entry_at(i)->clear_entry();
+  }
+}
 #endif // INCLUDE_JVMTI
 
 void ConstantPoolCacheEntry::print(outputStream* st, int index) const {
diff --git a/src/hotspot/share/oops/cpCache.hpp b/src/hotspot/share/oops/cpCache.hpp
index 38d424c9019..121a13b1dda 100644
--- a/src/hotspot/share/oops/cpCache.hpp
+++ b/src/hotspot/share/oops/cpCache.hpp
@@ -148,13 +148,13 @@ class ConstantPoolCacheEntry {
   void set_bytecode_2(Bytecodes::Code code);
   void set_f1(Metadata* f1) {
     Metadata* existing_f1 = _f1; // read once
-    assert(existing_f1 == NULL || existing_f1 == f1, "illegal field change");
+    //assert(existing_f1 == NULL || existing_f1 == f1, "illegal field change");
     _f1 = f1;
   }
   void release_set_f1(Metadata* f1);
   void set_f2(intx f2) {
     intx existing_f2 = _f2; // read once
-    assert(existing_f2 == 0 || existing_f2 == f2, "illegal field change");
+    //assert(existing_f2 == 0 || existing_f2 == f2, "illegal field change");
     _f2 = f2;
   }
   void set_f2_as_vfinal_method(Method* f2) {
@@ -180,6 +180,8 @@ class ConstantPoolCacheEntry {
     tos_state_bits             = 4,
     tos_state_mask             = right_n_bits(tos_state_bits),
     tos_state_shift            = BitsPerInt - tos_state_bits,  // see verify_tos_state_shift below
+    // (DCEVM) We need to remember entries which has resolved reference indices as we don't want to clean them
+    is_resolved_ref_shift      = 27,
     // misc. option bits; can be any bit position in [16..27]
     is_field_entry_shift       = 26,  // (F) is it a field or a method?
     has_local_signature_shift  = 25,  // (S) does the call site have a per-site signature (sig-poly methods)?
@@ -213,6 +215,7 @@ class ConstantPoolCacheEntry {
   void initialize_resolved_reference_index(int ref_index) {
     assert(_f2 == 0, "set once");  // note: ref_index might be zero also
     _f2 = ref_index;
+    _flags = 1 << is_resolved_ref_shift;
   }
 
   void set_field(                                // sets entry to resolved field state
@@ -353,6 +356,7 @@ class ConstantPoolCacheEntry {
   bool is_method_entry() const                   { return (_flags & (1 << is_field_entry_shift))    == 0; }
   bool is_field_entry() const                    { return (_flags & (1 << is_field_entry_shift))    != 0; }
   bool is_long() const                           { return flag_state() == ltos; }
+  bool is_resolved_reference() const             { return (_flags & (1 << is_resolved_ref_shift))   != 0; }
   bool is_double() const                         { return flag_state() == dtos; }
   TosState flag_state() const                    { assert((uint)number_of_states <= (uint)tos_state_mask+1, "");
                                                    return (TosState)((_flags >> tos_state_shift) & tos_state_mask); }
@@ -379,6 +383,10 @@ class ConstantPoolCacheEntry {
          bool* trace_name_printed);
   bool check_no_old_or_obsolete_entries();
   Method* get_interesting_method_entry();
+
+  // Enhanced RedefineClasses() API support (DCEVM):
+  // Clear cached entry, let it be re-resolved
+  void clear_entry();
 #endif // INCLUDE_JVMTI
 
   // Debugging & Printing
@@ -501,6 +509,10 @@ class ConstantPoolCache: public MetaspaceObj {
   void adjust_method_entries(bool* trace_name_printed);
   bool check_no_old_or_obsolete_entries();
   void dump_cache();
+
+  // Enhanced RedefineClasses() API support (DCEVM):
+  // Clear all entries
+  void clear_entries();
 #endif // INCLUDE_JVMTI
 
   // RedefineClasses support
diff --git a/src/hotspot/share/oops/instanceKlass.cpp b/src/hotspot/share/oops/instanceKlass.cpp
index cfd7fe74321..51bee899aa9 100644
--- a/src/hotspot/share/oops/instanceKlass.cpp
+++ b/src/hotspot/share/oops/instanceKlass.cpp
@@ -998,7 +998,8 @@ bool InstanceKlass::link_class_impl(TRAPS) {
       // itable().verify(tty, true);
 #endif
       set_init_state(linked);
-      if (JvmtiExport::should_post_class_prepare()) {
+      // (DCEVM) Must check for old version in order to prevent infinite loops.
+      if (JvmtiExport::should_post_class_prepare() && old_version() == NULL /* JVMTI deadlock otherwise */) {
         Thread *thread = THREAD;
         assert(thread->is_Java_thread(), "thread->is_Java_thread()");
         JvmtiExport::post_class_prepare((JavaThread *) thread, this);
@@ -1078,7 +1079,8 @@ void InstanceKlass::initialize_impl(TRAPS) {
     // If we were to use wait() instead of waitInterruptibly() then
     // we might end up throwing IE from link/symbol resolution sites
     // that aren't expected to throw.  This would wreak havoc.  See 6320309.
-    while (is_being_initialized() && !is_reentrant_initialization(jt)) {
+    while ((is_being_initialized() && !is_reentrant_initialization(jt)) 
+            || (old_version() != NULL && InstanceKlass::cast(old_version())->is_being_initialized())) {
       wait = true;
       jt->set_class_to_be_initialized(this);
       ol.wait_uninterruptibly(jt);
@@ -1362,6 +1364,18 @@ bool InstanceKlass::implements_interface(Klass* k) const {
   return false;
 }
 
+bool InstanceKlass::implements_interface_any_version(Klass* k) const {
+  k = k->newest_version();
+  if (this->newest_version() == k) return true;
+  assert(k->is_interface(), "should be an interface class");
+  for (int i = 0; i < transitive_interfaces()->length(); i++) {
+    if (transitive_interfaces()->at(i)->newest_version() == k) {
+      return true;
+    }
+  }
+  return false;
+}
+
 bool InstanceKlass::is_same_or_direct_interface(Klass *k) const {
   // Verify direct super interface
   if (this == k) return true;
@@ -1628,6 +1642,23 @@ void InstanceKlass::methods_do(void f(Method* method)) {
   }
 }
 
+/**
+  Update information contains mapping of fields from old class to the new class.
+  Info is stored on HEAP, you need to call clear_update_information to free the space.
+*/
+void InstanceKlass::store_update_information(GrowableArray<int> &values) {
+  int *arr = NEW_C_HEAP_ARRAY(int, values.length(), mtClass);
+  for (int i = 0; i < values.length(); i++) {
+    arr[i] = values.at(i);
+  }
+  set_update_information(arr);
+}
+
+void InstanceKlass::clear_update_information() {
+  FREE_C_HEAP_ARRAY(int, update_information());
+  set_update_information(NULL);
+}
+
 
 void InstanceKlass::do_local_static_fields(FieldClosure* cl) {
   for (JavaFieldStream fs(this); !fs.done(); fs.next()) {
@@ -2310,12 +2341,30 @@ void InstanceKlass::add_dependent_nmethod(nmethod* nm) {
 
 void InstanceKlass::remove_dependent_nmethod(nmethod* nm) {
   dependencies().remove_dependent_nmethod(nm);
+  // (DCEVM) Hack as dependencies get wrong version of Klass*
+//  if (this->old_version() != NULL) {
+//    InstanceKlass::cast(this->old_version())->remove_dependent_nmethod(nm, true);
+//    return;
+//  }
 }
 
 void InstanceKlass::clean_dependency_context() {
   dependencies().clean_unloading_dependents();
 }
 
+bool InstanceKlass::update_jmethod_id(Method* method, jmethodID newMethodID) {
+  size_t idnum = (size_t)method->method_idnum();
+  jmethodID* jmeths = methods_jmethod_ids_acquire();
+  size_t length;                                // length assigned as debugging crumb
+  jmethodID id = NULL;
+  if (jmeths != NULL &&                         // If there is a cache
+      (length = (size_t)jmeths[0]) > idnum) {   // and if it is long enough,
+    jmeths[idnum+1] = newMethodID;              // Set method id (may be NULL)
+    return true;
+  }
+  return false;
+}
+
 #ifndef PRODUCT
 void InstanceKlass::print_dependent_nmethods(bool verbose) {
   dependencies().print_dependent_nmethods(verbose);
@@ -3718,7 +3767,7 @@ void InstanceKlass::verify_on(outputStream* st) {
     }
 
     guarantee(sib->is_klass(), "should be klass");
-    guarantee(sib->super() == super, "siblings should have same superklass");
+    guarantee(sib->super() == super || super->newest_version() == SystemDictionary::Object_klass(), "siblings should have same superklass");
   }
 
   // Verify local interfaces
diff --git a/src/hotspot/share/oops/instanceKlass.hpp b/src/hotspot/share/oops/instanceKlass.hpp
index b1894200fc1..aa3d38d95e0 100644
--- a/src/hotspot/share/oops/instanceKlass.hpp
+++ b/src/hotspot/share/oops/instanceKlass.hpp
@@ -137,6 +137,7 @@ class InstanceKlass: public Klass {
   friend class JVMCIVMStructs;
   friend class ClassFileParser;
   friend class CompileReplay;
+  friend class VM_EnhancedRedefineClasses;
 
  public:
   static const KlassID ID = InstanceKlassID;
@@ -955,6 +956,7 @@ public:
                 size_t *length_p, jmethodID* id_p);
   void ensure_space_for_methodids(int start_offset = 0);
   jmethodID jmethod_id_or_null(Method* method);
+  bool update_jmethod_id(Method* method, jmethodID newMethodID);
 
   // annotations support
   Annotations* annotations() const          { return _annotations; }
@@ -1029,6 +1031,7 @@ public:
 
   // subclass/subinterface checks
   bool implements_interface(Klass* k) const;
+  bool implements_interface_any_version(Klass* k) const;
   bool is_same_or_direct_interface(Klass* k) const;
 
 #ifdef ASSERT
@@ -1060,6 +1063,10 @@ public:
   void do_nonstatic_fields(FieldClosure* cl); // including inherited fields
   void do_local_static_fields(void f(fieldDescriptor*, Handle, TRAPS), Handle, TRAPS);
 
+  // Advanced class redefinition: FIXME: why here?
+  void store_update_information(GrowableArray<int> &values);
+  void clear_update_information();
+
   void methods_do(void f(Method* method));
   void array_klasses_do(void f(Klass* k));
   void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);
diff --git a/src/hotspot/share/oops/klass.cpp b/src/hotspot/share/oops/klass.cpp
index 98d0ce1b753..352d8f84631 100644
--- a/src/hotspot/share/oops/klass.cpp
+++ b/src/hotspot/share/oops/klass.cpp
@@ -200,7 +200,13 @@ void* Klass::operator new(size_t size, ClassLoaderData* loader_data, size_t word
 Klass::Klass(KlassID id) : _id(id),
                            _java_mirror(NULL),
                            _prototype_header(markWord::prototype()),
-                           _shared_class_path_index(-1) {
+                           _shared_class_path_index(-1),
+                           _new_version(NULL),
+                           _old_version(NULL),
+                           _is_redefining(false),
+                           _is_copying_backwards(false),
+                           _redefinition_flags(Klass::NoRedefinition),
+                           _update_information(NULL) {
   CDS_ONLY(_shared_class_flags = 0;)
   CDS_JAVA_HEAP_ONLY(_archived_mirror = 0;)
   _primary_supers[0] = this;
@@ -465,6 +471,27 @@ void Klass::clean_subklass() {
   }
 }
 
+void Klass::remove_from_sibling_list() {
+  debug_only(verify();)
+
+  // remove ourselves to superklass' subklass list
+  InstanceKlass* super = superklass();
+  if (super == NULL) return;        // special case: class Object
+  if (super->subklass() == this) {
+    // this klass is the first subklass
+    super->set_subklass(next_sibling());
+  } else {
+    Klass* sib = super->subklass();
+    assert(sib != NULL, "cannot find this class in sibling list!");
+    while (sib->next_sibling() != this) {
+      sib = sib->next_sibling();
+      assert(sib != NULL, "cannot find this class in sibling list!");
+    }
+    sib->set_next_sibling(next_sibling());
+  }
+  debug_only(verify();)
+}
+
 void Klass::clean_weak_klass_links(bool unloading_occurred, bool clean_alive_klasses) {
   if (!ClassUnloading || !unloading_occurred) {
     return;
diff --git a/src/hotspot/share/oops/klass.hpp b/src/hotspot/share/oops/klass.hpp
index c90477cca96..e06c068cdd7 100644
--- a/src/hotspot/share/oops/klass.hpp
+++ b/src/hotspot/share/oops/klass.hpp
@@ -165,6 +165,18 @@ class Klass : public Metadata {
   markWord _prototype_header;   // Used when biased locking is both enabled and disabled for this type
   jint     _biased_lock_revocation_count;
 
+  // Advanced class redefinition
+
+  // Old version (used in advanced class redefinition)
+  Klass*      _old_version;
+  // New version (used in advanced class redefinition)
+  Klass*      _new_version;
+
+  int         _redefinition_flags;     // Level of class redefinition
+  bool        _is_redefining;
+  int*        _update_information;
+  bool        _is_copying_backwards;   // Does the class need to copy fields backwards? => possibly overwrite itself?
+
 private:
   // This is an index into FileMapHeader::_shared_path_table[], to
   // associate this class with the JAR file where it's loaded from during
@@ -291,6 +303,7 @@ protected:
 
   InstanceKlass* superklass() const;
   void append_to_sibling_list();           // add newly created receiver to superklass' subklass list
+  void remove_from_sibling_list();         // enhanced class redefinition
 
   void set_next_link(Klass* k) { _next_link = k; }
   Klass* next_link() const { return _next_link; }   // The next klass defined by the class loader.
@@ -334,11 +347,45 @@ protected:
   virtual ModuleEntry* module() const = 0;
   virtual PackageEntry* package() const = 0;
 
+  // Advanced class redefinition
+  Klass* old_version() const             { return _old_version; }
+  void set_old_version(Klass* klass)     { assert(_old_version == NULL || klass == NULL, "Old version can only be set once!"); _old_version = klass; }
+  Klass* new_version() const             { return _new_version; }
+  void set_new_version(Klass* klass)     { assert(_new_version == NULL || klass == NULL, "New version can only be set once!"); _new_version = klass; }
+  bool is_redefining() const             { return _is_redefining; }
+  void set_redefining(bool b)            { _is_redefining = b; }
+  int redefinition_flags() const         { return _redefinition_flags; }
+  bool check_redefinition_flag(int flags) const { return (_redefinition_flags & flags) != 0; }
+  void clear_redefinition_flag(int flag) { _redefinition_flags &= ~flag; }
+  void set_redefinition_flag(int flag)   { _redefinition_flags |= flag; }
+  void set_redefinition_flags(int flags) { _redefinition_flags = flags; }
+
+  const Klass* newest_version() const    { return _new_version == NULL ? this : _new_version->newest_version(); }
+        Klass* newest_version()          { return _new_version == NULL ? this : _new_version->newest_version(); }
+
+  const Klass* active_version() const   { return _new_version == NULL || _new_version->is_redefining() ? this : _new_version->active_version(); }
+        Klass* active_version()         { return _new_version == NULL || _new_version->is_redefining() ? this : _new_version->active_version(); }
+
+  // update information
+  int *update_information() const        { return _update_information; }
+  void set_update_information(int *info) { _update_information = info; }
+  bool is_copying_backwards() const      { return _is_copying_backwards; }
+  void set_copying_backwards(bool b)     { _is_copying_backwards = b; }
+
  protected:                                // internal accessors
   void     set_subklass(Klass* s);
   void     set_next_sibling(Klass* s);
 
  public:
+   enum RedefinitionFlags {
+     NoRedefinition,                             // This class is not redefined at all!
+     ModifyClass = 1,                            // There are changes to the class meta data.
+     ModifyClassSize = ModifyClass << 1,         // The size of the class meta data changes.
+     ModifyInstances = ModifyClassSize << 1,     // There are change to the instance format.
+     ModifyInstanceSize = ModifyInstances << 1,  // The size of instances changes.
+     RemoveSuperType = ModifyInstanceSize << 1,  // A super type of this class is removed.
+     MarkedAsAffected = RemoveSuperType << 1     // This class has been marked as an affected class.
+   };
 
   // Compiler support
   static ByteSize super_offset()                 { return in_ByteSize(offset_of(Klass, _super)); }
diff --git a/src/hotspot/share/oops/method.cpp b/src/hotspot/share/oops/method.cpp
index 3d8ade5496c..516f2bb8f2f 100644
--- a/src/hotspot/share/oops/method.cpp
+++ b/src/hotspot/share/oops/method.cpp
@@ -1540,6 +1540,8 @@ methodHandle Method::clone_with_new_data(const methodHandle& m, u_char* new_code
 
   // Reset correct method/const method, method size, and parameter info
   newm->set_constMethod(newcm);
+  newm->set_new_version(newm->new_version());
+  newm->set_old_version(newm->old_version());
   newm->constMethod()->set_code_size(new_code_length);
   newm->constMethod()->set_constMethod_size(new_const_method_size);
   assert(newm->code_size() == new_code_length, "check");
@@ -2204,6 +2206,10 @@ void Method::ensure_jmethod_ids(ClassLoaderData* loader_data, int capacity) {
 
 // Add a method id to the jmethod_ids
 jmethodID Method::make_jmethod_id(ClassLoaderData* loader_data, Method* m) {
+  // FIXME: (DCEVM) ???
+  if (m != m->newest_version()) {
+    m = m->newest_version();
+  }
   ClassLoaderData* cld = loader_data;
 
   if (!SafepointSynchronize::is_at_safepoint()) {
diff --git a/src/hotspot/share/oops/method.hpp b/src/hotspot/share/oops/method.hpp
index 5b912ca6f5d..83ed2d9c3c1 100644
--- a/src/hotspot/share/oops/method.hpp
+++ b/src/hotspot/share/oops/method.hpp
@@ -78,6 +78,9 @@ class Method : public Metadata {
   MethodCounters*   _method_counters;
   AccessFlags       _access_flags;               // Access flags
   int               _vtable_index;               // vtable index of this method (see VtableIndexFlag)
+  // (DCEVM) Newer version of method available?
+  Method*           _new_version;
+  Method*           _old_version;
                                                  // note: can have vtables with >2**16 elements (because of inheritance)
   u2                _intrinsic_id;               // vmSymbols::intrinsic_id (0 == _none)
 
@@ -154,6 +157,23 @@ class Method : public Metadata {
   int name_index() const                         { return constMethod()->name_index();         }
   void set_name_index(int index)                 { constMethod()->set_name_index(index);       }
 
+  Method* new_version() const                    { return _new_version; }
+  void set_new_version(Method* m)                { _new_version = m; }
+  Method* newest_version()                       { return (_new_version == NULL) ? this : _new_version->newest_version(); }
+
+  Method* old_version() const                    { return _old_version; }
+  void set_old_version(Method* m) {
+    /*if (m == NULL) {
+      _old_version = NULL;
+      return;
+    }*/
+
+    assert(_old_version == NULL, "may only be set once");
+    assert(this->code_size() == m->code_size(), "must have same code length");
+    _old_version = m;
+  }
+  const Method* oldest_version() const           { return (_old_version == NULL) ? this : _old_version->oldest_version(); }
+
   // signature
   Symbol* signature() const                      { return constants()->symbol_at(signature_index()); }
   int signature_index() const                    { return constMethod()->signature_index();         }
diff --git a/src/hotspot/share/prims/jni.cpp b/src/hotspot/share/prims/jni.cpp
index 3c91e8e2ead..51243524106 100644
--- a/src/hotspot/share/prims/jni.cpp
+++ b/src/hotspot/share/prims/jni.cpp
@@ -338,6 +338,7 @@ JNI_ENTRY(jclass, jni_DefineClass(JNIEnv *env, const char *name, jobject loaderR
                                                    class_loader,
                                                    Handle(),
                                                    &st,
+                                                   NULL,
                                                    CHECK_NULL);
 
   if (log_is_enabled(Debug, class, resolve) && k != NULL) {
diff --git a/src/hotspot/share/prims/jvm.cpp b/src/hotspot/share/prims/jvm.cpp
index 2d866dee6dd..333b65ccfc1 100644
--- a/src/hotspot/share/prims/jvm.cpp
+++ b/src/hotspot/share/prims/jvm.cpp
@@ -963,6 +963,7 @@ static jclass jvm_define_class_common(JNIEnv *env, const char *name,
                                                    class_loader,
                                                    protection_domain,
                                                    &st,
+                                                   NULL,
                                                    CHECK_NULL);
 
   if (log_is_enabled(Debug, class, resolve) && k != NULL) {
diff --git a/src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.cpp b/src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.cpp
new file mode 100644
index 00000000000..83c0952de37
--- /dev/null
+++ b/src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.cpp
@@ -0,0 +1,2255 @@
+/*
+ * Copyright (c) 2003, 2016, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include "precompiled.hpp"
+#include "aot/aotLoader.hpp"
+#include "classfile/classFileStream.hpp"
+#include "classfile/metadataOnStackMark.hpp"
+#include "classfile/systemDictionary.hpp"
+#include "classfile/verifier.hpp"
+#include "interpreter/oopMapCache.hpp"
+#include "interpreter/rewriter.hpp"
+#include "logging/logStream.hpp"
+#include "memory/metadataFactory.hpp"
+#include "memory/metaspaceShared.hpp"
+#include "memory/resourceArea.hpp"
+#include "memory/iterator.inline.hpp"
+#include "gc/serial/markSweep.hpp" // FIXME: other GC?
+#include "oops/fieldStreams.hpp"
+#include "oops/klassVtable.hpp"
+#include "oops/oop.inline.hpp"
+#include "oops/constantPool.inline.hpp"
+#include "prims/jvmtiImpl.hpp"
+#include "prims/jvmtiClassFileReconstituter.hpp"
+#include "prims/jvmtiEnhancedRedefineClasses.hpp"
+#include "prims/methodComparator.hpp"
+#include "prims/resolvedMethodTable.hpp"
+#include "runtime/deoptimization.hpp"
+#include "runtime/jniHandles.inline.hpp"
+#include "runtime/relocator.hpp"
+#include "utilities/bitMap.inline.hpp"
+#include "prims/jvmtiThreadState.inline.hpp"
+#include "utilities/events.hpp"
+#include "oops/constantPool.inline.hpp"
+
+Array<Method*>* VM_EnhancedRedefineClasses::_old_methods = NULL;
+Array<Method*>* VM_EnhancedRedefineClasses::_new_methods = NULL;
+Method**  VM_EnhancedRedefineClasses::_matching_old_methods = NULL;
+Method**  VM_EnhancedRedefineClasses::_matching_new_methods = NULL;
+Method**  VM_EnhancedRedefineClasses::_deleted_methods      = NULL;
+Method**  VM_EnhancedRedefineClasses::_added_methods        = NULL;
+int         VM_EnhancedRedefineClasses::_matching_methods_length = 0;
+int         VM_EnhancedRedefineClasses::_deleted_methods_length  = 0;
+int         VM_EnhancedRedefineClasses::_added_methods_length    = 0;
+Klass*      VM_EnhancedRedefineClasses::_the_class_oop = NULL;
+
+/**
+ * Create new instance of enhanced class redefiner.
+ *
+ * This class implements VM_GC_Operation - the usual usage should be:
+ *     VM_EnhancedRedefineClasses op(class_count, class_definitions, jvmti_class_load_kind_redefine);
+ *     VMThread::execute(&op);
+ * Which
+ *
+ * @param class_count size of class_defs
+ * @param class_defs class definition - either new class or redefined class
+ *               note that this is not the final array of classes to be redefined
+ *               we need to scan for all affected classes (e.g. subclasses) and
+ *               caculcate redefinition for them as well.
+ * @param class_load_kind always jvmti_class_load_kind_redefine
+ */
+VM_EnhancedRedefineClasses::VM_EnhancedRedefineClasses(jint class_count, const jvmtiClassDefinition *class_defs, JvmtiClassLoadKind class_load_kind) :
+        VM_GC_Operation(Universe::heap()->total_collections(), GCCause::_heap_inspection, Universe::heap()->total_full_collections(), true) {
+  _affected_klasses = NULL;
+  _class_count = class_count;
+  _class_defs = class_defs;
+  _class_load_kind = class_load_kind;
+  _res = JVMTI_ERROR_NONE;
+  _any_class_has_resolved_methods = false;
+}
+
+static inline InstanceKlass* get_ik(jclass def) {
+  oop mirror = JNIHandles::resolve_non_null(def);
+  return InstanceKlass::cast(java_lang_Class::as_Klass(mirror));
+}
+
+/**
+ * Start the redefinition:
+ * - Load new class definitions - @see load_new_class_versions
+ * - Start mark&sweep GC.
+ * @return true if success, otherwise all chnages are rollbacked.
+ */
+bool VM_EnhancedRedefineClasses::doit_prologue() {
+
+  if (_class_count == 0) {
+    _res = JVMTI_ERROR_NONE;
+    return false;
+  }
+  if (_class_defs == NULL) {
+    _res = JVMTI_ERROR_NULL_POINTER;
+    return false;
+  }
+  for (int i = 0; i < _class_count; i++) {
+    if (_class_defs[i].klass == NULL) {
+      _res = JVMTI_ERROR_INVALID_CLASS;
+      return false;
+    }
+    if (_class_defs[i].class_byte_count == 0) {
+      _res = JVMTI_ERROR_INVALID_CLASS_FORMAT;
+      return false;
+    }
+    if (_class_defs[i].class_bytes == NULL) {
+      _res = JVMTI_ERROR_NULL_POINTER;
+      return false;
+    }
+
+    // classes for primitives and arrays and vm anonymous classes cannot be redefined
+    // check here so following code can assume these classes are InstanceKlass
+    oop mirror = JNIHandles::resolve_non_null(_class_defs[i].klass);
+    if (!is_modifiable_class(mirror)) {
+      _res = JVMTI_ERROR_UNMODIFIABLE_CLASS;
+      return false;
+    }
+  }
+
+  // Start timer after all the sanity checks; not quite accurate, but
+  // better than adding a bunch of stop() calls.
+  if (log_is_enabled(Info, redefine, class, timer)) {
+    _timer_vm_op_prologue.start();
+  }
+
+  // We first load new class versions in the prologue, because somewhere down the
+  // call chain it is required that the current thread is a Java thread.
+  _res = load_new_class_versions(Thread::current());
+
+  // prepare GC, lock heap
+  if (_res == JVMTI_ERROR_NONE && !VM_GC_Operation::doit_prologue()) {
+    _res = JVMTI_ERROR_INTERNAL;
+  }
+
+  if (_res != JVMTI_ERROR_NONE) {
+    rollback();
+    // TODO free any successfully created classes
+    /*for (int i = 0; i < _class_count; i++) {
+      if (_new_classes[i] != NULL) {
+        ClassLoaderData* cld = _new_classes[i]->class_loader_data();
+        // Free the memory for this class at class unloading time.  Not before
+        // because CMS might think this is still live.
+        cld->add_to_deallocate_list(InstanceKlass::cast(_new_classes[i]));
+      }
+    }*/
+    delete _new_classes;
+    _new_classes = NULL;
+    delete _affected_klasses;
+    _affected_klasses = NULL;
+
+    _timer_vm_op_prologue.stop();
+    return false;
+  }
+
+  _timer_vm_op_prologue.stop();
+  return true;
+}
+
+/**
+ * Closer for static fields - copy value from old class to the new class.
+ */
+class FieldCopier : public FieldClosure {
+  public:
+  void do_field(fieldDescriptor* fd) {
+    InstanceKlass* cur = InstanceKlass::cast(fd->field_holder());
+    oop cur_oop = cur->java_mirror();
+
+    InstanceKlass* old = InstanceKlass::cast(cur->old_version());
+    oop old_oop = old->java_mirror();
+
+    fieldDescriptor result;
+    bool found = old->find_local_field(fd->name(), fd->signature(), &result);
+    if (found && result.is_static()) {
+      log_trace(redefine, class, obsolete, metadata)("Copying static field value for field %s old_offset=%d new_offset=%d",
+                                               fd->name()->as_C_string(), result.offset(), fd->offset());
+      memcpy(cur_oop->obj_field_addr_raw<HeapWord>(fd->offset()),
+             old_oop->obj_field_addr_raw<HeapWord>(result.offset()),
+             type2aelembytes(fd->field_type()));
+
+      // Static fields may have references to java.lang.Class
+      if (fd->field_type() == T_OBJECT) {
+         oop oop = cur_oop->obj_field(fd->offset());
+         if (oop != NULL && oop->is_instance() && InstanceKlass::cast(oop->klass())->is_mirror_instance_klass()) {
+            Klass* klass = java_lang_Class::as_Klass(oop);
+            if (klass != NULL && klass->is_instance_klass()) {
+              assert(oop == InstanceKlass::cast(klass)->java_mirror(), "just checking");
+              if (klass->new_version() != NULL) {
+                oop = InstanceKlass::cast(klass->new_version())->java_mirror();
+                cur_oop->obj_field_put(fd->offset(), oop);
+              }
+            }
+         }
+        }
+      }
+    }
+};
+
+
+// TODO: review...
+void VM_EnhancedRedefineClasses::mark_as_scavengable(nmethod* nm) {
+  if (!nm->on_scavenge_root_list()) {
+    CodeCache::add_scavenge_root_nmethod(nm);
+  }
+}
+
+// TODO comment
+struct StoreBarrier {
+  // TODO: j10 review change ::oop_store -> HeapAccess<>::oop_store
+  template <class T> static void oop_store(T* p, oop v) { HeapAccess<>::oop_store(p, v); }
+};
+
+
+// TODO comment
+struct StoreNoBarrier {
+  template <class T> static void oop_store(T* p, oop v) { RawAccess<IS_NOT_NULL>::oop_store(p, v); }
+};
+
+/**
+  Closure to scan all heap objects and update method handles
+*/
+template <class S>
+class ChangePointersOopClosure : public BasicOopIterateClosure {
+  // import java_lang_invoke_MemberName.*
+  enum {
+    REFERENCE_KIND_SHIFT = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,
+    REFERENCE_KIND_MASK  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,
+  };
+
+
+  bool update_member_name(oop obj) {
+    int flags    =       java_lang_invoke_MemberName::flags(obj);
+    int ref_kind =       (flags >> REFERENCE_KIND_SHIFT) & REFERENCE_KIND_MASK;
+    if (MethodHandles::ref_kind_is_method(ref_kind)) {
+      Method* m = (Method*) java_lang_invoke_MemberName::vmtarget(obj);
+      if (m != NULL && m->method_holder()->new_version() != NULL) {
+        // Let's try to re-resolve method
+        InstanceKlass* newest = InstanceKlass::cast(m->method_holder()->newest_version());
+        Method* new_method = newest->find_method(m->name(), m->signature());
+
+        if (new_method != NULL) {
+          // Note: we might set NULL at this point, which should force AbstractMethodError at runtime
+          Thread *thread = Thread::current();
+          CallInfo info(new_method, newest, thread);
+          Handle objHandle(thread, obj);  // TODO : review thread
+          MethodHandles::init_method_MemberName(objHandle, info);
+        } else {
+          java_lang_invoke_MemberName::set_method(obj, NULL);
+        }
+      }
+    } else if (MethodHandles::ref_kind_is_field(ref_kind)) {
+      Klass* k = (Klass*) java_lang_invoke_MemberName::vmtarget(obj);
+      if (k == NULL) {
+        return false; // Was cleared before, this MemberName is invalid.
+      }
+
+      if (k != NULL && k->new_version() != NULL) {
+        // Let's try to re-resolve field
+        fieldDescriptor fd;
+        int offset = java_lang_invoke_MemberName::vmindex(obj);
+        bool is_static = MethodHandles::ref_kind_is_static(ref_kind);
+        InstanceKlass* ik = InstanceKlass::cast(k);
+        if (ik->find_local_field_from_offset(offset, is_static, &fd)) {
+          InstanceKlass* newest = InstanceKlass::cast(k->newest_version());
+          fieldDescriptor fd_new;
+          if (newest->find_local_field(fd.name(), fd.signature(), &fd_new)) {
+            Handle objHandle(Thread::current(), obj);  // TODO : review thread
+            MethodHandles::init_field_MemberName(objHandle, fd_new, MethodHandles::ref_kind_is_setter(ref_kind));
+          } else {
+            // Matching field is not found in new version, not much we can do here.
+            // JVM will crash once faulty MH is invoked.
+            // However, to avoid that all DMH's using this faulty MH are cleared (set to NULL)
+            // Eventually, we probably want to replace them with something more meaningful,
+            // like instance throwing NoSuchFieldError or DMH that will resort to dynamic
+            // field resolution (with possibility of type conversion)
+            java_lang_invoke_MemberName::set_method(obj, NULL);
+            java_lang_invoke_MemberName::set_vmindex(obj, 0);
+            return false;
+          }
+        }
+      }
+    }
+    return true;
+  }
+
+  bool update_direct_method_handle(oop obj) {
+    // Always update member name first.
+    oop mem_name = java_lang_invoke_DirectMethodHandle::member(obj);
+    if (!update_member_name(mem_name)) {
+      return false;
+    }
+
+    // Here we rely on DirectMethodHandle implementation.
+    // The current implementation caches field offset in $StaticAccessor/$Accessor
+    int flags    =       java_lang_invoke_MemberName::flags(mem_name);
+    int ref_kind =       (flags >> REFERENCE_KIND_SHIFT) & REFERENCE_KIND_MASK;
+    if (MethodHandles::ref_kind_is_field(ref_kind)) {
+      // Note: we don't care about staticBase field (which is java.lang.Class)
+      // It should be processed during normal object update.
+      // Update offset in StaticAccessor
+      int offset = java_lang_invoke_MemberName::vmindex(mem_name);
+      if (offset != 0) { // index of 0 means that field no longer exist
+        if (java_lang_invoke_DirectMethodHandle_StaticAccessor::is_instance(obj)) {
+          java_lang_invoke_DirectMethodHandle_StaticAccessor::set_static_offset(obj, offset);
+        } else if (java_lang_invoke_DirectMethodHandle_Accessor::is_instance(obj)) {
+          java_lang_invoke_DirectMethodHandle_Accessor::set_field_offset(obj, offset);
+        }
+      }
+    }
+    return true;
+  }
+
+  // Forward pointers to InstanceKlass and mirror class to new versions
+  template <class T>
+  inline void do_oop_work(T* p) {
+    oop obj = RawAccess<>::oop_load(p);
+    if (obj == NULL) {
+      return;
+    }
+    if (obj->is_instance() && InstanceKlass::cast(obj->klass())->is_mirror_instance_klass()) {
+      Klass* klass = java_lang_Class::as_Klass(obj);
+      if (klass != NULL && klass->is_instance_klass()) {
+        assert(obj == InstanceKlass::cast(klass)->java_mirror(), "just checking");
+        if (klass->new_version() != NULL) {
+          obj = InstanceKlass::cast(klass->new_version())->java_mirror();
+          S::oop_store(p, obj);
+        }
+      }
+    }
+
+    // JSR 292 support, uptade java.lang.invoke.MemberName instances
+    if (java_lang_invoke_MemberName::is_instance(obj)) {
+      update_member_name(obj);
+    } else if (java_lang_invoke_DirectMethodHandle::is_instance(obj)) {
+      if (!update_direct_method_handle(obj)) {
+        // DMH is no longer valid, replace it with null reference.
+        // See note above. We probably want to replace this with something more meaningful.
+        S::oop_store(p, NULL);
+      }
+    }
+  }
+
+  virtual void do_oop(oop* o) {
+    do_oop_work(o);
+  }
+
+  virtual void do_oop(narrowOop* o) {
+    do_oop_work(o);
+  }
+};
+
+/**
+ * Closure to scan all objects on heap for objects of changed classes
+ *   - if the fields are compatible, only update class definition reference
+ *   - otherwise if the new object size is smaller then old size, reshufle
+ *          the fields and fill the gap with "dead_space"
+ *   - otherwise set the _needs_instance_update flag, we need to do full GC
+ *          and reshuffle object positions durring mark&sweep
+ */
+class ChangePointersObjectClosure : public ObjectClosure {
+  private:
+
+  OopIterateClosure *_closure;
+  bool _needs_instance_update;
+  oop _tmp_obj;
+  int _tmp_obj_size;
+
+public:
+  ChangePointersObjectClosure(OopIterateClosure *closure) : _closure(closure), _needs_instance_update(false), _tmp_obj(NULL), _tmp_obj_size(0) {}
+
+  bool needs_instance_update() {
+    return _needs_instance_update;
+  }
+
+  void copy_to_tmp(oop o) {
+    int size = o->size();
+    if (_tmp_obj_size < size) {
+      _tmp_obj_size = size;
+      _tmp_obj = (oop)resource_allocate_bytes(size * HeapWordSize);
+    }
+    Copy::aligned_disjoint_words((HeapWord*)o, (HeapWord*)_tmp_obj, size);
+  }
+
+  virtual void do_object(oop obj) {
+    if (obj->is_instance() && InstanceKlass::cast(obj->klass())->is_mirror_instance_klass()) {
+      // static fields may have references to old java.lang.Class instances, update them
+      // at the same time, we don't want to update other oops in the java.lang.Class
+      // Causes SIGSEGV?
+      //instanceMirrorKlass::oop_fields_iterate(obj, _closure);
+    } else {
+      obj->oop_iterate(_closure);
+    }
+
+    if (obj->klass()->new_version() != NULL) {
+      Klass* new_klass = obj->klass()->new_version();
+
+      if (new_klass->update_information() != NULL) {
+        int size_diff = obj->size() - obj->size_given_klass(new_klass);
+
+        // Either new size is bigger or gap is to small to be filled
+        if (size_diff < 0 || (size_diff > 0 && (size_t) size_diff < CollectedHeap::min_fill_size())) {
+          // We need an instance update => set back to old klass
+          _needs_instance_update = true;
+        } else {
+          oop src = obj;
+          if (new_klass->is_copying_backwards()) {
+            copy_to_tmp(obj);
+            src = _tmp_obj;
+          }
+          src->set_klass(obj->klass()->new_version());
+          //  FIXME: instance updates...
+          //guarantee(false, "instance updates!");
+          MarkSweep::update_fields(obj, src, new_klass->update_information());
+
+          if (size_diff > 0) {
+            HeapWord* dead_space = ((HeapWord *)obj) + obj->size();
+            CollectedHeap::fill_with_object(dead_space, size_diff);
+          }
+        }
+      } else {
+        obj->set_klass(obj->klass()->new_version());
+      }
+    }
+  }
+};
+
+
+/**
+  Main transformation method - runs in VM thread.
+
+  - UseSharedSpaces - TODO what does it mean?
+  - for each sratch class call redefine_single_class
+  - clear code cache (flush_dependent_code)
+  - iterate the heap and update object defintions, check it old/new class fields
+       are compatible. If new class size is smaller then old, it can be solved directly here.
+  - iterate the heap and update method handles to new version
+  - Swap marks to have same hashcodes
+  - copy static fields
+  - notify JVM of the modification
+*/
+void VM_EnhancedRedefineClasses::doit() {
+  Thread *thread = Thread::current();
+
+  if (UseSharedSpaces) {
+    // Sharing is enabled so we remap the shared readonly space to
+    // shared readwrite, private just in case we need to redefine
+    // a shared class. We do the remap during the doit() phase of
+    // the safepoint to be safer.
+    if (!MetaspaceShared::remap_shared_readonly_as_readwrite()) {
+      log_info(redefine, class, load)("failed to remap shared readonly space to readwrite, private");
+      _res = JVMTI_ERROR_INTERNAL;
+      return;
+    }
+  }
+
+  // Mark methods seen on stack and everywhere else so old methods are not
+  // cleaned up if they're on the stack.
+  MetadataOnStackMark md_on_stack(true);
+  HandleMark hm(thread);   // make sure any handles created are deleted
+                           // before the stack walk again.
+
+  for (int i = 0; i < _new_classes->length(); i++) {
+    redefine_single_class(_new_classes->at(i), thread);
+  }
+
+  // Deoptimize all compiled code that depends on this class (do only once, because it clears whole cache)
+  flush_dependent_code(NULL, thread);
+
+  // JSR-292 support
+  if (_any_class_has_resolved_methods) {
+    bool trace_name_printed = false;
+    ResolvedMethodTable::adjust_method_entries(&trace_name_printed);
+  }
+
+  ChangePointersOopClosure<StoreNoBarrier> oopClosureNoBarrier;
+  ChangePointersOopClosure<StoreBarrier> oopClosure;
+  ChangePointersObjectClosure objectClosure(&oopClosure);
+
+  log_trace(redefine, class, obsolete, metadata)("Before updating instances");
+  {
+    // Since we may update oops inside nmethod's code blob to point to java.lang.Class in new generation, we need to
+    // make sure such references are properly recognized by GC. For that, If ScavengeRootsInCode is true, we need to
+    // mark such nmethod's as "scavengable".
+    // For now, mark all nmethod's as scavengable that are not scavengable already
+    if (ScavengeRootsInCode) {
+      CodeCache::nmethods_do(mark_as_scavengable);
+    }
+
+    Universe::heap()->ensure_parsability(false);
+    Universe::heap()->object_iterate(&objectClosure);
+    Universe::root_oops_do(&oopClosureNoBarrier);
+  }
+  log_trace(redefine, class, obsolete, metadata)("After updating instances");
+
+  for (int i = 0; i < _new_classes->length(); i++) {
+    InstanceKlass* cur = InstanceKlass::cast(_new_classes->at(i));
+    InstanceKlass* old = InstanceKlass::cast(cur->old_version());
+
+    // Swap marks to have same hashcodes
+    markOop cur_mark = cur->prototype_header();
+    markOop old_mark = old->prototype_header();
+    cur->set_prototype_header(old_mark);
+    old->set_prototype_header(cur_mark);
+
+    //swap_marks(cur, old);
+    cur_mark = cur->java_mirror()->mark();
+    old_mark = old->java_mirror()->mark();
+    cur->java_mirror()->set_mark(old_mark);
+    old->java_mirror()->set_mark(cur_mark);
+
+
+      // Revert pool holder for old version of klass (it was updated by one of ours closure!)
+    old->constants()->set_pool_holder(old);
+
+    Klass* array_klasses = old->array_klasses();
+    if (array_klasses != NULL) {
+      assert(cur->array_klasses() == NULL, "just checking");
+
+      // Transfer the array classes, otherwise we might get cast exceptions when casting array types.
+      // Also, set array klasses element klass.
+      cur->set_array_klasses(array_klasses);
+      ObjArrayKlass::cast(array_klasses)->set_element_klass(cur);
+      java_lang_Class::release_set_array_klass(cur->java_mirror(), array_klasses);
+      java_lang_Class::set_component_mirror(array_klasses->java_mirror(), cur->java_mirror());
+    }
+
+    // Initialize the new class! Special static initialization that does not execute the
+    // static constructor but copies static field values from the old class if name
+    // and signature of a static field match.
+    FieldCopier copier;
+    cur->do_local_static_fields(&copier); // TODO (tw): What about internal static fields??
+    //java_lang_Class::set_klass(old->java_mirror(), cur); // FIXME-isd (from JDK8): is that correct?
+    //FIXME-isd (from JDK8): do we need this: ??? old->set_java_mirror(cur->java_mirror());
+
+    // Transfer init state
+    InstanceKlass::ClassState state = old->init_state();
+    if (state > InstanceKlass::linked) {
+      cur->set_init_state(state);
+    }
+  }
+
+//  if (objectClosure.needs_instance_update()) {
+    // Do a full garbage collection to update the instance sizes accordingly
+    Universe::set_redefining_gc_run(true);
+    notify_gc_begin(true);
+    Universe::heap()->collect_as_vm_thread(GCCause::_heap_inspection);
+    notify_gc_end();
+    Universe::set_redefining_gc_run(false);
+//  }
+
+  // Unmark Klass*s as "redefining"
+  for (int i = 0; i < _new_classes->length(); i++) {
+    InstanceKlass* cur = _new_classes->at(i);
+    cur->set_redefining(false);
+    cur->clear_update_information();
+  }
+
+  // TODO: explain...
+  SystemDictionary::update_constraints_after_redefinition();
+
+  // TODO: explain...
+  ciObjectFactory::resort_shared_ci_metadata();
+
+  // FIXME - check if it was in JDK8. Copied from standard JDK9 hotswap.
+  //MethodDataCleaner clean_weak_method_links;
+  //ClassLoaderDataGraph::classes_do(&clean_weak_method_links);
+
+  // Disable any dependent concurrent compilations
+  SystemDictionary::notice_modification();
+
+  // Set flag indicating that some invariants are no longer true.
+  // See jvmtiExport.hpp for detailed explanation.
+  JvmtiExport::set_has_redefined_a_class();
+
+  // check_class() is optionally called for product bits, but is
+  // always called for non-product bits.
+#ifdef PRODUCT
+  if (log_is_enabled(Trace, redefine, class, obsolete, metadata)) {
+#endif
+    log_trace(redefine, class, obsolete, metadata)("calling check_class");
+    CheckClass check_class(thread);
+    ClassLoaderDataGraph::classes_do(&check_class);
+#ifdef PRODUCT
+  }
+#endif
+}
+
+/**
+ * Cleanup - runs in JVM thread
+ *  - free used memory
+ *  - end GC
+ */
+void VM_EnhancedRedefineClasses::doit_epilogue() {
+  VM_GC_Operation::doit_epilogue();
+
+  if (_new_classes != NULL) {
+    delete _new_classes;
+  }
+  _new_classes = NULL;
+  if (_affected_klasses != NULL) {
+    delete _affected_klasses;
+  }
+  _affected_klasses = NULL;
+
+  // Reset the_class_oop to null for error printing.
+  _the_class_oop = NULL;
+
+  if (log_is_enabled(Info, redefine, class, timer)) {
+    // Used to have separate timers for "doit" and "all", but the timer
+    // overhead skewed the measurements.
+    jlong doit_time = _timer_rsc_phase1.milliseconds() +
+                      _timer_rsc_phase2.milliseconds();
+    jlong all_time = _timer_vm_op_prologue.milliseconds() + doit_time;
+
+    log_info(redefine, class, timer)
+      ("vm_op: all=" JLONG_FORMAT "  prologue=" JLONG_FORMAT "  doit=" JLONG_FORMAT,
+       all_time, _timer_vm_op_prologue.milliseconds(), doit_time);
+    log_info(redefine, class, timer)
+      ("redefine_single_class: phase1=" JLONG_FORMAT "  phase2=" JLONG_FORMAT,
+       _timer_rsc_phase1.milliseconds(), _timer_rsc_phase2.milliseconds());
+  }
+}
+
+/**
+ * Exclude java primitives and arrays from redefinition
+ * @param klass_mirror  pointer to the klass
+ * @return true if is modifiable
+ */
+bool VM_EnhancedRedefineClasses::is_modifiable_class(oop klass_mirror) {
+  // classes for primitives cannot be redefined
+  if (java_lang_Class::is_primitive(klass_mirror)) {
+    return false;
+  }
+  Klass* k = java_lang_Class::as_Klass(klass_mirror);
+  // classes for arrays cannot be redefined
+  if (k == NULL || !k->is_instance_klass()) {
+    return false;
+  }
+
+  // Cannot redefine or retransform an anonymous class.
+  if (InstanceKlass::cast(k)->is_anonymous()) {
+    return false;
+  }
+  return true;
+}
+
+/**
+  Load and link new classes (either redefined or affected by redefinition - subclass, ...)
+
+  - find sorted affected classes
+  - resolve new class
+  - calculate redefine flags (field change, method change, supertype change, ...)
+  - calculate modified fields and mapping to old fields
+  - link new classes
+
+  The result is sotred in _affected_klasses(old definitions) and _new_classes(new definitions) arrays.
+*/
+jvmtiError VM_EnhancedRedefineClasses::load_new_class_versions(TRAPS) {
+
+  _affected_klasses = new (ResourceObj::C_HEAP, mtInternal) GrowableArray<Klass*>(_class_count, true);
+  _new_classes = new (ResourceObj::C_HEAP, mtInternal) GrowableArray<InstanceKlass*>(_class_count, true);
+
+  ResourceMark rm(THREAD);
+
+  // Retrieve an array of all classes that need to be redefined into _affected_klasses
+  jvmtiError err = find_sorted_affected_classes(THREAD);
+  if (err != JVMTI_ERROR_NONE) {
+    return err;
+  }
+
+  // thread local state - used to transfer class_being_redefined object to SystemDictonery::resolve_from_stream
+  JvmtiThreadState *state = JvmtiThreadState::state_for(JavaThread::current());
+  // state can only be NULL if the current thread is exiting which
+  // should not happen since we're trying to do a RedefineClasses
+  guarantee(state != NULL, "exiting thread calling load_new_class_versions");
+
+  _max_redefinition_flags = Klass::NoRedefinition;
+
+  for (int i = 0; i < _affected_klasses->length(); i++) {
+    // Create HandleMark so that any handles created while loading new class
+    // versions are deleted. Constant pools are deallocated while merging
+    // constant pools
+    HandleMark hm(THREAD);
+    InstanceKlass* the_class = InstanceKlass::cast(_affected_klasses->at(i));
+    Symbol*  the_class_sym = the_class->name();
+
+    // Ensure class is linked before redefine
+    if (!the_class->is_linked()) {
+      the_class->link_class(THREAD);
+      if (HAS_PENDING_EXCEPTION) {
+        Symbol* ex_name = PENDING_EXCEPTION->klass()->name();
+        log_info(redefine, class, load, exceptions)("link_class exception: '%s'", ex_name->as_C_string());
+        CLEAR_PENDING_EXCEPTION;
+        if (ex_name == vmSymbols::java_lang_OutOfMemoryError()) {
+          return JVMTI_ERROR_OUT_OF_MEMORY;
+        } else {
+          return JVMTI_ERROR_INTERNAL;
+        }
+      }
+    }
+
+    log_debug(redefine, class, load)
+      ("loading name=%s kind=%d (avail_mem=" UINT64_FORMAT "K)",
+       the_class->external_name(), _class_load_kind, os::available_memory() >> 10);
+
+    // class bytes...
+    const unsigned char* class_bytes;
+    jint class_byte_count;
+    jvmtiError error;
+    jboolean not_changed;
+    if ((error = find_class_bytes(the_class, &class_bytes, &class_byte_count, &not_changed)) != JVMTI_ERROR_NONE) {
+      log_info(redefine, class, load, exceptions)("error finding class bytes: %d", (int) error);
+      return error;
+    }
+    assert(class_bytes != NULL && class_byte_count != 0, "class bytes should be defined at this point!");
+
+    ClassFileStream st((u1*)class_bytes,
+                       class_byte_count,
+                       "__VM_EnhancedRedefineClasses__",
+                       ClassFileStream::verify);
+
+    // Parse the stream.
+    Handle the_class_loader(THREAD, the_class->class_loader());
+    Handle protection_domain(THREAD, the_class->protection_domain());
+    // Set redefined class handle in JvmtiThreadState class.
+    // This redefined class is sent to agent event handler for class file
+    // load hook event.
+    state->set_class_being_redefined(the_class, _class_load_kind);
+
+    InstanceKlass* k = SystemDictionary::resolve_from_stream(the_class_sym,
+                                                the_class_loader,
+                                                protection_domain,
+                                                &st,
+                                                the_class,
+                                                THREAD);
+    // Clear class_being_redefined just to be sure.
+    state->clear_class_being_redefined();
+
+    if (HAS_PENDING_EXCEPTION) {
+      Symbol* ex_name = PENDING_EXCEPTION->klass()->name();
+      log_info(redefine, class, load, exceptions)("parse_stream exception: '%s'", ex_name->as_C_string());
+      CLEAR_PENDING_EXCEPTION;
+
+      if (ex_name == vmSymbols::java_lang_UnsupportedClassVersionError()) {
+        return JVMTI_ERROR_UNSUPPORTED_VERSION;
+      } else if (ex_name == vmSymbols::java_lang_ClassFormatError()) {
+        return JVMTI_ERROR_INVALID_CLASS_FORMAT;
+      } else if (ex_name == vmSymbols::java_lang_ClassCircularityError()) {
+        return JVMTI_ERROR_CIRCULAR_CLASS_DEFINITION;
+      } else if (ex_name == vmSymbols::java_lang_NoClassDefFoundError()) {
+        // The message will be "XXX (wrong name: YYY)"
+        return JVMTI_ERROR_NAMES_DONT_MATCH;
+      } else if (ex_name == vmSymbols::java_lang_OutOfMemoryError()) {
+        return JVMTI_ERROR_OUT_OF_MEMORY;
+      } else {  // Just in case more exceptions can be thrown..
+        return JVMTI_ERROR_FAILS_VERIFICATION;
+      }
+    }
+
+    InstanceKlass* new_class = k;
+    the_class->set_new_version(new_class);
+    _new_classes->append(new_class);
+
+    int redefinition_flags = Klass::NoRedefinition;
+    if (not_changed) {
+      redefinition_flags = Klass::NoRedefinition;
+    } else {
+      redefinition_flags = calculate_redefinition_flags(new_class);
+      if (redefinition_flags >= Klass::RemoveSuperType) {
+        return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_HIERARCHY_CHANGED;
+      }
+    }
+
+    if (new_class->super() != NULL) {
+      redefinition_flags = redefinition_flags | new_class->super()->redefinition_flags();
+    }
+
+    for (int j = 0; j < new_class->local_interfaces()->length(); j++) {
+      redefinition_flags = redefinition_flags | (new_class->local_interfaces()->at(j))->redefinition_flags();
+    }
+
+    new_class->set_redefinition_flags(redefinition_flags);
+
+    _max_redefinition_flags = _max_redefinition_flags | redefinition_flags;
+
+    if ((redefinition_flags & Klass::ModifyInstances) != 0) {
+       calculate_instance_update_information(_new_classes->at(i));
+    } else {
+      // Fields were not changed, transfer special flags only
+      assert(new_class->layout_helper() >> 1 == new_class->old_version()->layout_helper() >> 1, "must be equal");
+      assert(new_class->fields()->length() == InstanceKlass::cast(new_class->old_version())->fields()->length(), "must be equal");
+
+      JavaFieldStream old_fs(the_class);
+      JavaFieldStream new_fs(new_class);
+      for (; !old_fs.done() && !new_fs.done(); old_fs.next(), new_fs.next()) {
+        AccessFlags flags = new_fs.access_flags();
+        flags.set_is_field_modification_watched(old_fs.access_flags().is_field_modification_watched());
+        flags.set_is_field_access_watched(old_fs.access_flags().is_field_access_watched());
+        flags.set_has_field_initialized_final_update(old_fs.access_flags().has_field_initialized_final_update());
+        new_fs.set_access_flags(flags);
+      }
+    }
+
+    log_debug(redefine, class, load)
+      ("loaded name=%s (avail_mem=" UINT64_FORMAT "K)", the_class->external_name(), os::available_memory() >> 10);
+  }
+
+  // Link and verify new classes _after_ all classes have been updated in the system dictionary!
+  for (int i = 0; i < _affected_klasses->length(); i++) {
+    Klass* the_class = _affected_klasses->at(i);
+    assert (the_class->new_version() != NULL, "new version must be present");
+    InstanceKlass* new_class(InstanceKlass::cast(the_class->new_version()));
+
+    new_class->link_class(THREAD);
+
+    if (HAS_PENDING_EXCEPTION) {
+      Symbol* ex_name = PENDING_EXCEPTION->klass()->name();
+      log_info(redefine, class, load, exceptions)("link_class exception: '%s'", new_class->name()->as_C_string());
+      CLEAR_PENDING_EXCEPTION;
+      if (ex_name == vmSymbols::java_lang_OutOfMemoryError()) {
+        return JVMTI_ERROR_OUT_OF_MEMORY;
+      } else {
+        return JVMTI_ERROR_INTERNAL;
+      }
+    }
+  }
+  return JVMTI_ERROR_NONE;
+}
+
+/**
+  Calculated the difference between new and old class  (field change, method change, supertype change, ...).
+*/
+int VM_EnhancedRedefineClasses::calculate_redefinition_flags(InstanceKlass* new_class) {
+  int result = Klass::NoRedefinition;
+  log_info(redefine, class, load)("Comparing different class versions of class %s",new_class->name()->as_C_string());
+
+  assert(new_class->old_version() != NULL, "must have old version");
+  InstanceKlass* the_class = InstanceKlass::cast(new_class->old_version());
+
+  // Check whether class is in the error init state.
+  if (the_class->is_in_error_state()) {
+    // TBD #5057930: special error code is needed in 1.6
+    //result = Klass::union_redefinition_level(result, Klass::Invalid);
+  }
+
+  int i;
+
+  // Check superclasses
+  assert(new_class->super() == NULL || new_class->super()->new_version() == NULL, "superclass must be of newest version");
+  if (the_class->super() != new_class->super()) {
+    // Super class changed
+    Klass* cur_klass = the_class->super();
+    while (cur_klass != NULL) {
+      if (!new_class->is_subclass_of(cur_klass->newest_version())) {
+        log_info(redefine, class, load)("removed super class %s", cur_klass->name()->as_C_string());
+        result = result | Klass::RemoveSuperType | Klass::ModifyInstances | Klass::ModifyClass;
+      }
+      cur_klass = cur_klass->super();
+    }
+
+    cur_klass = new_class->super();
+    while (cur_klass != NULL) {
+      if (!the_class->is_subclass_of(cur_klass->old_version())) {
+        log_info(redefine, class, load)("added super class %s", cur_klass->name()->as_C_string());
+        result = result | Klass::ModifyClass | Klass::ModifyInstances;
+      }
+      cur_klass = cur_klass->super();
+    }
+  }
+
+  // Check interfaces
+
+  // Interfaces removed?
+  Array<Klass*>* old_interfaces = the_class->transitive_interfaces();
+  for (i = 0; i < old_interfaces->length(); i++) {
+    InstanceKlass* old_interface = InstanceKlass::cast(old_interfaces->at(i));
+    if (!new_class->implements_interface_any_version(old_interface)) {
+      result = result | Klass::RemoveSuperType | Klass::ModifyClass;
+      log_info(redefine, class, load)("removed interface %s", old_interface->name()->as_C_string());
+    }
+  }
+
+  // Interfaces added?
+  Array<Klass*>* new_interfaces = new_class->transitive_interfaces();
+  for (i = 0; i<new_interfaces->length(); i++) {
+    if (!the_class->implements_interface_any_version(new_interfaces->at(i))) {
+      result = result | Klass::ModifyClass;
+      log_info(redefine, class, load)("added interface %s", new_interfaces->at(i)->name()->as_C_string());
+    }
+  }
+
+  // Check whether class modifiers are the same.
+  jushort old_flags = (jushort) the_class->access_flags().get_flags();
+  jushort new_flags = (jushort) new_class->access_flags().get_flags();
+  if (old_flags != new_flags) {
+    // FIXME: Can this have any effects?
+  }
+
+  // Check if the number, names, types and order of fields declared in these classes
+  // are the same.
+  JavaFieldStream old_fs(the_class);
+  JavaFieldStream new_fs(new_class);
+  for (; !old_fs.done() && !new_fs.done(); old_fs.next(), new_fs.next()) {
+    // access
+    old_flags = old_fs.access_flags().as_short();
+    new_flags = new_fs.access_flags().as_short();
+    if ((old_flags ^ new_flags) & JVM_RECOGNIZED_FIELD_MODIFIERS) {
+      // FIXME: can this have any effect?
+    }
+    // offset
+    if (old_fs.offset() != new_fs.offset()) {
+      result = result | Klass::ModifyInstances;
+    }
+    // name and signature
+    Symbol* name_sym1 = the_class->constants()->symbol_at(old_fs.name_index());
+    Symbol* sig_sym1 = the_class->constants()->symbol_at(old_fs.signature_index());
+    Symbol* name_sym2 = new_class->constants()->symbol_at(new_fs.name_index());
+    Symbol* sig_sym2 = new_class->constants()->symbol_at(new_fs.signature_index());
+    if (name_sym1 != name_sym2 || sig_sym1 != sig_sym2) {
+      result = result | Klass::ModifyInstances;
+    }
+  }
+
+  // If both streams aren't done then we have a differing number of
+  // fields.
+  if (!old_fs.done() || !new_fs.done()) {
+    result = result | Klass::ModifyInstances;
+  }
+
+  // Do a parallel walk through the old and new methods. Detect
+  // cases where they match (exist in both), have been added in
+  // the new methods, or have been deleted (exist only in the
+  // old methods).  The class file parser places methods in order
+  // by method name, but does not order overloaded methods by
+  // signature.  In order to determine what fate befell the methods,
+  // this code places the overloaded new methods that have matching
+  // old methods in the same order as the old methods and places
+  // new overloaded methods at the end of overloaded methods of
+  // that name. The code for this order normalization is adapted
+  // from the algorithm used in InstanceKlass::find_method().
+  // Since we are swapping out of order entries as we find them,
+  // we only have to search forward through the overloaded methods.
+  // Methods which are added and have the same name as an existing
+  // method (but different signature) will be put at the end of
+  // the methods with that name, and the name mismatch code will
+  // handle them.
+  Array<Method*>* k_old_methods(the_class->methods());
+  Array<Method*>* k_new_methods(new_class->methods());
+  int n_old_methods = k_old_methods->length();
+  int n_new_methods = k_new_methods->length();
+  Thread* thread = Thread::current();
+
+  int ni = 0;
+  int oi = 0;
+  while (true) {
+    Method* k_old_method;
+    Method* k_new_method;
+    enum { matched, added, deleted, undetermined } method_was = undetermined;
+
+    if (oi >= n_old_methods) {
+      if (ni >= n_new_methods) {
+        break; // we've looked at everything, done
+      }
+      // New method at the end
+      k_new_method = k_new_methods->at(ni);
+      method_was = added;
+    } else if (ni >= n_new_methods) {
+      // Old method, at the end, is deleted
+      k_old_method = k_old_methods->at(oi);
+      method_was = deleted;
+    } else {
+      // There are more methods in both the old and new lists
+      k_old_method = k_old_methods->at(oi);
+      k_new_method = k_new_methods->at(ni);
+      if (k_old_method->name() != k_new_method->name()) {
+        // Methods are sorted by method name, so a mismatch means added
+        // or deleted
+        if (k_old_method->name()->fast_compare(k_new_method->name()) > 0) {
+          method_was = added;
+        } else {
+          method_was = deleted;
+        }
+      } else if (k_old_method->signature() == k_new_method->signature()) {
+        // Both the name and signature match
+        method_was = matched;
+      } else {
+        // The name matches, but the signature doesn't, which means we have to
+        // search forward through the new overloaded methods.
+        int nj;  // outside the loop for post-loop check
+        for (nj = ni + 1; nj < n_new_methods; nj++) {
+          Method* m = k_new_methods->at(nj);
+          if (k_old_method->name() != m->name()) {
+            // reached another method name so no more overloaded methods
+            method_was = deleted;
+            break;
+          }
+          if (k_old_method->signature() == m->signature()) {
+            // found a match so swap the methods
+            k_new_methods->at_put(ni, m);
+            k_new_methods->at_put(nj, k_new_method);
+            k_new_method = m;
+            method_was = matched;
+            break;
+          }
+        }
+
+        if (nj >= n_new_methods) {
+          // reached the end without a match; so method was deleted
+          method_was = deleted;
+        }
+      }
+    }
+
+    switch (method_was) {
+    case matched:
+      // methods match, be sure modifiers do too
+      old_flags = (jushort) k_old_method->access_flags().get_flags();
+      new_flags = (jushort) k_new_method->access_flags().get_flags();
+      if ((old_flags ^ new_flags) & ~(JVM_ACC_NATIVE)) {
+        // TODO Can this have any effects? Probably yes on vtables?
+        result = result | Klass::ModifyClass;
+      }
+      {
+        u2 new_num = k_new_method->method_idnum();
+        u2 old_num = k_old_method->method_idnum();
+        if (new_num != old_num) {
+        Method* idnum_owner = new_class->method_with_idnum(old_num);
+          if (idnum_owner != NULL) {
+            // There is already a method assigned this idnum -- switch them
+            // Take current and original idnum from the new_method
+            idnum_owner->set_method_idnum(new_num);
+            idnum_owner->set_orig_method_idnum(k_new_method->orig_method_idnum());
+          }
+          // Take current and original idnum from the old_method
+          k_new_method->set_method_idnum(old_num);
+          k_new_method->set_orig_method_idnum(k_old_method->orig_method_idnum());
+          if (thread->has_pending_exception()) {
+            return JVMTI_ERROR_OUT_OF_MEMORY;
+          }
+        }
+      }
+      log_trace(redefine, class, normalize)
+        ("Method matched: new: %s [%d] == old: %s [%d]",
+         k_new_method->name_and_sig_as_C_string(), ni, k_old_method->name_and_sig_as_C_string(), oi);
+      // advance to next pair of methods
+      ++oi;
+      ++ni;
+      break;
+    case added:
+      // method added, see if it is OK
+      new_flags = (jushort) k_new_method->access_flags().get_flags();
+      if ((new_flags & JVM_ACC_PRIVATE) == 0
+           // hack: private should be treated as final, but alas
+          || (new_flags & (JVM_ACC_FINAL|JVM_ACC_STATIC)) == 0
+         ) {
+        // new methods must be private
+        result = result | Klass::ModifyClass;
+      }
+      {
+        u2 num = new_class->next_method_idnum();
+        if (num == ConstMethod::UNSET_IDNUM) {
+          // cannot add any more methods
+        result = result | Klass::ModifyClass;
+        }
+        u2 new_num = k_new_method->method_idnum();
+        Method* idnum_owner = new_class->method_with_idnum(num);
+        if (idnum_owner != NULL) {
+          // There is already a method assigned this idnum -- switch them
+          // Take current and original idnum from the new_method
+          idnum_owner->set_method_idnum(new_num);
+          idnum_owner->set_orig_method_idnum(k_new_method->orig_method_idnum());
+        }
+        k_new_method->set_method_idnum(num);
+        k_new_method->set_orig_method_idnum(num);
+        if (thread->has_pending_exception()) {
+          return JVMTI_ERROR_OUT_OF_MEMORY;
+        }
+      }
+      log_trace(redefine, class, normalize)
+        ("Method added: new: %s [%d]", k_new_method->name_and_sig_as_C_string(), ni);
+      ++ni; // advance to next new method
+      break;
+    case deleted:
+      // method deleted, see if it is OK
+      old_flags = (jushort) k_old_method->access_flags().get_flags();
+      if ((old_flags & JVM_ACC_PRIVATE) == 0
+           // hack: private should be treated as final, but alas
+          || (old_flags & (JVM_ACC_FINAL|JVM_ACC_STATIC)) == 0
+         ) {
+        // deleted methods must be private
+        result = result | Klass::ModifyClass;
+      }
+      log_trace(redefine, class, normalize)
+        ("Method deleted: old: %s [%d]", k_old_method->name_and_sig_as_C_string(), oi);
+      ++oi; // advance to next old method
+      break;
+    default:
+      ShouldNotReachHere();
+    }
+  }
+
+  if (new_class->size() != new_class->old_version()->size()) {
+    result |= Klass::ModifyClassSize;
+  }
+
+  if (new_class->size_helper() != (InstanceKlass::cast((new_class->old_version()))->size_helper())) {
+    result |= Klass::ModifyInstanceSize;
+  }
+
+  // TODO Check method bodies to be able to return NoChange?
+  return result;
+}
+
+
+/** 
+  Searches for the class bytecode of the given class and returns it as a byte array.
+  
+  @param the_class definition of a class, either existing class or new_class
+  @param class_bytes - if the class is redefined, it contains new class definition, otherwise just original class bytecode.
+  @param class_byte_count - size of class_bytes
+  @param not_changed - new_class not available or same as current class
+*/
+jvmtiError VM_EnhancedRedefineClasses::find_class_bytes(InstanceKlass* the_class, const unsigned char **class_bytes, jint *class_byte_count, jboolean *not_changed) {
+
+  *not_changed = false;
+
+  // Search for the index in the redefinition array that corresponds to the current class
+  int i;
+  for (i = 0; i < _class_count; i++) {
+    if (the_class == get_ik(_class_defs[i].klass))
+      break;
+  }
+
+  if (i == _class_count) {
+    *not_changed = true;
+
+    // Redefine with same bytecodes. This is a class that is only indirectly affected by redefinition,
+    // so the user did not specify a different bytecode for that class.
+    if (the_class->get_cached_class_file_bytes() == NULL) {
+      // Not cached, we need to reconstitute the class file from the
+      // VM representation. We don't attach the reconstituted class
+      // bytes to the InstanceKlass here because they have not been
+      // validated and we're not at a safepoint.
+      JvmtiClassFileReconstituter reconstituter(the_class);
+      if (reconstituter.get_error() != JVMTI_ERROR_NONE) {
+        return reconstituter.get_error();
+      }
+
+      *class_byte_count = (jint)reconstituter.class_file_size();
+      *class_bytes      = (unsigned char*) reconstituter.class_file_bytes();
+    } else {
+      // it is cached, get it from the cache
+      *class_byte_count = the_class->get_cached_class_file_len();
+      *class_bytes      = the_class->get_cached_class_file_bytes();
+    }
+  } else {
+    // Redefine with bytecodes at index j
+    *class_bytes = _class_defs[i].class_bytes;
+    *class_byte_count = _class_defs[i].class_byte_count;
+  }
+
+  return JVMTI_ERROR_NONE;
+}
+
+/**
+  Calculate difference between non static fields of old and new class and store the info into new class:
+     instanceKlass->store_update_information
+     instanceKlass->copy_backwards
+*/
+void VM_EnhancedRedefineClasses::calculate_instance_update_information(Klass* new_version) {
+
+  class CalculateFieldUpdates : public FieldClosure {
+
+  private:
+    InstanceKlass* _old_ik;
+    GrowableArray<int> _update_info;
+    int _position;
+    bool _copy_backwards;
+
+  public:
+
+    bool does_copy_backwards() {
+      return _copy_backwards;
+    }
+
+    CalculateFieldUpdates(InstanceKlass* old_ik) :
+        _old_ik(old_ik), _position(instanceOopDesc::base_offset_in_bytes()), _copy_backwards(false) {
+      _update_info.append(_position);
+      _update_info.append(0);
+    }
+
+    GrowableArray<int> &finish() {
+      _update_info.append(0);
+      return _update_info;
+    }
+
+    void do_field(fieldDescriptor* fd) {
+      int alignment = fd->offset() - _position;
+      if (alignment > 0) {
+        // This field was aligned, so we need to make sure that we fill the gap
+        fill(alignment);
+      }
+
+      assert(_position == fd->offset(), "must be correct offset!");
+
+      fieldDescriptor old_fd;
+      if (_old_ik->find_field(fd->name(), fd->signature(), false, &old_fd) != NULL) {
+        // Found field in the old class, copy
+        copy(old_fd.offset(), type2aelembytes(fd->field_type()));
+
+        if (old_fd.offset() < fd->offset()) {
+          _copy_backwards = true;
+        }
+
+        // Transfer special flags
+        fd->set_is_field_modification_watched(old_fd.is_field_modification_watched());
+        fd->set_is_field_access_watched(old_fd.is_field_access_watched());
+      } else {
+        // New field, fill
+        fill(type2aelembytes(fd->field_type()));
+      }
+   }
+
+  private:
+    void fill(int size) {
+      if (_update_info.length() > 0 && _update_info.at(_update_info.length() - 1) < 0) {
+        (*_update_info.adr_at(_update_info.length() - 1)) -= size;
+      } else {
+        _update_info.append(-size);
+      }
+      _position += size;
+    }
+
+    void copy(int offset, int size) {
+      int prev_end = -1;
+      if (_update_info.length() > 0 && _update_info.at(_update_info.length() - 1) > 0) {
+        prev_end = _update_info.at(_update_info.length() - 2) + _update_info.at(_update_info.length() - 1);
+      }
+
+      if (prev_end == offset) {
+        (*_update_info.adr_at(_update_info.length() - 2)) += size;
+      } else {
+        _update_info.append(size);
+        _update_info.append(offset);
+      }
+
+      _position += size;
+    }
+  };
+
+  InstanceKlass* ik = InstanceKlass::cast(new_version);
+  InstanceKlass* old_ik = InstanceKlass::cast(new_version->old_version());
+
+  //
+  CalculateFieldUpdates cl(old_ik);
+  ik->do_nonstatic_fields(&cl);
+
+  GrowableArray<int> result = cl.finish();
+  ik->store_update_information(result);
+  ik->set_copying_backwards(cl.does_copy_backwards());
+/* TODO logging
+  if (RC_TRACE_ENABLED(0x00000001)) {
+    RC_TRACE(0x00000001, ("Instance update information for %s:", new_version->name()->as_C_string()));
+    if (cl.does_copy_backwards()) {
+      RC_TRACE(0x00000001, ("\tDoes copy backwards!"));
+    }
+    for (int i=0; i<result.length(); i++) {
+      int curNum = result.at(i);
+      if (curNum < 0) {
+        RC_TRACE(0x00000001, ("\t%d CLEAN", curNum));
+      } else if (curNum > 0) {
+        RC_TRACE(0x00000001, ("\t%d COPY from %d", curNum, result.at(i + 1)));
+        i++;
+      } else {
+        RC_TRACE(0x00000001, ("\tEND"));
+      }
+    }
+  }*/
+}
+
+/**
+  Rollback all changes - clear new classes from the system dictionary, return old classes to directory, free memory.
+*/
+void VM_EnhancedRedefineClasses::rollback() {
+  log_info(redefine, class, load)("Rolling back redefinition, result=%d", _res);
+  ClassLoaderDataGraph::rollback_redefinition();
+
+  for (int i = 0; i < _new_classes->length(); i++) {
+    SystemDictionary::remove_from_hierarchy(_new_classes->at(i));
+  }
+
+  for (int i = 0; i < _new_classes->length(); i++) {
+    InstanceKlass* new_class = _new_classes->at(i);
+    new_class->set_redefining(false);
+    new_class->old_version()->set_new_version(NULL);
+    new_class->set_old_version(NULL);
+  }
+  _new_classes->clear();
+}
+
+
+// Rewrite faster byte-codes back to their slower equivalent. Undoes rewriting happening in templateTable_xxx.cpp
+// The reason is that once we zero cpool caches, we need to re-resolve all entries again. Faster bytecodes do not
+// do that, they assume that cache entry is resolved already.
+void VM_EnhancedRedefineClasses::unpatch_bytecode(Method* method) {
+  RawBytecodeStream bcs(method);
+  Bytecodes::Code code;
+  Bytecodes::Code java_code;
+  while (!bcs.is_last_bytecode()) {
+    code = bcs.raw_next();
+
+    // dcevm : workaround check _illegal in case of lambda methods etc.
+    // TODO: skip lambda/intrinsic before while loop?  (method()->is_method_handle_intrinsic() || method()->is_compiled_lambda_form())
+    if (code == Bytecodes::_illegal) {
+      return;
+    }
+
+    address bcp = bcs.bcp();
+
+    if (code == Bytecodes::_breakpoint) {
+      int bci = method->bci_from(bcp);
+      code = method->orig_bytecode_at(bci);
+      java_code = Bytecodes::java_code(code);
+      if (code != java_code &&
+           (java_code == Bytecodes::_getfield ||
+            java_code == Bytecodes::_putfield ||
+            java_code == Bytecodes::_aload_0)) {
+        // Let breakpoint table handling unpatch bytecode
+        method->set_orig_bytecode_at(bci, java_code);
+      }
+    } else {
+      java_code = Bytecodes::java_code(code);
+      if (code != java_code &&
+           (java_code == Bytecodes::_getfield ||
+            java_code == Bytecodes::_putfield ||
+            java_code == Bytecodes::_aload_0)) {
+        *bcp = java_code;
+      }
+    }
+
+    // Additionally, we need to unpatch bytecode at bcp+1 for fast_xaccess (which would be fast field access)
+    if (code == Bytecodes::_fast_iaccess_0 || code == Bytecodes::_fast_aaccess_0 || code == Bytecodes::_fast_faccess_0) {
+      Bytecodes::Code code2 = Bytecodes::code_or_bp_at(bcp + 1);
+      assert(code2 == Bytecodes::_fast_igetfield ||
+             code2 == Bytecodes::_fast_agetfield ||
+             code2 == Bytecodes::_fast_fgetfield, "");
+        *(bcp + 1) = Bytecodes::java_code(code2);
+      }
+    }
+  }
+
+// Unevolving classes may point to methods of the_class directly
+// from their constant pool caches, itables, and/or vtables. We
+// use the ClassLoaderDataGraph::classes_do() facility and this helper
+// to fix up these pointers.
+// Adjust cpools and vtables closure
+void VM_EnhancedRedefineClasses::ClearCpoolCacheAndUnpatch::do_klass(Klass* k) {
+  // This is a very busy routine. We don't want too much tracing
+  // printed out.
+  bool trace_name_printed = false;
+  InstanceKlass *the_class = InstanceKlass::cast(_the_class_oop);
+
+  // If the class being redefined is java.lang.Object, we need to fix all
+  // array class vtables also
+  if (k->is_array_klass() && _the_class_oop == SystemDictionary::Object_klass()) {
+    k->vtable().adjust_method_entries(the_class, &trace_name_printed);
+  } else if (k->is_instance_klass()) {
+    HandleMark hm(_thread);
+    InstanceKlass *ik = InstanceKlass::cast(k);
+
+    // HotSpot specific optimization! HotSpot does not currently
+    // support delegation from the bootstrap class loader to a
+    // user-defined class loader. This means that if the bootstrap
+    // class loader is the initiating class loader, then it will also
+    // be the defining class loader. This also means that classes
+    // loaded by the bootstrap class loader cannot refer to classes
+    // loaded by a user-defined class loader. Note: a user-defined
+    // class loader can delegate to the bootstrap class loader.
+    //
+    // If the current class being redefined has a user-defined class
+    // loader as its defining class loader, then we can skip all
+    // classes loaded by the bootstrap class loader.
+    bool is_user_defined =
+            InstanceKlass::cast(_the_class_oop)->class_loader() != NULL;
+    if (is_user_defined && ik->class_loader() == NULL) {
+       return;
+    }
+
+    // Fix the vtable embedded in the_class and subclasses of the_class,
+    // if one exists. We discard scratch_class and we don't keep an
+    // InstanceKlass around to hold obsolete methods so we don't have
+    // any other InstanceKlass embedded vtables to update. The vtable
+    // holds the Method*s for virtual (but not final) methods.
+    // Default methods, or concrete methods in interfaces are stored
+    // in the vtable, so if an interface changes we need to check
+    // adjust_method_entries() for every InstanceKlass, which will also
+    // adjust the default method vtable indices.
+    // We also need to adjust any default method entries that are
+    // not yet in the vtable, because the vtable setup is in progress.
+    // This must be done after we adjust the default_methods and
+    // default_vtable_indices for methods already in the vtable.
+    // If redefining Unsafe, walk all the vtables looking for entries.
+// FIXME - code from standard redefine - if needed, it should switch to new_class
+//    if (ik->vtable_length() > 0 && (_the_class_oop->is_interface()
+//        || _the_class_oop == SystemDictionary::internal_Unsafe_klass()
+//        || ik->is_subtype_of(_the_class_oop))) {
+//      // ik->vtable() creates a wrapper object; rm cleans it up
+//      ResourceMark rm(_thread);
+//
+//      ik->vtable()->adjust_method_entries(the_class, &trace_name_printed);
+//      ik->adjust_default_methods(the_class, &trace_name_printed);
+//    }
+
+    // If the current class has an itable and we are either redefining an
+    // interface or if the current class is a subclass of the_class, then
+    // we potentially have to fix the itable. If we are redefining an
+    // interface, then we have to call adjust_method_entries() for
+    // every InstanceKlass that has an itable since there isn't a
+    // subclass relationship between an interface and an InstanceKlass.
+    // If redefining Unsafe, walk all the itables looking for entries.
+// FIXME - code from standard redefine - if needed, it should switch to new_class
+//    if (ik->itable_length() > 0 && (_the_class_oop->is_interface()
+//        || _the_class_oop == SystemDictionary::internal_Unsafe_klass()
+//        || ik->is_subclass_of(_the_class_oop))) {
+//      // ik->itable() creates a wrapper object; rm cleans it up
+//      ResourceMark rm(_thread);
+//
+//      ik->itable()->adjust_method_entries(the_class, &trace_name_printed);
+//    }
+   
+   constantPoolHandle other_cp = constantPoolHandle(ik->constants());
+
+  // Update host klass of anonymous classes (for example, produced by lambdas) to newest version.
+  if (ik->is_anonymous() && ik->host_klass()->new_version() != NULL) {
+    ik->set_host_klass(InstanceKlass::cast(ik->host_klass()->newest_version()));
+  }
+
+  for (int i = 0; i < other_cp->length(); i++) {
+    if (other_cp->tag_at(i).is_klass()) {
+      Klass* klass = other_cp->resolved_klass_at(i);
+      if (klass->new_version() != NULL) {
+        // Constant pool entry points to redefined class -- update to the new version
+        other_cp->klass_at_put(i, klass->newest_version());
+      }
+      klass = other_cp->resolved_klass_at(i);
+      assert(klass->new_version() == NULL, "Must be new klass!");
+    }
+  }
+
+  // DCEVM - clear whole cache (instead special methods for class/method update in standard redefinition)
+  ConstantPoolCache* cp_cache = other_cp->cache();
+  if (cp_cache != NULL) {
+    cp_cache->clear_entries();
+  }
+
+  // If bytecode rewriting is enabled, we also need to unpatch bytecode to force resolution of zeroed entries
+  if (RewriteBytecodes) {
+    ik->methods_do(unpatch_bytecode);
+  }
+  }
+}
+
+// Clean method data for this class
+void VM_EnhancedRedefineClasses::MethodDataCleaner::do_klass(Klass* k) {
+  if (k->is_instance_klass()) {
+    InstanceKlass *ik = InstanceKlass::cast(k);
+    // Clean MethodData of this class's methods so they don't refer to
+    // old methods that are no longer running.
+    Array<Method*>* methods = ik->methods();
+    int num_methods = methods->length();
+    for (int index = 0; index < num_methods; ++index) {
+      if (methods->at(index)->method_data() != NULL) {
+        methods->at(index)->method_data()->clean_weak_method_links();
+      }
+    }
+  }
+}
+
+void VM_EnhancedRedefineClasses::update_jmethod_ids() {
+  for (int j = 0; j < _matching_methods_length; ++j) {
+    Method* old_method = _matching_old_methods[j];
+    jmethodID jmid = old_method->find_jmethod_id_or_null();
+    if (old_method->new_version() != NULL && jmid == NULL) {
+       // (DCEVM) Have to create jmethodID in this case
+       jmid = old_method->jmethod_id();
+    }
+
+    if (jmid != NULL) {
+      // There is a jmethodID, change it to point to the new method
+      methodHandle new_method_h(_matching_new_methods[j]);
+
+      if (old_method->new_version() == NULL) {
+        methodHandle old_method_h(_matching_old_methods[j]);
+        jmethodID new_jmethod_id = Method::make_jmethod_id(old_method_h->method_holder()->class_loader_data(), old_method_h());
+        bool result = InstanceKlass::cast(old_method_h->method_holder())->update_jmethod_id(old_method_h(), new_jmethod_id);
+      } else {
+        jmethodID mid = new_method_h->jmethod_id();
+        bool result = InstanceKlass::cast(new_method_h->method_holder())->update_jmethod_id(new_method_h(), jmid);
+      }
+
+      Method::change_method_associated_with_jmethod_id(jmid, new_method_h());
+      assert(Method::resolve_jmethod_id(jmid) == _matching_new_methods[j], "should be replaced");
+    }
+  }
+}
+
+/**
+  Set method as obsolete / old / deleted.
+*/
+void VM_EnhancedRedefineClasses::check_methods_and_mark_as_obsolete() {
+  for (int j = 0; j < _matching_methods_length; ++j/*, ++old_index*/) {
+    Method* old_method = _matching_old_methods[j];
+    Method* new_method = _matching_new_methods[j];
+
+    if (MethodComparator::methods_EMCP(old_method, new_method)) {
+      old_method->set_new_version(new_method);
+      new_method->set_old_version(old_method);
+
+      // Transfer breakpoints
+      InstanceKlass *ik = InstanceKlass::cast(old_method->method_holder());
+      for (BreakpointInfo* bp = ik->breakpoints(); bp != NULL; bp = bp->next()) {
+        if (bp->match(old_method)) {
+          assert(bp->match(new_method), "if old method is method, then new method must match too");
+          new_method->set_breakpoint(bp->bci());
+        }
+      }
+    } else {
+      // mark obsolete methods as such
+      old_method->set_is_obsolete();
+
+      // obsolete methods need a unique idnum so they become new entries in
+      // the jmethodID cache in InstanceKlass
+      assert(old_method->method_idnum() == new_method->method_idnum(), "must match");
+//      u2 num = InstanceKlass::cast(_the_class_oop)->next_method_idnum();
+//      if (num != ConstMethod::UNSET_IDNUM) {
+//        old_method->set_method_idnum(num);
+//      }
+    }
+    old_method->set_is_old();
+  }
+  for (int i = 0; i < _deleted_methods_length; ++i) {
+    Method* old_method = _deleted_methods[i];
+
+    old_method->set_is_old();
+    old_method->set_is_obsolete();
+    // FIXME: this flag was added in dcevm10 since it is required in resolvedMethodTable.cpp
+    old_method->set_is_deleted();
+  }
+}
+
+// This internal class transfers the native function registration from old methods
+// to new methods.  It is designed to handle both the simple case of unchanged
+// native methods and the complex cases of native method prefixes being added and/or
+// removed.
+// It expects only to be used during the VM_EnhancedRedefineClasses op (a safepoint).
+//
+// This class is used after the new methods have been installed in "the_class".
+//
+// So, for example, the following must be handled.  Where 'm' is a method and
+// a number followed by an underscore is a prefix.
+//
+//                                      Old Name    New Name
+// Simple transfer to new method        m       ->  m
+// Add prefix                           m       ->  1_m
+// Remove prefix                        1_m     ->  m
+// Simultaneous add of prefixes         m       ->  3_2_1_m
+// Simultaneous removal of prefixes     3_2_1_m ->  m
+// Simultaneous add and remove          1_m     ->  2_m
+// Same, caused by prefix removal only  3_2_1_m ->  3_2_m
+//
+class TransferNativeFunctionRegistration {
+ private:
+  InstanceKlass* the_class;
+  int prefix_count;
+  char** prefixes;
+
+  // Recursively search the binary tree of possibly prefixed method names.
+  // Iteration could be used if all agents were well behaved. Full tree walk is
+  // more resilent to agents not cleaning up intermediate methods.
+  // Branch at each depth in the binary tree is:
+  //    (1) without the prefix.
+  //    (2) with the prefix.
+  // where 'prefix' is the prefix at that 'depth' (first prefix, second prefix,...)
+  Method* search_prefix_name_space(int depth, char* name_str, size_t name_len,
+                                     Symbol* signature) {
+    TempNewSymbol name_symbol = SymbolTable::probe(name_str, (int)name_len);
+    if (name_symbol != NULL) {
+      Method* method = the_class->lookup_method(name_symbol, signature);
+      if (method != NULL) {
+        // Even if prefixed, intermediate methods must exist.
+        if (method->is_native()) {
+          // Wahoo, we found a (possibly prefixed) version of the method, return it.
+          return method;
+        }
+        if (depth < prefix_count) {
+          // Try applying further prefixes (other than this one).
+          method = search_prefix_name_space(depth+1, name_str, name_len, signature);
+          if (method != NULL) {
+            return method; // found
+          }
+
+          // Try adding this prefix to the method name and see if it matches
+          // another method name.
+          char* prefix = prefixes[depth];
+          size_t prefix_len = strlen(prefix);
+          size_t trial_len = name_len + prefix_len;
+          char* trial_name_str = NEW_RESOURCE_ARRAY(char, trial_len + 1);
+          strcpy(trial_name_str, prefix);
+          strcat(trial_name_str, name_str);
+          method = search_prefix_name_space(depth+1, trial_name_str, trial_len,
+                                            signature);
+          if (method != NULL) {
+            // If found along this branch, it was prefixed, mark as such
+            method->set_is_prefixed_native();
+            return method; // found
+          }
+        }
+      }
+    }
+    return NULL;  // This whole branch bore nothing
+  }
+
+  // Return the method name with old prefixes stripped away.
+  char* method_name_without_prefixes(Method* method) {
+    Symbol* name = method->name();
+    char* name_str = name->as_utf8();
+
+    // Old prefixing may be defunct, strip prefixes, if any.
+    for (int i = prefix_count-1; i >= 0; i--) {
+      char* prefix = prefixes[i];
+      size_t prefix_len = strlen(prefix);
+      if (strncmp(prefix, name_str, prefix_len) == 0) {
+        name_str += prefix_len;
+      }
+    }
+    return name_str;
+  }
+
+  // Strip any prefixes off the old native method, then try to find a
+  // (possibly prefixed) new native that matches it.
+  Method* strip_and_search_for_new_native(Method* method) {
+    ResourceMark rm;
+    char* name_str = method_name_without_prefixes(method);
+    return search_prefix_name_space(0, name_str, strlen(name_str),
+                                    method->signature());
+  }
+
+ public:
+
+  // Construct a native method transfer processor for this class.
+  TransferNativeFunctionRegistration(InstanceKlass* _the_class) {
+    assert(SafepointSynchronize::is_at_safepoint(), "sanity check");
+
+    the_class = _the_class;
+    prefixes = JvmtiExport::get_all_native_method_prefixes(&prefix_count);
+  }
+
+  // Attempt to transfer any of the old or deleted methods that are native
+  void transfer_registrations(Method** old_methods, int methods_length) {
+    for (int j = 0; j < methods_length; j++) {
+      Method* old_method = old_methods[j];
+
+      if (old_method->is_native() && old_method->has_native_function()) {
+        Method* new_method = strip_and_search_for_new_native(old_method);
+        if (new_method != NULL) {
+          // Actually set the native function in the new method.
+          // Redefine does not send events (except CFLH), certainly not this
+          // behind the scenes re-registration.
+          new_method->set_native_function(old_method->native_function(),
+                              !Method::native_bind_event_is_interesting);
+        }
+      }
+    }
+  }
+};
+
+// Don't lose the association between a native method and its JNI function.
+void VM_EnhancedRedefineClasses::transfer_old_native_function_registrations(InstanceKlass* the_class) {
+  TransferNativeFunctionRegistration transfer(the_class);
+  transfer.transfer_registrations(_deleted_methods, _deleted_methods_length);
+  transfer.transfer_registrations(_matching_old_methods, _matching_methods_length);
+}
+
+// DCEVM - it always deoptimases everything! (because it is very difficult to find only correct dependencies)
+// Deoptimize all compiled code that depends on this class.
+//
+// If the can_redefine_classes capability is obtained in the onload
+// phase then the compiler has recorded all dependencies from startup.
+// In that case we need only deoptimize and throw away all compiled code
+// that depends on the class.
+//
+// If can_redefine_classes is obtained sometime after the onload
+// phase then the dependency information may be incomplete. In that case
+// the first call to RedefineClasses causes all compiled code to be
+// thrown away. As can_redefine_classes has been obtained then
+// all future compilations will record dependencies so second and
+// subsequent calls to RedefineClasses need only throw away code
+// that depends on the class.
+//
+void VM_EnhancedRedefineClasses::flush_dependent_code(InstanceKlass* k_h, TRAPS) {
+  assert_locked_or_safepoint(Compile_lock);
+
+  // All dependencies have been recorded from startup or this is a second or
+  // subsequent use of RedefineClasses
+  // FIXME: for now, deoptimize all!
+  if (0 && JvmtiExport::all_dependencies_are_recorded()) {
+    CodeCache::flush_evol_dependents_on(k_h);
+  } else {
+    CodeCache::mark_all_nmethods_for_deoptimization();
+
+    ResourceMark rm(THREAD);
+    DeoptimizationMarker dm;
+
+    // Deoptimize all activations depending on marked nmethods
+    Deoptimization::deoptimize_dependents();
+
+    // Make the dependent methods not entrant
+    CodeCache::make_marked_nmethods_not_entrant();
+
+    // From now on we know that the dependency information is complete
+    JvmtiExport::set_all_dependencies_are_recorded(true);
+  }
+}
+
+/**
+  Compare _old_methods and _new_methods arrays and store the result into
+	_matching_old_methods, _matching_new_methods, _added_methods, _deleted_methods
+  
+  Setup _old_methods and _new_methods before the call - it should be called for one class only!
+*/
+void VM_EnhancedRedefineClasses::compute_added_deleted_matching_methods() {
+  Method* old_method;
+  Method* new_method;
+
+  _matching_old_methods = NEW_RESOURCE_ARRAY(Method*, _old_methods->length());
+  _matching_new_methods = NEW_RESOURCE_ARRAY(Method*, _old_methods->length());
+  _added_methods        = NEW_RESOURCE_ARRAY(Method*, _new_methods->length());
+  _deleted_methods      = NEW_RESOURCE_ARRAY(Method*, _old_methods->length());
+
+  _matching_methods_length = 0;
+  _deleted_methods_length  = 0;
+  _added_methods_length    = 0;
+
+  int nj = 0;
+  int oj = 0;
+  while (true) {
+    if (oj >= _old_methods->length()) {
+      if (nj >= _new_methods->length()) {
+        break; // we've looked at everything, done
+      }
+      // New method at the end
+      new_method = _new_methods->at(nj);
+      _added_methods[_added_methods_length++] = new_method;
+      ++nj;
+    } else if (nj >= _new_methods->length()) {
+      // Old method, at the end, is deleted
+      old_method = _old_methods->at(oj);
+      _deleted_methods[_deleted_methods_length++] = old_method;
+      ++oj;
+    } else {
+      old_method = _old_methods->at(oj);
+      new_method = _new_methods->at(nj);
+      if (old_method->name() == new_method->name()) {
+        if (old_method->signature() == new_method->signature()) {
+          _matching_old_methods[_matching_methods_length  ] = old_method;
+          _matching_new_methods[_matching_methods_length++] = new_method;
+          ++nj;
+          ++oj;
+        } else {
+          // added overloaded have already been moved to the end,
+          // so this is a deleted overloaded method
+          _deleted_methods[_deleted_methods_length++] = old_method;
+          ++oj;
+        }
+      } else { // names don't match
+        if (old_method->name()->fast_compare(new_method->name()) > 0) {
+          // new method
+          _added_methods[_added_methods_length++] = new_method;
+          ++nj;
+        } else {
+          // deleted method
+          _deleted_methods[_deleted_methods_length++] = old_method;
+          ++oj;
+        }
+      }
+    }
+  }
+  assert(_matching_methods_length + _deleted_methods_length == _old_methods->length(), "sanity");
+  assert(_matching_methods_length + _added_methods_length == _new_methods->length(), "sanity");
+}
+
+/**
+  FIXME - swap_annotations is never called, check that annotations work
+*/
+void VM_EnhancedRedefineClasses::swap_annotations(InstanceKlass* the_class,
+                                          InstanceKlass* new_class) {
+  // FIXME - probably original implementation only 
+  // Swap annotation fields values
+  Annotations* old_annotations = the_class->annotations();
+  the_class->set_annotations(new_class->annotations());
+  new_class->set_annotations(old_annotations);
+}
+
+
+// Install the redefinition of a class:
+//    - house keeping (flushing breakpoints and caches, deoptimizing
+//      dependent compiled code)
+//    - replacing parts in the_class with parts from new_class
+//    - adding a weak reference to track the obsolete but interesting
+//      parts of the_class
+//    - adjusting constant pool caches and vtables in other classes
+//      that refer to methods in the_class. These adjustments use the
+//      ClassLoaderDataGraph::classes_do() facility which only allows
+//      a helper method to be specified. The interesting parameters
+//      that we would like to pass to the helper method are saved in
+//      static global fields in the VM operation.
+void VM_EnhancedRedefineClasses::redefine_single_class(InstanceKlass* new_class_oop, TRAPS) {
+
+  HandleMark hm(THREAD);   // make sure handles from this call are freed
+
+  if (log_is_enabled(Info, redefine, class, timer)) {
+    _timer_rsc_phase1.start();
+  }
+
+  InstanceKlass* new_class = new_class_oop;
+  InstanceKlass* the_class = InstanceKlass::cast(new_class_oop->old_version());
+  assert(the_class != NULL, "must have old version");
+
+  // Remove all breakpoints in methods of this class
+  JvmtiBreakpoints& jvmti_breakpoints = JvmtiCurrentBreakpoints::get_jvmti_breakpoints();
+  jvmti_breakpoints.clearall_in_class_at_safepoint(the_class);
+
+  // DCEVM Deoptimization is always for whole java world, call only once after all classes are redefined
+  // Deoptimize all compiled code that depends on this class
+  //  flush_dependent_code(the_class, THREAD);
+
+  _old_methods = the_class->methods();
+  _new_methods = new_class->methods();
+  _the_class_oop = the_class;
+  compute_added_deleted_matching_methods();
+
+  // track number of methods that are EMCP for add_previous_version() call below
+  check_methods_and_mark_as_obsolete();
+  update_jmethod_ids();
+
+  _any_class_has_resolved_methods = the_class->has_resolved_methods() || _any_class_has_resolved_methods;
+
+  transfer_old_native_function_registrations(the_class);
+
+
+  // JSR-292 support
+  /* FIXME: j10 dropped support for it?
+  MemberNameTable* mnt = the_class->member_names();
+  assert(new_class->member_names() == NULL, "");
+  if (mnt != NULL) {
+    new_class->set_member_names(mnt);
+    the_class->set_member_names(NULL);
+
+    // FIXME: adjust_method_entries is used in standard hotswap JDK9
+    // bool trace_name_printed = false;
+    // mnt->adjust_method_entries(new_class(), &trace_name_printed);
+  }
+  */
+
+  // Adjust constantpool caches for all classes that reference methods of the evolved class.
+  ClearCpoolCacheAndUnpatch clear_cpool_cache(THREAD);
+  ClassLoaderDataGraph::classes_do(&clear_cpool_cache);
+
+  {
+    ResourceMark rm(THREAD);
+    // increment the classRedefinedCount field in the_class and in any
+    // direct and indirect subclasses of the_class
+    increment_class_counter(the_class, THREAD);
+    log_info(redefine, class, load)
+      ("redefined name=%s, count=%d (avail_mem=" UINT64_FORMAT "K)",
+       the_class->external_name(), java_lang_Class::classRedefinedCount(the_class->java_mirror()), os::available_memory() >> 10);
+    Events::log_redefinition(THREAD, "redefined class name=%s, count=%d",
+                             the_class->external_name(),
+                             java_lang_Class::classRedefinedCount(the_class->java_mirror()));
+
+  }
+  _timer_rsc_phase2.stop();
+} // end redefine_single_class()
+
+
+// Increment the classRedefinedCount field in the specific InstanceKlass
+// and in all direct and indirect subclasses.
+void VM_EnhancedRedefineClasses::increment_class_counter(InstanceKlass *ik, TRAPS) {
+  oop class_mirror = ik->java_mirror();
+  Klass* class_oop = java_lang_Class::as_Klass(class_mirror);
+  int new_count = java_lang_Class::classRedefinedCount(class_mirror) + 1;
+  java_lang_Class::set_classRedefinedCount(class_mirror, new_count);
+
+  if (class_oop != _the_class_oop) {
+    // _the_class_oop count is printed at end of redefine_single_class()
+    log_debug(redefine, class, subclass)("updated count in subclass=%s to %d", ik->external_name(), new_count);
+  }
+
+  for (Klass *subk = ik->subklass(); subk != NULL;
+       subk = subk->next_sibling()) {
+    if (subk->is_instance_klass()) {
+      // Only update instanceKlasses
+      InstanceKlass *subik = InstanceKlass::cast(subk);
+      // recursively do subclasses of the current subclass
+      increment_class_counter(subik, THREAD);
+    }
+  }
+}
+
+// FIXME - class check is currently disabled
+void VM_EnhancedRedefineClasses::CheckClass::do_klass(Klass* k) {
+  return;
+  bool no_old_methods = true;  // be optimistic
+
+  // Both array and instance classes have vtables.
+  // a vtable should never contain old or obsolete methods
+  ResourceMark rm(_thread);
+  if (k->vtable_length() > 0 &&
+      !k->vtable().check_no_old_or_obsolete_entries()) {
+    if (log_is_enabled(Trace, redefine, class, obsolete, metadata)) {
+      log_trace(redefine, class, obsolete, metadata)
+        ("klassVtable::check_no_old_or_obsolete_entries failure -- OLD or OBSOLETE method found -- class: %s",
+         k->signature_name());
+      k->vtable().dump_vtable();
+    }
+    no_old_methods = false;
+  }
+
+  if (k->is_instance_klass()) {
+    HandleMark hm(_thread);
+    InstanceKlass *ik = InstanceKlass::cast(k);
+
+    // an itable should never contain old or obsolete methods
+    if (ik->itable_length() > 0 &&
+        !ik->itable().check_no_old_or_obsolete_entries()) {
+      if (log_is_enabled(Trace, redefine, class, obsolete, metadata)) {
+        log_trace(redefine, class, obsolete, metadata)
+          ("klassItable::check_no_old_or_obsolete_entries failure -- OLD or OBSOLETE method found -- class: %s",
+           ik->signature_name());
+        ik->itable().dump_itable();
+      }
+      no_old_methods = false;
+    }
+
+    // the constant pool cache should never contain non-deleted old or obsolete methods
+    if (ik->constants() != NULL &&
+        ik->constants()->cache() != NULL &&
+        !ik->constants()->cache()->check_no_old_or_obsolete_entries()) {
+      if (log_is_enabled(Trace, redefine, class, obsolete, metadata)) {
+        log_trace(redefine, class, obsolete, metadata)
+          ("cp-cache::check_no_old_or_obsolete_entries failure -- OLD or OBSOLETE method found -- class: %s",
+           ik->signature_name());
+        ik->constants()->cache()->dump_cache();
+      }
+      no_old_methods = false;
+    }
+  }
+
+  // print and fail guarantee if old methods are found.
+  if (!no_old_methods) {
+    if (log_is_enabled(Trace, redefine, class, obsolete, metadata)) {
+      dump_methods();
+    } else {
+      log_trace(redefine, class)("Use the '-Xlog:redefine+class*:' option "
+        "to see more info about the following guarantee() failure.");
+    }
+    guarantee(false, "OLD and/or OBSOLETE method(s) found");
+  }
+}
+
+/**
+ * Logging of all methods (old, new, changed, ...)
+ */
+void VM_EnhancedRedefineClasses::dump_methods() {
+  int j;
+  log_trace(redefine, class, dump)("_old_methods --");
+  for (j = 0; j < _old_methods->length(); ++j) {
+    LogStreamHandle(Trace, redefine, class, dump) log_stream;
+    Method* m = _old_methods->at(j);
+    log_stream.print("%4d  (%5d)  ", j, m->vtable_index());
+    m->access_flags().print_on(&log_stream);
+    log_stream.print(" --  ");
+    m->print_name(&log_stream);
+    log_stream.cr();
+  }
+  log_trace(redefine, class, dump)("_new_methods --");
+  for (j = 0; j < _new_methods->length(); ++j) {
+    LogStreamHandle(Trace, redefine, class, dump) log_stream;
+    Method* m = _new_methods->at(j);
+    log_stream.print("%4d  (%5d)  ", j, m->vtable_index());
+    m->access_flags().print_on(&log_stream);
+    log_stream.print(" --  ");
+    m->print_name(&log_stream);
+    log_stream.cr();
+  }
+  log_trace(redefine, class, dump)("_matching_methods --");
+  for (j = 0; j < _matching_methods_length; ++j) {
+    LogStreamHandle(Trace, redefine, class, dump) log_stream;
+    Method* m = _matching_old_methods[j];
+    log_stream.print("%4d  (%5d)  ", j, m->vtable_index());
+    m->access_flags().print_on(&log_stream);
+    log_stream.print(" --  ");
+    m->print_name();
+    log_stream.cr();
+
+    m = _matching_new_methods[j];
+    log_stream.print("      (%5d)  ", m->vtable_index());
+    m->access_flags().print_on(&log_stream);
+    log_stream.cr();
+  }
+  log_trace(redefine, class, dump)("_deleted_methods --");
+  for (j = 0; j < _deleted_methods_length; ++j) {
+    LogStreamHandle(Trace, redefine, class, dump) log_stream;
+    Method* m = _deleted_methods[j];
+    log_stream.print("%4d  (%5d)  ", j, m->vtable_index());
+    m->access_flags().print_on(&log_stream);
+    log_stream.print(" --  ");
+    m->print_name(&log_stream);
+    log_stream.cr();
+  }
+  log_trace(redefine, class, dump)("_added_methods --");
+  for (j = 0; j < _added_methods_length; ++j) {
+    LogStreamHandle(Trace, redefine, class, dump) log_stream;
+    Method* m = _added_methods[j];
+    log_stream.print("%4d  (%5d)  ", j, m->vtable_index());
+    m->access_flags().print_on(&log_stream);
+    log_stream.print(" --  ");
+    m->print_name(&log_stream);
+    log_stream.cr();
+  }
+}
+
+// TODO - is it called anywhere?
+void VM_EnhancedRedefineClasses::print_on_error(outputStream* st) const {
+  VM_Operation::print_on_error(st);
+  if (_the_class_oop != NULL) {
+    ResourceMark rm;
+    st->print_cr(", redefining class %s", _the_class_oop->external_name());
+  }
+}
+
+/**
+ Helper class to traverse all loaded classes and figure out if the class is affected by redefinition.
+*/
+class AffectedKlassClosure : public KlassClosure {
+ private:
+   GrowableArray<Klass*>* _affected_klasses;
+ public:
+  AffectedKlassClosure(GrowableArray<Klass*>* affected_klasses) : _affected_klasses(affected_klasses) {}
+
+  void do_klass(Klass* klass) {
+    assert(!_affected_klasses->contains(klass), "must not occur more than once!");
+
+    if (klass->new_version() != NULL) {
+      return;
+    }
+    assert(klass->new_version() == NULL, "only last version is valid");
+
+    if (klass->check_redefinition_flag(Klass::MarkedAsAffected)) {
+      _affected_klasses->append(klass);
+      return;
+    }
+
+    int super_depth = klass->super_depth();
+    int idx;
+    for (idx = 0; idx < super_depth; idx++) {
+      Klass* primary = klass->primary_super_of_depth(idx);
+      if (primary == NULL) {
+        break;
+      }
+      if (primary->check_redefinition_flag(Klass::MarkedAsAffected)) {
+        log_trace(redefine, class, load)("found affected class: %s", klass->name()->as_C_string());
+        klass->set_redefinition_flag(Klass::MarkedAsAffected);
+        _affected_klasses->append(klass);
+         return;
+      }
+    }
+
+    int secondary_length = klass->secondary_supers()->length();
+    for (idx = 0; idx < secondary_length; idx++) {
+      Klass* secondary = klass->secondary_supers()->at(idx);
+      if (secondary->check_redefinition_flag(Klass::MarkedAsAffected)) {
+        log_trace(redefine, class, load)("found affected class: %s", klass->name()->as_C_string());
+        klass->set_redefinition_flag(Klass::MarkedAsAffected);
+        _affected_klasses->append(klass);
+        return;
+      }
+    }
+  }
+};
+
+/**
+  Find all affected classes by current redefinition (either because of redefine, class hierarchy or interface change).
+  Affected classes are stored in _affected_klasses and parent classes always precedes child class.  
+*/
+jvmtiError VM_EnhancedRedefineClasses::find_sorted_affected_classes(TRAPS) {
+  for (int i = 0; i < _class_count; i++) {
+    InstanceKlass* klass_handle = get_ik(_class_defs[i].klass);
+    klass_handle->set_redefinition_flag(Klass::MarkedAsAffected);
+    assert(klass_handle->new_version() == NULL, "must be new class");
+
+    log_trace(redefine, class, load)("marking class as being redefined: %s", klass_handle->name()->as_C_string());
+  }
+
+  // Find classes not directly redefined, but affected by a redefinition (because one of its supertypes is redefined)
+  AffectedKlassClosure closure(_affected_klasses);
+  // TODO: j10 - review chancge from SystemDictionary::classes_do(&closure);
+  ClassLoaderDataGraph::dictionary_classes_do(&closure);
+  log_trace(redefine, class, load)("%d classes affected", _affected_klasses->length());
+
+  // Sort the affected klasses such that a supertype is always on a smaller array index than its subtype.
+  jvmtiError result = do_topological_class_sorting(THREAD);
+
+  if (log_is_enabled(Trace, redefine, class, load)) {
+    log_trace(redefine, class, load)("redefine order:");
+    for (int i = 0; i < _affected_klasses->length(); i++) {
+      log_trace(redefine, class, load)("%s", _affected_klasses->at(i)->name()->as_C_string());
+    }
+  }
+  return JVMTI_ERROR_NONE;
+}
+
+/**
+  Pairs of class dependencies (for topological sort)
+*/
+struct KlassPair {
+  const Klass* _left;
+  const Klass* _right;
+
+  KlassPair() { }
+  KlassPair(const Klass* left, const Klass* right) : _left(left), _right(right) { }
+};
+
+static bool match_second(void* value, KlassPair elem) {
+  return elem._right == value;
+}
+
+/**
+ For each class to be redefined parse the bytecode and figure out the superclass and all interfaces.
+ First newly introduced classes (_class_defs) are scanned and then affected classed (_affected_klasses).
+ Affected flag is cleared (clear_redefinition_flag(Klass::MarkedAsAffected))
+
+ For each dependency create a KlassPair instance. Finnaly, affected classes (_affected_klasses) are sorted according to pairs.
+ 
+ TODO - the class file is potentionally parsed multiple times - introduce a cache?
+*/
+jvmtiError VM_EnhancedRedefineClasses::do_topological_class_sorting(TRAPS) {
+  ResourceMark mark(THREAD);
+
+  // Collect dependencies
+  GrowableArray<KlassPair> links;
+  for (int i = 0; i < _class_count; i++) {
+    InstanceKlass* klass = get_ik(_class_defs[i].klass);
+
+    ClassFileStream st((u1*)_class_defs[i].class_bytes,
+                           _class_defs[i].class_byte_count,
+                           "__VM_EnhancedRedefineClasses__",
+                           ClassFileStream::verify);
+
+    Handle protection_domain(THREAD, klass->protection_domain());
+
+    ClassFileParser parser(&st,
+                           klass->name(),
+                           klass->class_loader_data(),
+                           protection_domain,
+                           NULL, // host_klass
+                           NULL, // cp_patches
+                           ClassFileParser::INTERNAL, // publicity level
+                           true,
+                           THREAD);
+
+    const Klass* super_klass = parser.super_klass();
+    if (super_klass != NULL && _affected_klasses->contains((Klass*) super_klass)) {
+      links.append(KlassPair(super_klass, klass));
+    }
+
+    Array<Klass*>* local_interfaces = parser.local_interfaces();
+    for (int j = 0; j < local_interfaces->length(); j++) {
+      Klass* iface = local_interfaces->at(j);
+      if (iface != NULL && _affected_klasses->contains(iface)) {
+        links.append(KlassPair(iface, klass));
+      }
+    }
+
+    assert(klass->check_redefinition_flag(Klass::MarkedAsAffected), "");
+    klass->clear_redefinition_flag(Klass::MarkedAsAffected);
+  }
+
+  // Append dependencies based on current class definition
+  for (int i = 0; i < _affected_klasses->length(); i++) {
+    InstanceKlass* klass = InstanceKlass::cast(_affected_klasses->at(i));
+
+    if (klass->check_redefinition_flag(Klass::MarkedAsAffected)) {
+      klass->clear_redefinition_flag(Klass::MarkedAsAffected);
+      Klass* super_klass = klass->super();
+      if (_affected_klasses->contains(super_klass)) {
+        links.append(KlassPair(super_klass, klass));
+      }
+
+      Array<Klass*>* local_interfaces = klass->local_interfaces();
+      for (int j = 0; j < local_interfaces->length(); j++) {
+        Klass* interfaceKlass = local_interfaces->at(j);
+        if (_affected_klasses->contains(interfaceKlass)) {
+          links.append(KlassPair(interfaceKlass, klass));
+        }
+      }
+    }
+  }
+
+  for (int i = 0; i < _affected_klasses->length(); i++) {
+    int j;
+    for (j = i; j < _affected_klasses->length(); j++) {
+      // Search for node with no incoming edges
+      Klass* klass = _affected_klasses->at(j);
+      int k = links.find(klass, match_second);
+      if (k == -1) break;
+    }
+    if (j == _affected_klasses->length()) {
+      return JVMTI_ERROR_CIRCULAR_CLASS_DEFINITION;
+    }
+
+    // Remove all links from this node
+    const Klass* klass = _affected_klasses->at(j);
+    int k = 0;
+    while (k < links.length()) {
+      if (links.at(k)._left == klass) {
+        links.delete_at(k);
+      } else {
+        k++;
+      }
+    }
+
+    // Swap node
+    Klass* tmp = _affected_klasses->at(j);
+    _affected_klasses->at_put(j, _affected_klasses->at(i));
+    _affected_klasses->at_put(i, tmp);
+  }
+
+  return JVMTI_ERROR_NONE;
+}
diff --git a/src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.hpp b/src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.hpp
new file mode 100644
index 00000000000..b712d69a193
--- /dev/null
+++ b/src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.hpp
@@ -0,0 +1,202 @@
+/*
+ * Copyright (c) 2003, 2016, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef SHARE_VM_PRIMS_JVMTIREDEFINECLASSES2_HPP
+#define SHARE_VM_PRIMS_JVMTIREDEFINECLASSES2_HPP
+
+#include "jvmtifiles/jvmtiEnv.hpp"
+#include "memory/oopFactory.hpp"
+#include "memory/resourceArea.hpp"
+#include "oops/objArrayKlass.hpp"
+#include "oops/objArrayOop.hpp"
+#include "runtime/vm_operations.hpp"
+#include "gc/shared/vmGCOperations.hpp"
+#include "../../../java.base/unix/native/include/jni_md.h"
+
+/**
+ * Enhanced class redefiner.
+ *
+ * This class implements VM_GC_Operation - the usual usage should be:
+ *     VM_EnhancedRedefineClasses op(class_count, class_definitions, jvmti_class_load_kind_redefine);
+ *     VMThread::execute(&op);
+ * Which in turn runs:
+ *   - doit_prologue() - calculate all affected classes (add subclasses etc) and load new class versions
+ *   - doit() - main redefition, adjust existing objects on the heap, clear caches
+ *   - doit_epilogue() - cleanup
+*/
+class VM_EnhancedRedefineClasses: public VM_GC_Operation {
+ private:
+  // These static fields are needed by ClassLoaderDataGraph::classes_do()
+  // facility and the AdjustCpoolCacheAndVtable helper:
+  static Array<Method*>* _old_methods;
+  static Array<Method*>* _new_methods;
+  static Method**      _matching_old_methods;
+  static Method**      _matching_new_methods;
+  static Method**      _deleted_methods;
+  static Method**      _added_methods;
+  static int             _matching_methods_length;
+  static int             _deleted_methods_length;
+  static int             _added_methods_length;
+  static Klass*          _the_class_oop;
+
+  // The instance fields are used to pass information from
+  // doit_prologue() to doit() and doit_epilogue().
+  jint                        _class_count;
+  const jvmtiClassDefinition *_class_defs;  // ptr to _class_count defs
+
+  // This operation is used by both RedefineClasses and
+  // RetransformClasses.  Indicate which.
+  JvmtiClassLoadKind          _class_load_kind;
+
+  // _index_map_count is just an optimization for knowing if
+  // _index_map_p contains any entries.
+  int                         _index_map_count;
+  intArray *                  _index_map_p;
+
+  // _operands_index_map_count is just an optimization for knowing if
+  // _operands_index_map_p contains any entries.
+  int                         _operands_cur_length;
+  int                         _operands_index_map_count;
+  intArray *                  _operands_index_map_p;
+
+  GrowableArray<InstanceKlass*>*      _new_classes;
+  jvmtiError                  _res;
+
+  // Set if any of the InstanceKlasses have entries in the ResolvedMethodTable
+  // to avoid walking after redefinition if the redefined classes do not
+  // have any entries.
+  bool _any_class_has_resolved_methods;
+
+  // Enhanced class redefinition, affected klasses contain all classes which should be redefined
+  // either because of redefine, class hierarchy or interface change
+  GrowableArray<Klass*>*      _affected_klasses;
+
+  int                         _max_redefinition_flags;
+
+  // Performance measurement support. These timers do not cover all
+  // the work done for JVM/TI RedefineClasses() but they do cover
+  // the heavy lifting.
+  elapsedTimer  _timer_rsc_phase1;
+  elapsedTimer  _timer_rsc_phase2;
+  elapsedTimer  _timer_vm_op_prologue;
+
+  // These routines are roughly in call order unless otherwise noted.
+
+  /**
+    Load and link new classes (either redefined or affected by redefinition - subclass, ...)
+
+    - find sorted affected classes
+    - resolve new class
+    - calculate redefine flags (field change, method change, supertype change, ...)
+    - calculate modified fields and mapping to old fields
+    - link new classes
+
+    The result is sotred in _affected_klasses(old definitions) and _new_classes(new definitions) arrays.
+  */
+  jvmtiError load_new_class_versions(TRAPS);
+
+  // Searches for all affected classes and performs a sorting such tha
+  // a supertype is always before a subtype.
+  jvmtiError find_sorted_affected_classes(TRAPS);
+
+  jvmtiError do_topological_class_sorting(TRAPS);
+
+  jvmtiError find_class_bytes(InstanceKlass* the_class, const unsigned char **class_bytes, jint *class_byte_count, jboolean *not_changed);
+  int calculate_redefinition_flags(InstanceKlass* new_class);
+  void calculate_instance_update_information(Klass* new_version);
+
+  void rollback();
+  static void mark_as_scavengable(nmethod* nm);
+  static void unpatch_bytecode(Method* method);
+
+  // Figure out which new methods match old methods in name and signature,
+  // which methods have been added, and which are no longer present
+  void compute_added_deleted_matching_methods();
+
+  // Change jmethodIDs to point to the new methods
+  void update_jmethod_ids();
+
+  // marking methods as old and/or obsolete
+  void check_methods_and_mark_as_obsolete();
+  void transfer_old_native_function_registrations(InstanceKlass* the_class);
+
+  // Install the redefinition of a class
+  void redefine_single_class(InstanceKlass* new_class_oop, TRAPS);
+
+  void swap_annotations(InstanceKlass* new_class,
+                        InstanceKlass* scratch_class);
+
+  // Increment the classRedefinedCount field in the specific InstanceKlass
+  // and in all direct and indirect subclasses.
+  void increment_class_counter(InstanceKlass *ik, TRAPS);
+
+  void flush_dependent_code(InstanceKlass* k_h, TRAPS);
+
+  static void dump_methods();
+
+  // Check that there are no old or obsolete methods
+  class CheckClass : public KlassClosure {
+    Thread* _thread;
+   public:
+    CheckClass(Thread* t) : _thread(t) {}
+    void do_klass(Klass* k);
+  };
+
+  // Unevolving classes may point to methods of the_class directly
+  // from their constant pool caches, itables, and/or vtables. We
+  // use the ClassLoaderDataGraph::classes_do() facility and this helper
+  // to fix up these pointers.
+  class ClearCpoolCacheAndUnpatch : public KlassClosure {
+    Thread* _thread;
+   public:
+    ClearCpoolCacheAndUnpatch(Thread* t) : _thread(t) {}
+    void do_klass(Klass* k);
+  };
+
+  // Clean MethodData out
+  class MethodDataCleaner : public KlassClosure {
+   public:
+    MethodDataCleaner() {}
+    void do_klass(Klass* k);
+  };
+ public:
+  VM_EnhancedRedefineClasses(jint class_count,
+                     const jvmtiClassDefinition *class_defs,
+                     JvmtiClassLoadKind class_load_kind);
+  VMOp_Type type() const { return VMOp_RedefineClasses; }
+  bool doit_prologue();
+  void doit();
+  void doit_epilogue();
+
+  bool allow_nested_vm_operations() const        { return true; }
+  jvmtiError check_error()                       { return _res; }
+
+  // Modifiable test must be shared between IsModifiableClass query
+  // and redefine implementation
+  static bool is_modifiable_class(oop klass_mirror);
+
+  // Error printing
+  void print_on_error(outputStream* st) const;
+};
+#endif // SHARE_VM_PRIMS_JVMTIREDEFINECLASSES2_HPP
diff --git a/src/hotspot/share/prims/jvmtiEnv.cpp b/src/hotspot/share/prims/jvmtiEnv.cpp
index 342a0467112..b6838ac034d 100644
--- a/src/hotspot/share/prims/jvmtiEnv.cpp
+++ b/src/hotspot/share/prims/jvmtiEnv.cpp
@@ -51,6 +51,7 @@
 #include "prims/jvmtiManageCapabilities.hpp"
 #include "prims/jvmtiRawMonitor.hpp"
 #include "prims/jvmtiRedefineClasses.hpp"
+#include "prims/jvmtiEnhancedRedefineClasses.hpp"
 #include "prims/jvmtiTagMap.hpp"
 #include "prims/jvmtiThreadState.inline.hpp"
 #include "prims/jvmtiUtil.hpp"
@@ -383,8 +384,13 @@ JvmtiEnv::GetClassLoaderClasses(jobject initiating_loader, jint* class_count_ptr
 // is_modifiable_class_ptr - pre-checked for NULL
 jvmtiError
 JvmtiEnv::IsModifiableClass(oop k_mirror, jboolean* is_modifiable_class_ptr) {
-  *is_modifiable_class_ptr = VM_RedefineClasses::is_modifiable_class(k_mirror)?
-                                                       JNI_TRUE : JNI_FALSE;
+  if (AllowEnhancedClassRedefinition) {
+    *is_modifiable_class_ptr = VM_EnhancedRedefineClasses::is_modifiable_class(k_mirror)?
+                                                         JNI_TRUE : JNI_FALSE;
+  } else {
+    *is_modifiable_class_ptr = VM_RedefineClasses::is_modifiable_class(k_mirror)?
+                                                         JNI_TRUE : JNI_FALSE;
+  }
   return JVMTI_ERROR_NONE;
 } /* end IsModifiableClass */
 
@@ -414,7 +420,8 @@ JvmtiEnv::RetransformClasses(jint class_count, const jclass* classes) {
       return JVMTI_ERROR_INVALID_CLASS;
     }
 
-    if (!VM_RedefineClasses::is_modifiable_class(k_mirror)) {
+    if ((!AllowEnhancedClassRedefinition && !VM_RedefineClasses::is_modifiable_class(k_mirror)) ||
+        (AllowEnhancedClassRedefinition && !VM_EnhancedRedefineClasses::is_modifiable_class(k_mirror))) {
       return JVMTI_ERROR_UNMODIFIABLE_CLASS;
     }
 
@@ -446,10 +453,20 @@ JvmtiEnv::RetransformClasses(jint class_count, const jclass* classes) {
     }
     class_definitions[index].klass              = jcls;
   }
+
   EventRetransformClasses event;
-  VM_RedefineClasses op(class_count, class_definitions, jvmti_class_load_kind_retransform);
-  VMThread::execute(&op);
-  jvmtiError error = op.check_error();
+  jvmtiError error;
+
+  if (AllowEnhancedClassRedefinition) {
+    MutexLocker sd_mutex(EnhancedRedefineClasses_lock);
+    VM_EnhancedRedefineClasses op(class_count, class_definitions, jvmti_class_load_kind_retransform);
+    VMThread::execute(&op);
+    error = (op.check_error());
+  } else {
+    VM_RedefineClasses op(class_count, class_definitions, jvmti_class_load_kind_retransform);
+    VMThread::execute(&op);
+    error = op.check_error();
+  }
   if (error == JVMTI_ERROR_NONE) {
     event.set_classCount(class_count);
     event.set_redefinitionId(op.id());
@@ -465,9 +482,18 @@ jvmtiError
 JvmtiEnv::RedefineClasses(jint class_count, const jvmtiClassDefinition* class_definitions) {
 //TODO: add locking
   EventRedefineClasses event;
-  VM_RedefineClasses op(class_count, class_definitions, jvmti_class_load_kind_redefine);
-  VMThread::execute(&op);
-  jvmtiError error = op.check_error();
+  jvmtiError error;
+
+  if (AllowEnhancedClassRedefinition) {
+    MutexLocker sd_mutex(EnhancedRedefineClasses_lock);
+    VM_EnhancedRedefineClasses op(class_count, class_definitions, jvmti_class_load_kind_redefine);
+    VMThread::execute(&op);
+    error = (op.check_error());
+  } else {
+    VM_RedefineClasses op(class_count, class_definitions, jvmti_class_load_kind_redefine);
+    VMThread::execute(&op);
+    error = op.check_error();
+  }
   if (error == JVMTI_ERROR_NONE) {
     event.set_classCount(class_count);
     event.set_redefinitionId(op.id());
diff --git a/src/hotspot/share/prims/jvmtiExport.cpp b/src/hotspot/share/prims/jvmtiExport.cpp
index d4579dfa43d..8e20fb64a8b 100644
--- a/src/hotspot/share/prims/jvmtiExport.cpp
+++ b/src/hotspot/share/prims/jvmtiExport.cpp
@@ -2796,7 +2796,7 @@ JvmtiDynamicCodeEventCollector::JvmtiDynamicCodeEventCollector() : _code_blobs(N
 // iterate over any code blob descriptors collected and post a
 // DYNAMIC_CODE_GENERATED event to the profiler.
 JvmtiDynamicCodeEventCollector::~JvmtiDynamicCodeEventCollector() {
-  assert(!JavaThread::current()->owns_locks(), "all locks must be released to post deferred events");
+  assert(AllowEnhancedClassRedefinition || !JavaThread::current()->owns_locks(), "all locks must be released to post deferred events");
  // iterate over any code blob descriptors that we collected
  if (_code_blobs != NULL) {
    for (int i=0; i<_code_blobs->length(); i++) {
diff --git a/src/hotspot/share/prims/jvmtiExport.hpp b/src/hotspot/share/prims/jvmtiExport.hpp
index f903b32be64..345cb21af33 100644
--- a/src/hotspot/share/prims/jvmtiExport.hpp
+++ b/src/hotspot/share/prims/jvmtiExport.hpp
@@ -176,6 +176,7 @@ class JvmtiExport : public AllStatic {
   // systems as needed to relax invariant checks.
   static uint64_t _redefinition_count;
   friend class VM_RedefineClasses;
+  friend class VM_EnhancedRedefineClasses;
   inline static void increment_redefinition_count() {
     JVMTI_ONLY(_redefinition_count++;)
   }
diff --git a/src/hotspot/share/prims/jvmtiGetLoadedClasses.cpp b/src/hotspot/share/prims/jvmtiGetLoadedClasses.cpp
index 4918d90212f..1c7677f270f 100644
--- a/src/hotspot/share/prims/jvmtiGetLoadedClasses.cpp
+++ b/src/hotspot/share/prims/jvmtiGetLoadedClasses.cpp
@@ -70,12 +70,19 @@ public:
 
   void do_klass(Klass* k) {
     // Collect all jclasses
-    _classStack.push((jclass) _env->jni_reference(Handle(_cur_thread, k->java_mirror())));
-    if (_dictionary_walk) {
-      // Collect array classes this way when walking the dictionary (because array classes are
-      // not in the dictionary).
-      for (Klass* l = k->array_klass_or_null(); l != NULL; l = l->array_klass_or_null()) {
-        _classStack.push((jclass) _env->jni_reference(Handle(_cur_thread, l->java_mirror())));
+    // Collect all jclasses
+    // DCEVM : LoadedClassesClosure in dcevm7 iterates over classes from SystemDictionary therefore the class "k" is always
+    //         the new version (SystemDictionary stores only new versions). But the LoadedClassesClosure's functionality was
+    //         changed in java8  where jvmtiLoadedClasses collects all classes from all classloaders, therefore we
+    //         must use new versions only.
+    if (k->new_version()==NULL) {
+      _classStack.push((jclass) _env->jni_reference(Handle(_cur_thread, k->java_mirror())));
+      if (_dictionary_walk) {
+        // Collect array classes this way when walking the dictionary (because array classes are
+        // not in the dictionary).
+        for (Klass* l = k->array_klass_or_null(); l != NULL; l = l->array_klass_or_null()) {
+          _classStack.push((jclass) _env->jni_reference(Handle(_cur_thread, l->java_mirror())));
+        }
       }
     }
   }
diff --git a/src/hotspot/share/prims/jvmtiImpl.cpp b/src/hotspot/share/prims/jvmtiImpl.cpp
index c22eb9a3dc8..a1ea25de1ea 100644
--- a/src/hotspot/share/prims/jvmtiImpl.cpp
+++ b/src/hotspot/share/prims/jvmtiImpl.cpp
@@ -247,6 +247,11 @@ void JvmtiBreakpoint::each_method_version_do(method_action meth_act) {
   Symbol* m_name = _method->name();
   Symbol* m_signature = _method->signature();
 
+  // (DCEVM) Go through old versions of method
+  for (Method* m = _method->old_version(); m != NULL; m = m->old_version()) {
+    (m->*meth_act)(_bci);
+  }
+
   // search previous versions if they exist
   for (InstanceKlass* pv_node = ik->previous_versions();
        pv_node != NULL;
diff --git a/src/hotspot/share/runtime/arguments.cpp b/src/hotspot/share/runtime/arguments.cpp
index 18307a4c6d8..6148bce5b9b 100644
--- a/src/hotspot/share/runtime/arguments.cpp
+++ b/src/hotspot/share/runtime/arguments.cpp
@@ -2122,6 +2122,36 @@ unsigned int addopens_count = 0;
 unsigned int addmods_count = 0;
 unsigned int patch_mod_count = 0;
 
+// Check consistency of GC selection
+bool Arguments::check_gc_consistency() {
+  // Ensure that the user has not selected conflicting sets
+  // of collectors.
+  uint i = 0;
+  if (UseSerialGC)                       i++;
+  if (UseConcMarkSweepGC)                i++;
+  if (UseParallelGC || UseParallelOldGC) i++;
+  if (UseG1GC)                           i++;
+  if (AllowEnhancedClassRedefinition) {
+    // Must use serial GC. This limitation applies because the instance size changing GC modifications
+    // are only built into the mark and compact algorithm.
+    if (!UseSerialGC && i >= 1) {
+      jio_fprintf(defaultStream::error_stream(),
+                  "Must use the serial GC with enhanced class redefinition\n");
+      return false;
+    }
+  }
+
+  if (i > 1) {
+    jio_fprintf(defaultStream::error_stream(),
+                "Conflicting collector combinations in option list; "
+                "please refer to the release notes for the combinations "
+                "allowed\n");
+    return false;
+  }
+
+  return true;
+}
+
 // Check the consistency of vm_init_args
 bool Arguments::check_vm_args_consistency() {
   // Method for adding checks for flag consistency.
@@ -2138,6 +2168,8 @@ bool Arguments::check_vm_args_consistency() {
     status = false;
   }
 
+  status = status && check_gc_consistency();
+
   if (PrintNMTStatistics) {
 #if INCLUDE_NMT
     if (MemTracker::tracking_level() == NMT_off) {
diff --git a/src/hotspot/share/runtime/arguments.hpp b/src/hotspot/share/runtime/arguments.hpp
index c652ee3f9fe..9c656124d5b 100644
--- a/src/hotspot/share/runtime/arguments.hpp
+++ b/src/hotspot/share/runtime/arguments.hpp
@@ -511,6 +511,7 @@ class Arguments : AllStatic {
   static bool process_settings_file(const char* file_name, bool should_exist, jboolean ignore_unrecognized);
 
   static size_t conservative_max_heap_alignment() { return _conservative_max_heap_alignment; }
+
   // Return the maximum size a heap with compressed oops can take
   static size_t max_heap_for_compressed_oops();
 
diff --git a/src/hotspot/share/runtime/globals.hpp b/src/hotspot/share/runtime/globals.hpp
index 0fa47b34c40..de9193ca447 100644
--- a/src/hotspot/share/runtime/globals.hpp
+++ b/src/hotspot/share/runtime/globals.hpp
@@ -2458,6 +2458,11 @@ const size_t minimumSymbolTableSize = 1024;
                                                                             \
   diagnostic(bool, DeoptimizeNMethodBarriersALot, false,                    \
                 "Make nmethod barriers deoptimise a lot.")                  \
+                                                                            \
+  product(bool, AllowEnhancedClassRedefinition, true,                       \
+             "Allow enhanced class redefinition beyond swapping method "    \
+             "bodies")
+
 
 // Interface macros
 #define DECLARE_PRODUCT_FLAG(type, name, value, doc)      extern "C" type name;
diff --git a/src/hotspot/share/runtime/interfaceSupport.inline.hpp b/src/hotspot/share/runtime/interfaceSupport.inline.hpp
index 3793aaa8d75..9126dd7dffc 100644
--- a/src/hotspot/share/runtime/interfaceSupport.inline.hpp
+++ b/src/hotspot/share/runtime/interfaceSupport.inline.hpp
@@ -220,8 +220,8 @@ class ThreadToNativeFromVM : public ThreadStateTransition {
  public:
   ThreadToNativeFromVM(JavaThread *thread) : ThreadStateTransition(thread) {
     // We are leaving the VM at this point and going directly to native code.
-    // Block, if we are in the middle of a safepoint synchronization.
-    assert(!thread->owns_locks(), "must release all locks when leaving VM");
+    // DCEVM allow locks on leaving JVM
+    assert(AllowEnhancedClassRedefinition || !thread->owns_locks(), "must release all locks when leaving VM");
     thread->frame_anchor()->make_walkable(thread);
     trans(_thread_in_vm, _thread_in_native);
     // Check for pending. async. exceptions or suspends.
diff --git a/src/hotspot/share/runtime/javaCalls.cpp b/src/hotspot/share/runtime/javaCalls.cpp
index e08b1232777..a543935c995 100644
--- a/src/hotspot/share/runtime/javaCalls.cpp
+++ b/src/hotspot/share/runtime/javaCalls.cpp
@@ -56,7 +56,8 @@ JavaCallWrapper::JavaCallWrapper(const methodHandle& callee_method, Handle recei
   bool clear_pending_exception = true;
 
   guarantee(thread->is_Java_thread(), "crucial check - the VM thread cannot and must not escape to Java code");
-  assert(!thread->owns_locks(), "must release all locks when leaving VM");
+  // DCEVM allow locks on leaving JVM
+  assert(AllowEnhancedClassRedefinition || !thread->owns_locks(), "must release all locks when leaving VM");
   guarantee(thread->can_call_java(), "cannot make java calls from the native compiler");
   _result   = result;
 
diff --git a/src/hotspot/share/runtime/mutexLocker.cpp b/src/hotspot/share/runtime/mutexLocker.cpp
index e45e8982768..6f982072909 100644
--- a/src/hotspot/share/runtime/mutexLocker.cpp
+++ b/src/hotspot/share/runtime/mutexLocker.cpp
@@ -5,7 +5,7 @@
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
- *
+ *                     \
  * This code is distributed in the hope that it will be useful, but WITHOUT
  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
@@ -115,6 +115,7 @@ Mutex*   FreeList_lock                = NULL;
 Mutex*   OldSets_lock                 = NULL;
 Monitor* RootRegionScan_lock          = NULL;
 
+Mutex* EnhancedRedefineClasses_lock   = NULL;
 Mutex*   Management_lock              = NULL;
 Monitor* Service_lock                 = NULL;
 Monitor* Notification_lock            = NULL;
@@ -286,6 +287,7 @@ void mutex_init() {
   def(InitCompleted_lock           , PaddedMonitor, leaf,        true,  _safepoint_check_never);
   def(VtableStubs_lock             , PaddedMutex  , nonleaf,     true,  _safepoint_check_never);
   def(Notify_lock                  , PaddedMonitor, nonleaf,     true,  _safepoint_check_always);
+  def(EnhancedRedefineClasses_lock , PaddedMutex  , nonleaf+7,   false, Monitor::_safepoint_check_always);     // for ensuring that class redefinition is not done in parallel
   def(JNICritical_lock             , PaddedMonitor, nonleaf,     true,  _safepoint_check_always); // used for JNI critical regions
   def(AdapterHandlerLibrary_lock   , PaddedMutex  , nonleaf,     true,  _safepoint_check_always);
 
diff --git a/src/hotspot/share/runtime/mutexLocker.hpp b/src/hotspot/share/runtime/mutexLocker.hpp
index 90b17b0c234..29d1b67d921 100644
--- a/src/hotspot/share/runtime/mutexLocker.hpp
+++ b/src/hotspot/share/runtime/mutexLocker.hpp
@@ -106,6 +106,8 @@ extern Mutex*   PerfDataMemAlloc_lock;           // a lock on the allocator for
 extern Mutex*   PerfDataManager_lock;            // a long on access to PerfDataManager resources
 extern Mutex*   OopMapCacheAlloc_lock;           // protects allocation of oop_map caches
 
+extern Mutex* EnhancedRedefineClasses_lock;      // locks classes from parallel enhanced redefinition
+
 extern Mutex*   FreeList_lock;                   // protects the free region list during safepoints
 extern Mutex*   OldSets_lock;                    // protects the old region sets
 extern Monitor* RootRegionScan_lock;             // used to notify that the CM threads have finished scanning the IM snapshot regions
diff --git a/src/hotspot/share/runtime/reflection.cpp b/src/hotspot/share/runtime/reflection.cpp
index 02e7b1973fa..0e7722dba7d 100644
--- a/src/hotspot/share/runtime/reflection.cpp
+++ b/src/hotspot/share/runtime/reflection.cpp
@@ -626,6 +626,12 @@ bool Reflection::verify_member_access(const Klass* current_class,
                                       bool classloader_only,
                                       bool protected_restriction,
                                       TRAPS) {
+
+  // (DCEVM) Decide accessibility based on active version
+  if (current_class != NULL) {
+    current_class = current_class->active_version();
+  }
+
   // Verify that current_class can access a member of member_class, where that
   // field's access bits are "access".  We assume that we've already verified
   // that current_class can access member_class.
-- 
2.23.0

