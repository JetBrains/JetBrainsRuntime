From 7efd979183409db33e965cf80adf60c9a970c762 Mon Sep 17 00:00:00 2001
From: skybber <lada.dvorak7@gmail.com>
Date: Wed, 14 Nov 2018 21:09:39 +0100
Subject: [PATCH 01/48] Apply basic dcevm11 patch

---
 src/hotspot/share/ci/ciObjectFactory.cpp      |   25 +
 src/hotspot/share/ci/ciObjectFactory.hpp      |    3 +
 .../share/classfile/classFileParser.cpp       |   18 +-
 .../share/classfile/classFileParser.hpp       |    8 +
 src/hotspot/share/classfile/classLoader.cpp   |    1 +
 .../share/classfile/classLoaderData.cpp       |   20 +-
 .../share/classfile/classLoaderData.hpp       |    5 +
 .../share/classfile/classLoaderExt.cpp        |    1 +
 src/hotspot/share/classfile/dictionary.cpp    |   46 +-
 src/hotspot/share/classfile/dictionary.hpp    |   10 +
 src/hotspot/share/classfile/javaClasses.cpp   |   51 +
 src/hotspot/share/classfile/javaClasses.hpp   |   52 +-
 src/hotspot/share/classfile/klassFactory.cpp  |    3 +
 src/hotspot/share/classfile/klassFactory.hpp  |    1 +
 .../share/classfile/loaderConstraints.cpp     |   15 +-
 .../share/classfile/loaderConstraints.hpp     |    3 +
 .../share/classfile/systemDictionary.cpp      |   46 +-
 .../share/classfile/systemDictionary.hpp      |    9 +-
 .../classfile/systemDictionaryShared.cpp      |    2 +-
 src/hotspot/share/classfile/verifier.cpp      |    2 +-
 src/hotspot/share/classfile/verifier.hpp      |    1 +
 src/hotspot/share/classfile/vmSymbols.hpp     |    8 +
 .../share/gc/cms/compactibleFreeListSpace.cpp |   11 +-
 .../share/gc/cms/compactibleFreeListSpace.hpp |    5 +-
 src/hotspot/share/gc/g1/heapRegion.hpp        |    2 +-
 src/hotspot/share/gc/serial/genMarkSweep.cpp  |    4 +
 src/hotspot/share/gc/serial/markSweep.cpp     |   99 +
 src/hotspot/share/gc/serial/markSweep.hpp     |    7 +
 src/hotspot/share/gc/shared/gcConfig.cpp      |    5 +-
 src/hotspot/share/gc/shared/space.cpp         |  200 +-
 src/hotspot/share/gc/shared/space.hpp         |   18 +-
 src/hotspot/share/gc/shared/space.inline.hpp  |   49 +-
 .../share/interpreter/linkResolver.cpp        |    7 +-
 .../jfrEventClassTransformer.cpp              |    1 +
 src/hotspot/share/memory/universe.cpp         |   39 +
 src/hotspot/share/memory/universe.hpp         |   12 +
 src/hotspot/share/oops/cpCache.cpp            |   35 +-
 src/hotspot/share/oops/cpCache.hpp            |   16 +-
 src/hotspot/share/oops/instanceKlass.cpp      |   56 +-
 src/hotspot/share/oops/instanceKlass.hpp      |    7 +
 src/hotspot/share/oops/klass.cpp              |   29 +-
 src/hotspot/share/oops/klass.hpp              |   47 +
 src/hotspot/share/oops/method.cpp             |    6 +
 src/hotspot/share/oops/method.hpp             |   20 +
 src/hotspot/share/prims/jni.cpp               |    1 +
 src/hotspot/share/prims/jvm.cpp               |    1 +
 .../prims/jvmtiEnhancedRedefineClasses.cpp    | 2255 +++++++++++++++++
 .../prims/jvmtiEnhancedRedefineClasses.hpp    |  202 ++
 src/hotspot/share/prims/jvmtiEnv.cpp          |   26 +-
 src/hotspot/share/prims/jvmtiExport.cpp       |    2 +-
 src/hotspot/share/prims/jvmtiExport.hpp       |    1 +
 .../share/prims/jvmtiGetLoadedClasses.cpp     |    8 +-
 src/hotspot/share/prims/jvmtiImpl.cpp         |    5 +
 .../share/prims/resolvedMethodTable.cpp       |   39 +-
 src/hotspot/share/runtime/arguments.cpp       |   32 +
 src/hotspot/share/runtime/arguments.hpp       |    1 +
 src/hotspot/share/runtime/globals.hpp         |    6 +-
 .../share/runtime/interfaceSupport.inline.hpp |    4 +-
 src/hotspot/share/runtime/javaCalls.cpp       |    3 +-
 src/hotspot/share/runtime/mutexLocker.cpp     |    5 +-
 src/hotspot/share/runtime/mutexLocker.hpp     |    2 +
 src/hotspot/share/runtime/reflection.cpp      |    6 +
 62 files changed, 3530 insertions(+), 74 deletions(-)
 create mode 100644 src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.cpp
 create mode 100644 src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.hpp

diff --git a/src/hotspot/share/ci/ciObjectFactory.cpp b/src/hotspot/share/ci/ciObjectFactory.cpp
index a5dd94f4bb8..66bbe835e7b 100644
--- a/src/hotspot/share/ci/ciObjectFactory.cpp
+++ b/src/hotspot/share/ci/ciObjectFactory.cpp
@@ -708,3 +708,28 @@ void ciObjectFactory::print() {
              _unloaded_instances->length(),
              _unloaded_klasses->length());
 }
+
+
+int ciObjectFactory::compare_cimetadata(ciMetadata** a, ciMetadata** b) {
+  Metadata* am = (*a)->constant_encoding();
+  Metadata* bm = (*b)->constant_encoding();
+  return ((am > bm) ? 1 : ((am == bm) ? 0 : -1));
+}
+
+// FIXME: review... Resoring the ciObject arrays after class redefinition
+void ciObjectFactory::resort_shared_ci_metadata() {
+  if (_shared_ci_metadata == NULL) return;
+  _shared_ci_metadata->sort(ciObjectFactory::compare_cimetadata);
+
+#ifdef ASSERT
+  if (CIObjectFactoryVerify) {
+    Metadata* last = NULL;
+    for (int j = 0; j< _shared_ci_metadata->length(); j++) {
+      Metadata* o = _shared_ci_metadata->at(j)->constant_encoding();
+      assert(last < o, "out of order");
+      last = o;
+    }
+  }
+#endif // ASSERT
+}
+
diff --git a/src/hotspot/share/ci/ciObjectFactory.hpp b/src/hotspot/share/ci/ciObjectFactory.hpp
index 1063c9853e1..3e9d48c4cdc 100644
--- a/src/hotspot/share/ci/ciObjectFactory.hpp
+++ b/src/hotspot/share/ci/ciObjectFactory.hpp
@@ -88,6 +88,7 @@ private:
 
   ciInstance* get_unloaded_instance(ciInstanceKlass* klass);
 
+  static int compare_cimetadata(ciMetadata** a, ciMetadata** b);
 public:
   static bool is_initialized() { return _initialized; }
 
@@ -143,6 +144,8 @@ public:
 
   void print_contents();
   void print();
+
+  static void resort_shared_ci_metadata();
 };
 
 #endif // SHARE_VM_CI_CIOBJECTFACTORY_HPP
diff --git a/src/hotspot/share/classfile/classFileParser.cpp b/src/hotspot/share/classfile/classFileParser.cpp
index 0d4862dd449..ea150df9104 100644
--- a/src/hotspot/share/classfile/classFileParser.cpp
+++ b/src/hotspot/share/classfile/classFileParser.cpp
@@ -954,6 +954,8 @@ void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,
                                                   CHECK);
       }
 
+      interf = (Klass *) maybe_newest(interf);
+
       if (!interf->is_interface()) {
         THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),
                   err_msg("class %s can not implement %s, because it is not an interface (%s)",
@@ -3747,7 +3749,7 @@ const InstanceKlass* ClassFileParser::parse_super_class(ConstantPool* const cp,
     // However, make sure it is not an array type.
     bool is_array = false;
     if (cp->tag_at(super_class_index).is_klass()) {
-      super_klass = InstanceKlass::cast(cp->resolved_klass_at(super_class_index));
+      super_klass = InstanceKlass::cast(maybe_newest(cp->resolved_klass_at(super_class_index)));
       if (need_verify)
         is_array = super_klass->is_array_klass();
     } else if (need_verify) {
@@ -4415,7 +4417,10 @@ void ClassFileParser::set_precomputed_flags(InstanceKlass* ik) {
   if (!_has_empty_finalizer) {
     if (_has_finalizer ||
         (super != NULL && super->has_finalizer())) {
-      ik->set_has_finalizer();
+        // FIXME - condition from previous DCEVM version, however after reload new finelize() method is not active
+        if (ik->old_version() == NULL || ik->old_version()->has_finalizer()) {
+          ik->set_has_finalizer();
+        }
     }
   }
 
@@ -5792,6 +5797,7 @@ ClassFileParser::ClassFileParser(ClassFileStream* stream,
                                  const InstanceKlass* host_klass,
                                  GrowableArray<Handle>* cp_patches,
                                  Publicity pub_level,
+                                 const bool pick_newest,
                                  TRAPS) :
   _stream(stream),
   _requested_name(name),
@@ -5850,7 +5856,8 @@ ClassFileParser::ClassFileParser(ClassFileStream* stream,
   _has_finalizer(false),
   _has_empty_finalizer(false),
   _has_vanilla_constructor(false),
-  _max_bootstrap_specifier_index(-1) {
+  _max_bootstrap_specifier_index(-1),
+  _pick_newest(pick_newest) {
 
   _class_name = name != NULL ? name : vmSymbols::unknown_class_name();
 
@@ -6251,14 +6258,15 @@ void ClassFileParser::post_process_parsed_stream(const ClassFileStream* const st
         CHECK);
     }
     Handle loader(THREAD, _loader_data->class_loader());
-    _super_klass = (const InstanceKlass*)
+    const Klass* super_klass =
                        SystemDictionary::resolve_super_or_fail(_class_name,
                                                                super_class_name,
                                                                loader,
                                                                _protection_domain,
                                                                true,
                                                                CHECK);
-  }
+   _super_klass = (const InstanceKlass*) maybe_newest(super_klass);
+ }
 
   if (_super_klass != NULL) {
     if (_super_klass->has_nonstatic_concrete_methods()) {
diff --git a/src/hotspot/share/classfile/classFileParser.hpp b/src/hotspot/share/classfile/classFileParser.hpp
index 9f7204be9b6..3db14b678f3 100644
--- a/src/hotspot/share/classfile/classFileParser.hpp
+++ b/src/hotspot/share/classfile/classFileParser.hpp
@@ -115,6 +115,9 @@ class ClassFileParser {
   const intArray* _method_ordering;
   GrowableArray<Method*>* _all_mirandas;
 
+  // Enhanced class redefinition
+  const bool _pick_newest;
+
   enum { fixed_buffer_size = 128 };
   u_char _linenumbertable_buffer[fixed_buffer_size];
 
@@ -496,6 +499,9 @@ class ClassFileParser {
                      FieldLayoutInfo* info,
                      TRAPS);
 
+  // Enhanced class redefinition
+  inline const Klass* maybe_newest(const Klass* klass) const { return klass != NULL && _pick_newest ? klass->newest_version() : klass; }
+
  public:
   ClassFileParser(ClassFileStream* stream,
                   Symbol* name,
@@ -504,6 +510,7 @@ class ClassFileParser {
                   const InstanceKlass* host_klass,
                   GrowableArray<Handle>* cp_patches,
                   Publicity pub_level,
+                  const bool pick_newest,
                   TRAPS);
 
   ~ClassFileParser();
@@ -532,6 +539,7 @@ class ClassFileParser {
   ClassLoaderData* loader_data() const { return _loader_data; }
   const Symbol* class_name() const { return _class_name; }
   const InstanceKlass* super_klass() const { return _super_klass; }
+  Array<Klass*>* local_interfaces() const { return _local_interfaces; }
 
   ReferenceType reference_type() const { return _rt; }
   AccessFlags access_flags() const { return _access_flags; }
diff --git a/src/hotspot/share/classfile/classLoader.cpp b/src/hotspot/share/classfile/classLoader.cpp
index 2c73ef93934..28ed54b93e1 100644
--- a/src/hotspot/share/classfile/classLoader.cpp
+++ b/src/hotspot/share/classfile/classLoader.cpp
@@ -1501,6 +1501,7 @@ InstanceKlass* ClassLoader::load_class(Symbol* name, bool search_append_only, TR
                                                            protection_domain,
                                                            NULL, // host_klass
                                                            NULL, // cp_patches
+                                                           false, // pick_newest
                                                            THREAD);
   if (HAS_PENDING_EXCEPTION) {
     if (DumpSharedSpaces) {
diff --git a/src/hotspot/share/classfile/classLoaderData.cpp b/src/hotspot/share/classfile/classLoaderData.cpp
index c58026ae192..25103fff2c0 100644
--- a/src/hotspot/share/classfile/classLoaderData.cpp
+++ b/src/hotspot/share/classfile/classLoaderData.cpp
@@ -1242,6 +1242,12 @@ void ClassLoaderDataGraph::dictionary_classes_do(void f(InstanceKlass*)) {
   }
 }
 
+void ClassLoaderDataGraph::dictionary_classes_do(KlassClosure* klass_closure) {
+  FOR_ALL_DICTIONARY(cld) {
+    cld->dictionary()->classes_do(klass_closure);
+  }
+}
+
 // Only walks the classes defined in this class loader.
 void ClassLoaderDataGraph::dictionary_classes_do(void f(InstanceKlass*, TRAPS), TRAPS) {
   Thread* thread = Thread::current();
@@ -1251,6 +1257,12 @@ void ClassLoaderDataGraph::dictionary_classes_do(void f(InstanceKlass*, TRAPS),
   }
 }
 
+void ClassLoaderDataGraph::rollback_redefinition() {
+  FOR_ALL_DICTIONARY(cld) {
+    cld->dictionary()->rollback_redefinition();
+  }
+}
+
 // Walks all entries in the dictionary including entries initiated by this class loader.
 void ClassLoaderDataGraph::dictionary_all_entries_do(void f(InstanceKlass*, ClassLoaderData*)) {
   Thread* thread = Thread::current();
@@ -1372,11 +1384,15 @@ bool ClassLoaderDataGraph::do_unloading(bool clean_previous_versions) {
   // Mark metadata seen on the stack only so we can delete unneeded entries.
   // Only walk all metadata, including the expensive code cache walk, for Full GC
   // and only if class redefinition and if there's previous versions of
-  // Klasses to delete.
+  // Klassesoto delete.
+
+  // FIXME: dcevm - block asserts in MetadataOnStackMark
+  /*
   bool walk_all_metadata = clean_previous_versions &&
                            JvmtiExport::has_redefined_a_class() &&
                            InstanceKlass::has_previous_versions_and_reset();
   MetadataOnStackMark md_on_stack(walk_all_metadata);
+  */
 
   // Save previous _unloading pointer for CMS which may add to unloading list before
   // purging and we don't want to rewalk the previously unloaded class loader data.
@@ -1386,10 +1402,12 @@ bool ClassLoaderDataGraph::do_unloading(bool clean_previous_versions) {
   while (data != NULL) {
     if (data->is_alive()) {
       // clean metaspace
+      /*
       if (walk_all_metadata) {
         data->classes_do(InstanceKlass::purge_previous_versions);
       }
       data->free_deallocate_list();
+      */
       prev = data;
       data = data->next();
       loaders_processed++;
diff --git a/src/hotspot/share/classfile/classLoaderData.hpp b/src/hotspot/share/classfile/classLoaderData.hpp
index 152cc26efdb..7e357929971 100644
--- a/src/hotspot/share/classfile/classLoaderData.hpp
+++ b/src/hotspot/share/classfile/classLoaderData.hpp
@@ -125,6 +125,11 @@ class ClassLoaderDataGraph : public AllStatic {
   // Added for initialize_itable_for_klass to handle exceptions.
   static void dictionary_classes_do(void f(InstanceKlass*, TRAPS), TRAPS);
 
+  static void dictionary_classes_do(KlassClosure* klass_closure);
+
+  // Enhanced class redefinition
+  static void rollback_redefinition();
+
   // Iterate all classes and their class loaders, including initiating class loaders.
   static void dictionary_all_entries_do(void f(InstanceKlass*, ClassLoaderData*));
 
diff --git a/src/hotspot/share/classfile/classLoaderExt.cpp b/src/hotspot/share/classfile/classLoaderExt.cpp
index f5157e07c76..e9e52f00e8a 100644
--- a/src/hotspot/share/classfile/classLoaderExt.cpp
+++ b/src/hotspot/share/classfile/classLoaderExt.cpp
@@ -301,6 +301,7 @@ InstanceKlass* ClassLoaderExt::load_class(Symbol* name, const char* path, TRAPS)
                                                            protection_domain,
                                                            NULL, // host_klass
                                                            NULL, // cp_patches
+                                                           false,
                                                            THREAD);
 
   if (HAS_PENDING_EXCEPTION) {
diff --git a/src/hotspot/share/classfile/dictionary.cpp b/src/hotspot/share/classfile/dictionary.cpp
index 3ac1ca69ae5..118730f1b83 100644
--- a/src/hotspot/share/classfile/dictionary.cpp
+++ b/src/hotspot/share/classfile/dictionary.cpp
@@ -245,6 +245,19 @@ void Dictionary::classes_do(void f(InstanceKlass*)) {
   }
 }
 
+void Dictionary::classes_do(KlassClosure* closure) {
+  for (int index = 0; index < table_size(); index++) {
+    for (DictionaryEntry* probe = bucket(index);
+                          probe != NULL;
+                          probe = probe->next()) {
+      InstanceKlass* k = probe->instance_klass();
+      if (loader_data() == k->class_loader_data()) {
+        closure->do_klass(k);
+      }
+    }
+  }
+}
+
 // Added for initialize_itable_for_klass to handle exceptions
 //   Just the classes from defining class loaders
 void Dictionary::classes_do(void f(InstanceKlass*, TRAPS), TRAPS) {
@@ -329,6 +342,33 @@ DictionaryEntry* Dictionary::get_entry(int index, unsigned int hash,
   return NULL;
 }
 
+bool Dictionary::update_klass(unsigned int hash, Symbol* name, ClassLoaderData* loader_data, InstanceKlass* k, InstanceKlass* old_klass) {
+  // There are several entries for the same class in the dictionary: One extra entry for each parent classloader of the classloader of the class.
+  bool found = false;
+  for (int index = 0; index < table_size(); index++) {
+    for (DictionaryEntry* entry = bucket(index); entry != NULL; entry = entry->next()) {
+      if (entry->instance_klass() == old_klass) {
+        entry->set_literal(k);
+        found = true;
+      }
+    }
+  }
+  return found;
+}
+
+void Dictionary::rollback_redefinition() {
+  for (int index = 0; index < table_size(); index++) {
+    for (DictionaryEntry* entry = bucket(index);
+                          entry != NULL;
+                          entry = entry->next()) {
+      if (entry->instance_klass()->is_redefining()) {
+        entry->set_literal((InstanceKlass*) entry->instance_klass()->old_version());
+      }
+    }
+  }
+}
+
+
 
 InstanceKlass* Dictionary::find(unsigned int hash, Symbol* name,
                                 Handle protection_domain) {
@@ -337,7 +377,7 @@ InstanceKlass* Dictionary::find(unsigned int hash, Symbol* name,
   int index = hash_to_index(hash);
   DictionaryEntry* entry = get_entry(index, hash, name);
   if (entry != NULL && entry->is_valid_protection_domain(protection_domain)) {
-    return entry->instance_klass();
+    return old_if_redefined(entry->instance_klass());
   } else {
     return NULL;
   }
@@ -350,7 +390,7 @@ InstanceKlass* Dictionary::find_class(int index, unsigned int hash,
   assert (index == index_for(name), "incorrect index?");
 
   DictionaryEntry* entry = get_entry(index, hash, name);
-  return (entry != NULL) ? entry->instance_klass() : NULL;
+  return old_if_redefined((entry != NULL) ? entry->instance_klass() : NULL);
 }
 
 
@@ -362,7 +402,7 @@ InstanceKlass* Dictionary::find_shared_class(int index, unsigned int hash,
   assert (index == index_for(name), "incorrect index?");
 
   DictionaryEntry* entry = get_entry(index, hash, name);
-  return (entry != NULL) ? entry->instance_klass() : NULL;
+  return old_if_redefined((entry != NULL) ? entry->instance_klass() : NULL);
 }
 
 
diff --git a/src/hotspot/share/classfile/dictionary.hpp b/src/hotspot/share/classfile/dictionary.hpp
index d26f1f11fb4..fd4b134d7a7 100644
--- a/src/hotspot/share/classfile/dictionary.hpp
+++ b/src/hotspot/share/classfile/dictionary.hpp
@@ -71,6 +71,7 @@ public:
   InstanceKlass* find_shared_class(int index, unsigned int hash, Symbol* name);
 
   void classes_do(void f(InstanceKlass*));
+  void classes_do(KlassClosure* closure);
   void classes_do(void f(InstanceKlass*, TRAPS), TRAPS);
   void all_entries_do(void f(InstanceKlass*, ClassLoaderData*));
   void classes_do(MetaspaceClosure* it);
@@ -112,6 +113,15 @@ public:
   }
 
   void free_entry(DictionaryEntry* entry);
+
+  // Enhanced class redefinition
+  bool update_klass(unsigned int hash, Symbol* name, ClassLoaderData* loader_data, InstanceKlass* k, InstanceKlass* old_klass);
+
+  void rollback_redefinition();
+
+  static InstanceKlass* old_if_redefined(InstanceKlass* k) {
+    return (k != NULL && k->is_redefining()) ? ((InstanceKlass* )k->old_version()) : k;
+  }
 };
 
 // An entry in the class loader data dictionaries, this describes a class as
diff --git a/src/hotspot/share/classfile/javaClasses.cpp b/src/hotspot/share/classfile/javaClasses.cpp
index 438e7aa4ec9..a89443d22ea 100644
--- a/src/hotspot/share/classfile/javaClasses.cpp
+++ b/src/hotspot/share/classfile/javaClasses.cpp
@@ -2443,6 +2443,8 @@ void java_lang_Throwable::fill_in_stack_trace(Handle throwable, const methodHand
         skip_throwableInit_check = true;
       }
     }
+    // (DCEVM): Line numbers from newest version must be used for EMCP-swapped methods
+    method = method->newest_version();
     if (method->is_hidden()) {
       if (skip_hidden)  continue;
     }
@@ -3625,6 +3627,50 @@ void java_lang_invoke_DirectMethodHandle::serialize_offsets(SerializeClosure* f)
 }
 #endif
 
+// Support for java_lang_invoke_DirectMethodHandle$StaticAccessor
+
+int java_lang_invoke_DirectMethodHandle_StaticAccessor::_static_offset_offset;
+
+long java_lang_invoke_DirectMethodHandle_StaticAccessor::static_offset(oop dmh) {
+  assert(_static_offset_offset != 0, "");
+  return dmh->long_field(_static_offset_offset);
+}
+
+void java_lang_invoke_DirectMethodHandle_StaticAccessor::set_static_offset(oop dmh, long static_offset) {
+  assert(_static_offset_offset != 0, "");
+  dmh->long_field_put(_static_offset_offset, static_offset);
+}
+
+
+void java_lang_invoke_DirectMethodHandle_StaticAccessor::compute_offsets() {
+  Klass* klass_oop = SystemDictionary::DirectMethodHandle_StaticAccessor_klass();
+  if (klass_oop != NULL) {
+    compute_offset(_static_offset_offset, InstanceKlass::cast(klass_oop), vmSymbols::static_offset_name(), vmSymbols::long_signature());
+  }
+}
+
+// Support for java_lang_invoke_DirectMethodHandle$Accessor
+
+int java_lang_invoke_DirectMethodHandle_Accessor::_field_offset_offset;
+
+int java_lang_invoke_DirectMethodHandle_Accessor::field_offset(oop dmh) {
+  assert(_field_offset_offset != 0, "");
+  return dmh->int_field(_field_offset_offset);
+}
+
+void java_lang_invoke_DirectMethodHandle_Accessor::set_field_offset(oop dmh, int field_offset) {
+  assert(_field_offset_offset != 0, "");
+  dmh->int_field_put(_field_offset_offset, field_offset);
+}
+
+
+void java_lang_invoke_DirectMethodHandle_Accessor::compute_offsets() {
+  Klass* klass_oop = SystemDictionary::DirectMethodHandle_Accessor_klass();
+  if (klass_oop != NULL) {
+    compute_offset(_field_offset_offset, InstanceKlass::cast(klass_oop), vmSymbols::field_offset_name(), vmSymbols::int_signature());
+  }
+}
+
 // Support for java_lang_invoke_MethodHandle
 
 int java_lang_invoke_MethodHandle::_type_offset;
@@ -3811,6 +3857,11 @@ void java_lang_invoke_ResolvedMethodName::set_vmtarget(oop resolved_method, Meth
   resolved_method->address_field_put(_vmtarget_offset, (address)m);
 }
 
+void java_lang_invoke_ResolvedMethodName::set_vmholder_offset(oop resolved_method, Method* m) {
+  assert(is_instance(resolved_method), "wrong type");
+  resolved_method->obj_field_put(_vmholder_offset, m->method_holder()->java_mirror());
+}
+
 oop java_lang_invoke_ResolvedMethodName::find_resolved_method(const methodHandle& m, TRAPS) {
   // lookup ResolvedMethod oop in the table, or create a new one and intern it
   oop resolved_method = ResolvedMethodTable::find_method(m());
diff --git a/src/hotspot/share/classfile/javaClasses.hpp b/src/hotspot/share/classfile/javaClasses.hpp
index bd9cdca3fe3..ceb1670df5d 100644
--- a/src/hotspot/share/classfile/javaClasses.hpp
+++ b/src/hotspot/share/classfile/javaClasses.hpp
@@ -253,10 +253,10 @@ class java_lang_Class : AllStatic {
   static void set_init_lock(oop java_class, oop init_lock);
   static void set_protection_domain(oop java_class, oop protection_domain);
   static void set_class_loader(oop java_class, oop class_loader);
-  static void set_component_mirror(oop java_class, oop comp_mirror);
   static void initialize_mirror_fields(Klass* k, Handle mirror, Handle protection_domain, TRAPS);
   static void set_mirror_module_field(Klass* K, Handle mirror, Handle module, TRAPS);
  public:
+  static void set_component_mirror(oop java_class, oop comp_mirror);
   static void allocate_fixup_lists();
   static void compute_offsets();
 
@@ -1055,6 +1055,55 @@ class java_lang_invoke_DirectMethodHandle: AllStatic {
   static int member_offset_in_bytes()           { return _member_offset; }
 };
 
+// Interface to java.lang.invoke.DirectMethodHandle$StaticAccessor objects
+
+class java_lang_invoke_DirectMethodHandle_StaticAccessor: AllStatic {
+  friend class JavaClasses;
+
+ private:
+  static int _static_offset_offset;               // offset to static field
+
+  static void compute_offsets();
+
+ public:
+  // Accessors
+  static long      static_offset(oop dmh);
+  static void  set_static_offset(oop dmh, long value);
+
+  // Testers
+  static bool is_subclass(Klass* klass) {
+    return klass->is_subclass_of(SystemDictionary::DirectMethodHandle_StaticAccessor_klass());
+  }
+  static bool is_instance(oop obj) {
+    return obj != NULL && is_subclass(obj->klass());
+  }
+};
+
+// Interface to java.lang.invoke.DirectMethodHandle$Accessor objects
+
+class java_lang_invoke_DirectMethodHandle_Accessor: AllStatic {
+  friend class JavaClasses;
+
+ private:
+  static int _field_offset_offset;               // offset to field
+
+  static void compute_offsets();
+
+ public:
+  // Accessors
+  static int      field_offset(oop dmh);
+  static void set_field_offset(oop dmh, int value);
+
+  // Testers
+  static bool is_subclass(Klass* klass) {
+    return klass->is_subclass_of(SystemDictionary::DirectMethodHandle_Accessor_klass());
+  }
+  static bool is_instance(oop obj) {
+    return obj != NULL && is_subclass(obj->klass());
+  }
+};
+
+
 // Interface to java.lang.invoke.LambdaForm objects
 // (These are a private interface for managing adapter code generation.)
 
@@ -1106,6 +1155,7 @@ class java_lang_invoke_ResolvedMethodName : AllStatic {
 
   static Method* vmtarget(oop resolved_method);
   static void set_vmtarget(oop resolved_method, Method* method);
+  static void set_vmholder_offset(oop resolved_method, Method* method);
 
   // find or create resolved member name
   static oop find_resolved_method(const methodHandle& m, TRAPS);
diff --git a/src/hotspot/share/classfile/klassFactory.cpp b/src/hotspot/share/classfile/klassFactory.cpp
index 3614480f16b..355016c8f61 100644
--- a/src/hotspot/share/classfile/klassFactory.cpp
+++ b/src/hotspot/share/classfile/klassFactory.cpp
@@ -102,6 +102,7 @@ InstanceKlass* KlassFactory::check_shared_class_file_load_hook(
                              NULL,
                              NULL,
                              ClassFileParser::BROADCAST, // publicity level
+                             false,
                              CHECK_NULL);
       InstanceKlass* new_ik = parser.create_instance_klass(true /* changed_by_loadhook */,
                                                            CHECK_NULL);
@@ -185,6 +186,7 @@ InstanceKlass* KlassFactory::create_from_stream(ClassFileStream* stream,
                                                 Handle protection_domain,
                                                 const InstanceKlass* host_klass,
                                                 GrowableArray<Handle>* cp_patches,
+                                                const bool pick_newest,
                                                 TRAPS) {
   assert(stream != NULL, "invariant");
   assert(loader_data != NULL, "invariant");
@@ -217,6 +219,7 @@ InstanceKlass* KlassFactory::create_from_stream(ClassFileStream* stream,
                          host_klass,
                          cp_patches,
                          ClassFileParser::BROADCAST, // publicity level
+                         pick_newest,
                          CHECK_NULL);
 
   InstanceKlass* result = parser.create_instance_klass(old_stream != stream, CHECK_NULL);
diff --git a/src/hotspot/share/classfile/klassFactory.hpp b/src/hotspot/share/classfile/klassFactory.hpp
index c08f8b9a119..b7b96c5cfdc 100644
--- a/src/hotspot/share/classfile/klassFactory.hpp
+++ b/src/hotspot/share/classfile/klassFactory.hpp
@@ -74,6 +74,7 @@ class KlassFactory : AllStatic {
                                            Handle protection_domain,
                                            const InstanceKlass* host_klass,
                                            GrowableArray<Handle>* cp_patches,
+                                           const bool pick_newest,
                                            TRAPS);
  public:
   static InstanceKlass* check_shared_class_file_load_hook(
diff --git a/src/hotspot/share/classfile/loaderConstraints.cpp b/src/hotspot/share/classfile/loaderConstraints.cpp
index 9cd141bbfa2..e4a23e8a27c 100644
--- a/src/hotspot/share/classfile/loaderConstraints.cpp
+++ b/src/hotspot/share/classfile/loaderConstraints.cpp
@@ -87,6 +87,19 @@ LoaderConstraintEntry** LoaderConstraintTable::find_loader_constraint(
   return pp;
 }
 
+void LoaderConstraintTable::update_after_redefinition() {
+  for (int index = 0; index < table_size(); index++) {
+    LoaderConstraintEntry** p = bucket_addr(index);
+    while(*p) {
+      LoaderConstraintEntry* probe = *p;
+      if (probe->klass() != NULL) {
+        // We swap the class with the newest version with an assumption that the hash will be the same
+        probe->set_klass((InstanceKlass*) probe->klass()->newest_version());
+      }
+      p = probe->next_addr();
+    }
+  }
+}
 
 void LoaderConstraintTable::purge_loader_constraints() {
   assert(SafepointSynchronize::is_at_safepoint(), "must be at safepoint");
@@ -442,7 +455,7 @@ void LoaderConstraintTable::verify(PlaceholderTable* placeholders) {
         if (k != NULL) {
           // We found the class in the dictionary, so we should
           // make sure that the Klass* matches what we already have.
-          guarantee(k == probe->klass(), "klass should be in dictionary");
+          guarantee(k == probe->klass()->newest_version(), "klass should be in dictionary");
         } else {
           // If we don't find the class in the dictionary, it
           // has to be in the placeholders table.
diff --git a/src/hotspot/share/classfile/loaderConstraints.hpp b/src/hotspot/share/classfile/loaderConstraints.hpp
index 8a7a1248e62..611e18aaf85 100644
--- a/src/hotspot/share/classfile/loaderConstraints.hpp
+++ b/src/hotspot/share/classfile/loaderConstraints.hpp
@@ -55,6 +55,9 @@ public:
     return (LoaderConstraintEntry**)Hashtable<InstanceKlass*, mtClass>::bucket_addr(i);
   }
 
+  // (DCEVM) update all klasses with newest version
+  void update_after_redefinition();
+
   // Check class loader constraints
   bool add_entry(Symbol* name, InstanceKlass* klass1, Handle loader1,
                                     InstanceKlass* klass2, Handle loader2);
diff --git a/src/hotspot/share/classfile/systemDictionary.cpp b/src/hotspot/share/classfile/systemDictionary.cpp
index 91dc9faeccb..9dbd6cc9c12 100644
--- a/src/hotspot/share/classfile/systemDictionary.cpp
+++ b/src/hotspot/share/classfile/systemDictionary.cpp
@@ -201,6 +201,7 @@ Klass* SystemDictionary::resolve_or_fail(Symbol* class_name, Handle class_loader
     // can return a null klass
     klass = handle_resolution_exception(class_name, throw_error, klass, THREAD);
   }
+  assert(klass == NULL || klass->new_version() == NULL || klass->newest_version()->is_redefining(), "must be");
   return klass;
 }
 
@@ -875,6 +876,7 @@ Klass* SystemDictionary::resolve_instance_class_or_null(Symbol* name,
     ClassLoaderData* loader_data = k->class_loader_data();
     MutexLocker mu(SystemDictionary_lock, THREAD);
     Klass* kk = find_class(name, loader_data);
+    // FIXME: (kk == k() && !k->is_redefining()) || (k->is_redefining() && kk == k->old_version())
     assert(kk == k, "should be present in dictionary");
   }
 #endif
@@ -997,6 +999,7 @@ InstanceKlass* SystemDictionary::parse_stream(Symbol* class_name,
                                                       protection_domain,
                                                       host_klass,
                                                       cp_patches,
+                                                      false, // pick_newest
                                                       CHECK_NULL);
 
   if (host_klass != NULL && k != NULL) {
@@ -1050,10 +1053,13 @@ InstanceKlass* SystemDictionary::resolve_from_stream(Symbol* class_name,
                                                      Handle class_loader,
                                                      Handle protection_domain,
                                                      ClassFileStream* st,
+                                                     InstanceKlass* old_klass,
                                                      TRAPS) {
 
   HandleMark hm(THREAD);
 
+  bool is_redefining = (old_klass != NULL);
+
   // Classloaders that support parallelism, e.g. bootstrap classloader,
   // do not acquire lock here
   bool DoObjectLock = true;
@@ -1077,6 +1083,7 @@ InstanceKlass* SystemDictionary::resolve_from_stream(Symbol* class_name,
  InstanceKlass* k = NULL;
 
 #if INCLUDE_CDS
+  // FIXME: what to do during redefinition?
   if (!DumpSharedSpaces) {
     k = SystemDictionaryShared::lookup_from_stream(class_name,
                                                    class_loader,
@@ -1096,9 +1103,15 @@ InstanceKlass* SystemDictionary::resolve_from_stream(Symbol* class_name,
                                          protection_domain,
                                          NULL, // host_klass
                                          NULL, // cp_patches
+                                         is_redefining, // pick_newest
                                          CHECK_NULL);
   }
 
+  if (is_redefining && k != NULL) {
+    k->set_redefining(true);
+    k->set_old_version(old_klass);
+  }
+
   assert(k != NULL, "no klass created");
   Symbol* h_name = k->name();
   assert(class_name == NULL || class_name == h_name, "name mismatch");
@@ -1106,7 +1119,7 @@ InstanceKlass* SystemDictionary::resolve_from_stream(Symbol* class_name,
   // Add class just loaded
   // If a class loader supports parallel classloading handle parallel define requests
   // find_or_define_instance_class may return a different InstanceKlass
-  if (is_parallelCapable(class_loader)) {
+  if (is_parallelCapable(class_loader) && !is_redefining) {
     InstanceKlass* defined_k = find_or_define_instance_class(h_name, class_loader, k, THREAD);
     if (!HAS_PENDING_EXCEPTION && defined_k != k) {
       // If a parallel capable class loader already defined this class, register 'k' for cleanup.
@@ -1115,7 +1128,7 @@ InstanceKlass* SystemDictionary::resolve_from_stream(Symbol* class_name,
       k = defined_k;
     }
   } else {
-    define_instance_class(k, THREAD);
+    define_instance_class(k, old_klass, THREAD);
   }
 
   // If defining the class throws an exception register 'k' for cleanup.
@@ -1130,7 +1143,7 @@ InstanceKlass* SystemDictionary::resolve_from_stream(Symbol* class_name,
     MutexLocker mu(SystemDictionary_lock, THREAD);
 
     Klass* check = find_class(h_name, k->class_loader_data());
-    assert(check == k, "should be present in the dictionary");
+    assert((check == k && !k->is_redefining()) || (k->is_redefining() && check == k->old_version()), "should be present in the dictionary");
   } );
 
   return k;
@@ -1554,11 +1567,12 @@ static void post_class_define_event(InstanceKlass* k, const ClassLoaderData* def
   }
 }
 
-void SystemDictionary::define_instance_class(InstanceKlass* k, TRAPS) {
+void SystemDictionary::define_instance_class(InstanceKlass* k, InstanceKlass* old_klass, TRAPS) {
 
   HandleMark hm(THREAD);
   ClassLoaderData* loader_data = k->class_loader_data();
   Handle class_loader_h(THREAD, loader_data->class_loader());
+  bool is_redefining = (old_klass != NULL);
 
  // for bootstrap and other parallel classloaders don't acquire lock,
  // use placeholder token
@@ -1583,7 +1597,11 @@ void SystemDictionary::define_instance_class(InstanceKlass* k, TRAPS) {
   Symbol*  name_h = k->name();
   Dictionary* dictionary = loader_data->dictionary();
   unsigned int d_hash = dictionary->compute_hash(name_h);
-  check_constraints(d_hash, k, class_loader_h, true, CHECK);
+  if (is_redefining) {
+    bool ok = dictionary->update_klass(d_hash, name_h, loader_data, k, old_klass);
+    assert (ok, "must have found old class and updated!");
+  }
+  check_constraints(d_hash, k, class_loader_h, !is_redefining, CHECK);
 
   // Register class just loaded with class loader (placed in Vector)
   // Note we do this before updating the dictionary, as this can
@@ -1617,7 +1635,7 @@ void SystemDictionary::define_instance_class(InstanceKlass* k, TRAPS) {
   k->eager_initialize(THREAD);
 
   // notify jvmti
-  if (JvmtiExport::should_post_class_load()) {
+  if (!is_redefining && JvmtiExport::should_post_class_load()) {
       assert(THREAD->is_Java_thread(), "thread->is_Java_thread()");
       JvmtiExport::post_class_load((JavaThread *) THREAD, k);
 
@@ -1695,7 +1713,7 @@ InstanceKlass* SystemDictionary::find_or_define_instance_class(Symbol* class_nam
     }
   }
 
-  define_instance_class(k, THREAD);
+  define_instance_class(k, NULL, THREAD);
 
   Handle linkage_exception = Handle(); // null handle
 
@@ -1818,6 +1836,18 @@ void SystemDictionary::add_to_hierarchy(InstanceKlass* k, TRAPS) {
   CodeCache::flush_dependents_on(k);
 }
 
+// Enhanced class redefinition
+void SystemDictionary::remove_from_hierarchy(InstanceKlass* k) {
+    assert(k != NULL, "just checking");
+
+  // remove receiver from sibling list
+  k->remove_from_sibling_list();
+}
+
+void SystemDictionary::update_constraints_after_redefinition() {
+  constraints()->update_after_redefinition();
+}
+
 // ----------------------------------------------------------------------------
 // GC support
 
@@ -2089,7 +2119,7 @@ void SystemDictionary::check_constraints(unsigned int d_hash,
       // also hold array classes.
 
       assert(check->is_instance_klass(), "noninstance in systemdictionary");
-      if ((defining == true) || (k != check)) {
+      if ((defining == true) || ((k != check) && k->old_version() != check)) {
         throwException = true;
         ss.print("loader %s", loader_data->loader_name_and_id());
         ss.print(" attempted duplicate %s definition for %s. (%s)",
diff --git a/src/hotspot/share/classfile/systemDictionary.hpp b/src/hotspot/share/classfile/systemDictionary.hpp
index d38dd0fd003..717f34ce9a0 100644
--- a/src/hotspot/share/classfile/systemDictionary.hpp
+++ b/src/hotspot/share/classfile/systemDictionary.hpp
@@ -158,6 +158,8 @@ class OopStorage;
                                                                                                                          \
   /* support for dynamic typing; it's OK if these are NULL in earlier JDKs */                                            \
   do_klass(DirectMethodHandle_klass,                    java_lang_invoke_DirectMethodHandle,       Opt                 ) \
+  do_klass(DirectMethodHandle_StaticAccessor_klass,     java_lang_invoke_DirectMethodHandle_StaticAccessor, Opt        ) \
+  do_klass(DirectMethodHandle_Accessor_klass,           java_lang_invoke_DirectMethodHandle_Accessor, Opt              ) \
   do_klass(MethodHandle_klass,                          java_lang_invoke_MethodHandle,             Pre                 ) \
   do_klass(VarHandle_klass,                             java_lang_invoke_VarHandle,                Pre                 ) \
   do_klass(MemberName_klass,                            java_lang_invoke_MemberName,               Pre                 ) \
@@ -315,6 +317,7 @@ public:
                                             Handle class_loader,
                                             Handle protection_domain,
                                             ClassFileStream* st,
+                                            InstanceKlass* old_klass,
                                             TRAPS);
 
   // Lookup an already loaded class. If not found NULL is returned.
@@ -452,6 +455,10 @@ public:
   }
   static BasicType box_klass_type(Klass* k);  // inverse of box_klass
 
+  // Enhanced class redefinition
+  static void remove_from_hierarchy(InstanceKlass* k);
+  static void update_constraints_after_redefinition();
+
 protected:
   // Returns the class loader data to be used when looking up/updating the
   // system dictionary.
@@ -645,7 +652,7 @@ protected:
   // after waiting, but before reentering SystemDictionary_lock
   // to preserve lock order semantics.
   static void double_lock_wait(Handle lockObject, TRAPS);
-  static void define_instance_class(InstanceKlass* k, TRAPS);
+  static void define_instance_class(InstanceKlass* k, InstanceKlass* old_klass, TRAPS);
   static InstanceKlass* find_or_define_instance_class(Symbol* class_name,
                                                 Handle class_loader,
                                                 InstanceKlass* k, TRAPS);
diff --git a/src/hotspot/share/classfile/systemDictionaryShared.cpp b/src/hotspot/share/classfile/systemDictionaryShared.cpp
index ed75ecd9261..1f1441f3ffb 100644
--- a/src/hotspot/share/classfile/systemDictionaryShared.cpp
+++ b/src/hotspot/share/classfile/systemDictionaryShared.cpp
@@ -520,7 +520,7 @@ InstanceKlass* SystemDictionaryShared::find_or_load_shared_class(
 
       k = load_shared_class_for_builtin_loader(name, class_loader, THREAD);
       if (k != NULL) {
-        define_instance_class(k, CHECK_NULL);
+        define_instance_class(k, NULL, CHECK_NULL);
       }
     }
   }
diff --git a/src/hotspot/share/classfile/verifier.cpp b/src/hotspot/share/classfile/verifier.cpp
index ea4f74fc783..472216b6a40 100644
--- a/src/hotspot/share/classfile/verifier.cpp
+++ b/src/hotspot/share/classfile/verifier.cpp
@@ -237,7 +237,7 @@ bool Verifier::is_eligible_for_verification(InstanceKlass* klass, bool should_ve
   Symbol* name = klass->name();
   Klass* refl_magic_klass = SystemDictionary::reflect_MagicAccessorImpl_klass();
 
-  bool is_reflect = refl_magic_klass != NULL && klass->is_subtype_of(refl_magic_klass);
+  bool is_reflect = refl_magic_klass != NULL && (klass->is_subtype_of(refl_magic_klass) || klass->is_subtype_of(refl_magic_klass->newest_version()));
 
   return (should_verify_for(klass->class_loader(), should_verify_class) &&
     // return if the class is a bootstrapping class
diff --git a/src/hotspot/share/classfile/verifier.hpp b/src/hotspot/share/classfile/verifier.hpp
index 05239c57866..c1357bde0ed 100644
--- a/src/hotspot/share/classfile/verifier.hpp
+++ b/src/hotspot/share/classfile/verifier.hpp
@@ -348,6 +348,7 @@ class ClassVerifier : public StackObj {
 
   VerificationType object_type() const;
 
+  InstanceKlass*      _klass_to_verify;
   InstanceKlass*      _klass;  // the class being verified
   methodHandle        _method; // current method being verified
   VerificationType    _this_type; // the verification type of the current class
diff --git a/src/hotspot/share/classfile/vmSymbols.hpp b/src/hotspot/share/classfile/vmSymbols.hpp
index 73645b42857..e2bac31b27f 100644
--- a/src/hotspot/share/classfile/vmSymbols.hpp
+++ b/src/hotspot/share/classfile/vmSymbols.hpp
@@ -284,6 +284,8 @@
   template(java_lang_invoke_CallSite,                 "java/lang/invoke/CallSite")                \
   template(java_lang_invoke_ConstantCallSite,         "java/lang/invoke/ConstantCallSite")        \
   template(java_lang_invoke_DirectMethodHandle,       "java/lang/invoke/DirectMethodHandle")      \
+  template(java_lang_invoke_DirectMethodHandle_StaticAccessor, "java/lang/invoke/DirectMethodHandle$StaticAccessor") \
+  template(java_lang_invoke_DirectMethodHandle_Accessor, "java/lang/invoke/DirectMethodHandle$Accessor") \
   template(java_lang_invoke_MutableCallSite,          "java/lang/invoke/MutableCallSite")         \
   template(java_lang_invoke_VolatileCallSite,         "java/lang/invoke/VolatileCallSite")        \
   template(java_lang_invoke_MethodHandle,             "java/lang/invoke/MethodHandle")            \
@@ -444,6 +446,12 @@
   template(module_entry_name,                         "module_entry")                             \
   template(resolved_references_name,                  "<resolved_references>")                    \
   template(init_lock_name,                            "<init_lock>")                              \
+  template(static_offset_name,                        "staticOffset")                             \
+  template(static_base_name,                          "staticBase")                               \
+  template(field_offset_name,                         "fieldOffset")                              \
+  template(field_type_name,                           "fieldType")                                \
+                                                                                                  \
+   /* name symbols needed by intrinsics */                                                         \
                                                                                                   \
   /* name symbols needed by intrinsics */                                                         \
   VM_INTRINSICS_DO(VM_INTRINSIC_IGNORE, VM_SYMBOL_IGNORE, template, VM_SYMBOL_IGNORE, VM_ALIAS_IGNORE) \
diff --git a/src/hotspot/share/gc/cms/compactibleFreeListSpace.cpp b/src/hotspot/share/gc/cms/compactibleFreeListSpace.cpp
index 7b9096d73be..a93f764f1b9 100644
--- a/src/hotspot/share/gc/cms/compactibleFreeListSpace.cpp
+++ b/src/hotspot/share/gc/cms/compactibleFreeListSpace.cpp
@@ -376,6 +376,12 @@ CompactibleFreeListSpace::CompactibleFreeListSpace(BlockOffsetSharedArray* bs, M
   _used_stable = 0;
 }
 
+HeapWord* CompactibleFreeListSpace::forward_compact_top(size_t size,
+                                    CompactPoint* cp, HeapWord* compact_top) {
+  ShouldNotReachHere();
+  return NULL;
+}
+
 // Like CompactibleSpace forward() but always calls cross_threshold() to
 // update the block offset table.  Removed initialize_threshold call because
 // CFLS does not use a block offset array for contiguous spaces.
@@ -2190,7 +2196,8 @@ CompactibleFreeListSpace::refillLinearAllocBlock(LinearAllocBlock* blk) {
 
 // Support for compaction
 void CompactibleFreeListSpace::prepare_for_compaction(CompactPoint* cp) {
-  scan_and_forward(this, cp);
+  scan_and_forward(this, cp, false);
+   // of the free lists doesn't work after.
   // Prepare_for_compaction() uses the space between live objects
   // so that later phase can skip dead space quickly.  So verification
   // of the free lists doesn't work after.
@@ -2208,7 +2215,7 @@ void CompactibleFreeListSpace::adjust_pointers() {
 }
 
 void CompactibleFreeListSpace::compact() {
-  scan_and_compact(this);
+  scan_and_compact(this, false);
 }
 
 // Fragmentation metric = 1 - [sum of (fbs**2) / (sum of fbs)**2]
diff --git a/src/hotspot/share/gc/cms/compactibleFreeListSpace.hpp b/src/hotspot/share/gc/cms/compactibleFreeListSpace.hpp
index d034a960390..9fd2ea58320 100644
--- a/src/hotspot/share/gc/cms/compactibleFreeListSpace.hpp
+++ b/src/hotspot/share/gc/cms/compactibleFreeListSpace.hpp
@@ -112,11 +112,11 @@ class CompactibleFreeListSpace: public CompactibleSpace {
   template <typename SpaceType>
   friend void CompactibleSpace::scan_and_adjust_pointers(SpaceType* space);
   template <typename SpaceType>
-  friend void CompactibleSpace::scan_and_compact(SpaceType* space);
+  friend void CompactibleSpace::scan_and_compact(SpaceType* space, bool redefinition_run);
   template <typename SpaceType>
   friend void CompactibleSpace::verify_up_to_first_dead(SpaceType* space);
   template <typename SpaceType>
-  friend void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp);
+  friend void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp, bool redefinition_run);
 
   // "Size" of chunks of work (executed during parallel remark phases
   // of CMS collection); this probably belongs in CMSCollector, although
@@ -200,6 +200,7 @@ class CompactibleFreeListSpace: public CompactibleSpace {
 
   // Support for compacting cms
   HeapWord* cross_threshold(HeapWord* start, HeapWord* end);
+  HeapWord* forward_compact_top(size_t size, CompactPoint* cp, HeapWord* compact_top);
   HeapWord* forward(oop q, size_t size, CompactPoint* cp, HeapWord* compact_top);
 
   // Initialization helpers.
diff --git a/src/hotspot/share/gc/g1/heapRegion.hpp b/src/hotspot/share/gc/g1/heapRegion.hpp
index 01d3c4d8758..4486f429163 100644
--- a/src/hotspot/share/gc/g1/heapRegion.hpp
+++ b/src/hotspot/share/gc/g1/heapRegion.hpp
@@ -192,7 +192,7 @@ class HeapRegion: public G1ContiguousSpace {
   friend class VMStructs;
   // Allow scan_and_forward to call (private) overrides for auxiliary functions on this class
   template <typename SpaceType>
-  friend void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp);
+  friend void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp, bool redefinition_run);
  private:
 
   // The remembered set for this region.
diff --git a/src/hotspot/share/gc/serial/genMarkSweep.cpp b/src/hotspot/share/gc/serial/genMarkSweep.cpp
index b05c6602cba..b205fdc46e9 100644
--- a/src/hotspot/share/gc/serial/genMarkSweep.cpp
+++ b/src/hotspot/share/gc/serial/genMarkSweep.cpp
@@ -334,10 +334,14 @@ void GenMarkSweep::mark_sweep_phase4() {
   // in the same order in phase2, phase3 and phase4. We don't quite do that
   // here (perm_gen first rather than last), so we tell the validate code
   // to use a higher index (saved from phase2) when verifying perm_gen.
+  assert(_rescued_oops == NULL, "must be empty before processing");
   GenCollectedHeap* gch = GenCollectedHeap::heap();
 
   GCTraceTime(Info, gc, phases) tm("Phase 4: Move objects", _gc_timer);
 
+//  MarkSweep::copy_rescued_objects_back();
+
   GenCompactClosure blk;
   gch->generation_iterate(&blk, true);
+  MarkSweep::copy_rescued_objects_back();
 }
diff --git a/src/hotspot/share/gc/serial/markSweep.cpp b/src/hotspot/share/gc/serial/markSweep.cpp
index fcc4f40ec3b..d0ff86c8215 100644
--- a/src/hotspot/share/gc/serial/markSweep.cpp
+++ b/src/hotspot/share/gc/serial/markSweep.cpp
@@ -56,6 +56,8 @@ ReferenceProcessor*     MarkSweep::_ref_processor   = NULL;
 STWGCTimer*             MarkSweep::_gc_timer        = NULL;
 SerialOldTracer*        MarkSweep::_gc_tracer       = NULL;
 
+GrowableArray<HeapWord*>*   MarkSweep::_rescued_oops       = NULL;
+
 MarkSweep::FollowRootClosure  MarkSweep::follow_root_closure;
 
 MarkAndPushClosure            MarkSweep::mark_and_push_closure;
@@ -221,3 +223,100 @@ void MarkSweep::initialize() {
   MarkSweep::_gc_timer = new (ResourceObj::C_HEAP, mtGC) STWGCTimer();
   MarkSweep::_gc_tracer = new (ResourceObj::C_HEAP, mtGC) SerialOldTracer();
 }
+
+// (DCEVM) Copy the rescued objects to their destination address after compaction.
+void MarkSweep::copy_rescued_objects_back() {
+
+  if (_rescued_oops != NULL) {
+
+    for (int i=0; i<_rescued_oops->length(); i++) {
+      HeapWord* rescued_ptr = _rescued_oops->at(i);
+      oop rescued_obj = (oop) rescued_ptr;
+
+      int size = rescued_obj->size();
+      oop new_obj = rescued_obj->forwardee();
+
+      assert(rescued_obj->klass()->new_version() != NULL, "just checking");
+
+      if (rescued_obj->klass()->new_version()->update_information() != NULL) {
+        MarkSweep::update_fields(rescued_obj, new_obj);
+      } else {
+        rescued_obj->set_klass(rescued_obj->klass()->new_version());
+        Copy::aligned_disjoint_words((HeapWord*)rescued_obj, (HeapWord*)new_obj, size);
+      }
+
+      FREE_RESOURCE_ARRAY(HeapWord, rescued_ptr, size);
+
+      new_obj->init_mark();
+      assert(oopDesc::is_oop(new_obj), "must be a valid oop");
+    }
+    _rescued_oops->clear();
+    _rescued_oops = NULL;
+  }
+}
+
+// (DCEVM) Update instances of a class whose fields changed.
+void MarkSweep::update_fields(oop q, oop new_location) {
+
+  assert(q->klass()->new_version() != NULL, "class of old object must have new version");
+
+  Klass* old_klass_oop = q->klass();
+  Klass* new_klass_oop = q->klass()->new_version();
+
+  InstanceKlass *old_klass = InstanceKlass::cast(old_klass_oop);
+  InstanceKlass *new_klass = InstanceKlass::cast(new_klass_oop);
+
+  int size = q->size_given_klass(old_klass);
+  int new_size = q->size_given_klass(new_klass);
+
+  HeapWord* tmp = NULL;
+  oop tmp_obj = q;
+
+  // Save object somewhere, there is an overlap in fields
+  if (new_klass_oop->is_copying_backwards()) {
+    if (((HeapWord *)q >= (HeapWord *)new_location && (HeapWord *)q < (HeapWord *)new_location + new_size) ||
+        ((HeapWord *)new_location >= (HeapWord *)q && (HeapWord *)new_location < (HeapWord *)q + size)) {
+       tmp = NEW_RESOURCE_ARRAY(HeapWord, size);
+       q = (oop) tmp;
+       Copy::aligned_disjoint_words((HeapWord*)tmp_obj, (HeapWord*)q, size);
+    }
+  }
+
+  q->set_klass(new_klass_oop);
+  int *cur = new_klass_oop->update_information();
+  assert(cur != NULL, "just checking");
+  MarkSweep::update_fields(new_location, q, cur);
+
+  if (tmp != NULL) {
+    FREE_RESOURCE_ARRAY(HeapWord, tmp, size);
+  }
+}
+
+void MarkSweep::update_fields(oop new_location, oop tmp_obj, int *cur) {
+  assert(cur != NULL, "just checking");
+  char* to = (char*)(HeapWord*)new_location;
+  while (*cur != 0) {
+    int size = *cur;
+    if (size > 0) {
+      cur++;
+      int offset = *cur;
+      HeapWord* from = (HeapWord*)(((char *)(HeapWord*)tmp_obj) + offset);
+      if (size == HeapWordSize) {
+        *((HeapWord*)to) = *from;
+      } else if (size == HeapWordSize * 2) {
+        *((HeapWord*)to) = *from;
+        *(((HeapWord*)to) + 1) = *(from + 1);
+      } else {
+        Copy::conjoint_jbytes(from, to, size);
+      }
+      to += size;
+      cur++;
+    } else {
+      assert(size < 0, "");
+      int skip = -*cur;
+      Copy::fill_to_bytes(to, skip, 0);
+      to += skip;
+      cur++;
+    }
+  }
+}
diff --git a/src/hotspot/share/gc/serial/markSweep.hpp b/src/hotspot/share/gc/serial/markSweep.hpp
index cf96ade0ef0..b6187dae4d0 100644
--- a/src/hotspot/share/gc/serial/markSweep.hpp
+++ b/src/hotspot/share/gc/serial/markSweep.hpp
@@ -87,6 +87,10 @@ class MarkSweep : AllStatic {
   friend class AdjustPointerClosure;
   friend class KeepAliveClosure;
   friend class VM_MarkSweep;
+  friend class GenMarkSweep;
+
+ public:
+  static GrowableArray<HeapWord*>*             _rescued_oops;
 
   //
   // Vars
@@ -144,6 +148,9 @@ class MarkSweep : AllStatic {
 
   static int adjust_pointers(oop obj);
 
+  static void copy_rescued_objects_back();
+  static void update_fields(oop q, oop new_location);
+  static void update_fields(oop new_location, oop tmp_obj, int *cur);
   static void follow_stack();   // Empty marking stack.
 
   static void follow_klass(Klass* klass);
diff --git a/src/hotspot/share/gc/shared/gcConfig.cpp b/src/hotspot/share/gc/shared/gcConfig.cpp
index ddc84c1d2a4..8aac2b39e1e 100644
--- a/src/hotspot/share/gc/shared/gcConfig.cpp
+++ b/src/hotspot/share/gc/shared/gcConfig.cpp
@@ -101,7 +101,10 @@ void GCConfig::fail_if_unsupported_gc_is_selected() {
 }
 
 void GCConfig::select_gc_ergonomically() {
-  if (os::is_server_class_machine()) {
+  if (AllowEnhancedClassRedefinition) {
+    // Enhanced class redefinition only supports serial GC at the moment
+    FLAG_SET_ERGO(bool, UseSerialGC, true);
+  } else if (os::is_server_class_machine()) {
 #if INCLUDE_G1GC
     FLAG_SET_ERGO_IF_DEFAULT(bool, UseG1GC, true);
 #elif INCLUDE_PARALLELGC
diff --git a/src/hotspot/share/gc/shared/space.cpp b/src/hotspot/share/gc/shared/space.cpp
index 5b6b2b1c798..763abc91a39 100644
--- a/src/hotspot/share/gc/shared/space.cpp
+++ b/src/hotspot/share/gc/shared/space.cpp
@@ -363,9 +363,8 @@ void CompactibleSpace::clear(bool mangle_space) {
   _compaction_top = bottom();
 }
 
-HeapWord* CompactibleSpace::forward(oop q, size_t size,
-                                    CompactPoint* cp, HeapWord* compact_top) {
-  // q is alive
+// (DCEVM) Calculates the compact_top that will be used for placing the next object with the giving size on the heap.
+HeapWord* CompactibleSpace::forward_compact_top(size_t size, CompactPoint* cp, HeapWord* compact_top) {
   // First check if we should switch compaction space
   assert(this == cp->space, "'this' should be current compaction space.");
   size_t compaction_max_size = pointer_delta(end(), compact_top);
@@ -385,8 +384,15 @@ HeapWord* CompactibleSpace::forward(oop q, size_t size,
     compaction_max_size = pointer_delta(cp->space->end(), compact_top);
   }
 
+  return compact_top;
+}
+
+HeapWord* CompactibleSpace::forward(oop q, size_t size,
+                                    CompactPoint* cp, HeapWord* compact_top) {
+  compact_top = forward_compact_top(size, cp, compact_top);
+
   // store the forwarding pointer into the mark word
-  if ((HeapWord*)q != compact_top) {
+  if ((HeapWord*)q != compact_top || (size_t)q->size() != size) {
     q->forward_to(oop(compact_top));
     assert(q->is_gc_marked(), "encoding the pointer should preserve the mark");
   } else {
@@ -410,7 +416,132 @@ HeapWord* CompactibleSpace::forward(oop q, size_t size,
 #if INCLUDE_SERIALGC
 
 void ContiguousSpace::prepare_for_compaction(CompactPoint* cp) {
-  scan_and_forward(this, cp);
+  if (!Universe::is_redefining_gc_run()) {
+    scan_and_forward(this, cp, false);
+  } else {
+    // Redefinition run
+    scan_and_forward(this, cp, true);
+  }
+}
+
+
+#ifdef ASSERT
+
+int CompactibleSpace::space_index(oop obj) {
+  GenCollectedHeap* heap = GenCollectedHeap::heap();
+
+  //if (heap->is_in_permanent(obj)) {
+  //  return -1;
+  //}
+
+  int index = 0;
+  CompactibleSpace* space = heap->old_gen()->first_compaction_space();
+  while (space != NULL) {
+    if (space->is_in_reserved(obj)) {
+      return index;
+    }
+    space = space->next_compaction_space();
+    index++;
+  }
+
+  space = heap->young_gen()->first_compaction_space();
+  while (space != NULL) {
+    if (space->is_in_reserved(obj)) {
+      return index;
+    }
+    space = space->next_compaction_space();
+    index++;
+  }
+
+  tty->print_cr("could not compute space_index for %08xh", (HeapWord*)obj);
+  index = 0;
+
+  Generation* gen = heap->old_gen();
+  tty->print_cr("  generation %s: %08xh - %08xh", gen->name(), gen->reserved().start(), gen->reserved().end());
+
+  space = gen->first_compaction_space();
+  while (space != NULL) {
+    tty->print_cr("    %2d space %08xh - %08xh", index, space->bottom(), space->end());
+    space = space->next_compaction_space();
+    index++;
+  }
+
+  gen = heap->young_gen();
+  tty->print_cr("  generation %s: %08xh - %08xh", gen->name(), gen->reserved().start(), gen->reserved().end());
+
+  space = gen->first_compaction_space();
+  while (space != NULL) {
+    tty->print_cr("    %2d space %08xh - %08xh", index, space->bottom(), space->end());
+    space = space->next_compaction_space();
+    index++;
+  }
+
+  ShouldNotReachHere();
+  return 0;
+}
+#endif
+
+bool CompactibleSpace::must_rescue(oop old_obj, oop new_obj) {
+  // Only redefined objects can have the need to be rescued.
+  if (oop(old_obj)->klass()->new_version() == NULL) return false;
+
+  //if (old_obj->is_perm()) {
+  //  // This object is in perm gen: Always rescue to satisfy invariant obj->klass() <= obj.
+  //  return true;
+  //}
+
+  int new_size = old_obj->size_given_klass(oop(old_obj)->klass()->new_version());
+  int original_size = old_obj->size();
+  
+  Generation* tenured_gen = GenCollectedHeap::heap()->old_gen();
+  bool old_in_tenured = tenured_gen->is_in_reserved(old_obj);
+  bool new_in_tenured = tenured_gen->is_in_reserved(new_obj);
+  if (old_in_tenured == new_in_tenured) {
+    // Rescue if object may overlap with a higher memory address.
+    bool overlap = ((HeapWord*)old_obj + original_size < (HeapWord*)new_obj + new_size);
+    if (old_in_tenured) {
+      // Old and new address are in same space, so just compare the address.
+      // Must rescue if object moves towards the top of the space.
+      assert(space_index(old_obj) == space_index(new_obj), "old_obj and new_obj must be in same space");
+    } else {
+      // In the new generation, eden is located before the from space, so a
+      // simple pointer comparison is sufficient.
+      assert(GenCollectedHeap::heap()->young_gen()->is_in_reserved(old_obj), "old_obj must be in DefNewGeneration");
+      assert(GenCollectedHeap::heap()->young_gen()->is_in_reserved(new_obj), "new_obj must be in DefNewGeneration");
+      assert(overlap == (space_index(old_obj) < space_index(new_obj)), "slow and fast computation must yield same result");
+    }
+    return overlap;
+
+  } else {
+    assert(space_index(old_obj) != space_index(new_obj), "old_obj and new_obj must be in different spaces");
+    if (tenured_gen->is_in_reserved(new_obj)) {
+      // Must never rescue when moving from the new into the old generation.
+      assert(GenCollectedHeap::heap()->young_gen()->is_in_reserved(old_obj), "old_obj must be in DefNewGeneration");
+      assert(space_index(old_obj) > space_index(new_obj), "must be");
+      return false;
+
+    } else /* if (tenured_gen->is_in_reserved(old_obj)) */ {
+      // Must always rescue when moving from the old into the new generation.
+      assert(GenCollectedHeap::heap()->young_gen()->is_in_reserved(new_obj), "new_obj must be in DefNewGeneration");
+      assert(space_index(old_obj) < space_index(new_obj), "must be");
+      return true;
+    }
+  }
+}
+
+HeapWord* CompactibleSpace::rescue(HeapWord* old_obj) {
+  assert(must_rescue(oop(old_obj), oop(old_obj)->forwardee()), "do not call otherwise");
+
+  int size = oop(old_obj)->size();
+  HeapWord* rescued_obj = NEW_RESOURCE_ARRAY(HeapWord, size);
+  Copy::aligned_disjoint_words(old_obj, rescued_obj, size);
+
+  if (MarkSweep::_rescued_oops == NULL) {
+    MarkSweep::_rescued_oops = new GrowableArray<HeapWord*>(128);
+  }
+
+  MarkSweep::_rescued_oops->append(rescued_obj);
+  return rescued_obj;
 }
 
 void CompactibleSpace::adjust_pointers() {
@@ -423,7 +554,12 @@ void CompactibleSpace::adjust_pointers() {
 }
 
 void CompactibleSpace::compact() {
-  scan_and_compact(this);
+  if(!Universe::is_redefining_gc_run()) {
+    scan_and_compact(this, false);
+  } else {
+    // Redefinition run
+    scan_and_compact(this, true);
+  }
 }
 
 #endif // INCLUDE_SERIALGC
@@ -719,6 +855,58 @@ void OffsetTableContigSpace::verify() const {
   guarantee(p == top(), "end of last object must match end of space");
 }
 
+// Compute the forward sizes and leave out objects whose position could
+// possibly overlap other objects.
+HeapWord* CompactibleSpace::forward_with_rescue(HeapWord* q, size_t size,
+                                                CompactPoint* cp, HeapWord* compact_top) {
+  size_t forward_size = size;
+
+  // (DCEVM) There is a new version of the class of q => different size
+  if (oop(q)->klass()->new_version() != NULL && oop(q)->klass()->new_version()->update_information() != NULL) {
+
+    size_t new_size = oop(q)->size_given_klass(oop(q)->klass()->new_version());
+    assert(size != new_size, "instances without changed size have to be updated prior to GC run");
+    forward_size = new_size;
+  }
+
+  compact_top = forward_compact_top(forward_size, cp, compact_top);
+
+  if (must_rescue(oop(q), oop(compact_top))) {
+    if (MarkSweep::_rescued_oops == NULL) {
+      MarkSweep::_rescued_oops = new GrowableArray<HeapWord*>(128);
+    }
+    MarkSweep::_rescued_oops->append(q);
+    return compact_top;
+  }
+
+  return forward(oop(q), forward_size, cp, compact_top);
+}
+
+// Compute the forwarding addresses for the objects that need to be rescued.
+HeapWord* CompactibleSpace::forward_rescued(CompactPoint* cp, HeapWord* compact_top) {
+  // TODO: empty the _rescued_oops after ALL spaces are compacted!
+  if (MarkSweep::_rescued_oops != NULL) {
+    for (int i=0; i<MarkSweep::_rescued_oops->length(); i++) {
+      HeapWord* q = MarkSweep::_rescued_oops->at(i);
+
+      /* size_t size = oop(q)->size();  changing this for cms for perm gen */
+      size_t size = block_size(q);
+
+      // (DCEVM) There is a new version of the class of q => different size
+      if (oop(q)->klass()->new_version() != NULL) {
+        size_t new_size = oop(q)->size_given_klass(oop(q)->klass()->new_version());
+        assert(size != new_size, "instances without changed size have to be updated prior to GC run");
+        size = new_size;
+      }
+
+      compact_top = cp->space->forward(oop(q), size, cp, compact_top);
+      assert(compact_top <= end(), "must not write over end of space!");
+    }
+    MarkSweep::_rescued_oops->clear();
+    MarkSweep::_rescued_oops = NULL;
+  }
+  return compact_top;
+}
 
 size_t TenuredSpace::allowed_dead_ratio() const {
   return MarkSweepDeadRatio;
diff --git a/src/hotspot/share/gc/shared/space.hpp b/src/hotspot/share/gc/shared/space.hpp
index 161820ad056..8eb5669fb79 100644
--- a/src/hotspot/share/gc/shared/space.hpp
+++ b/src/hotspot/share/gc/shared/space.hpp
@@ -420,6 +420,9 @@ public:
   // indicates when the next such action should be taken.
   virtual void prepare_for_compaction(CompactPoint* cp) = 0;
   // MarkSweep support phase3
+  DEBUG_ONLY(int space_index(oop obj));
+  bool must_rescue(oop old_obj, oop new_obj);
+  HeapWord* rescue(HeapWord* old_obj);
   virtual void adjust_pointers();
   // MarkSweep support phase4
   virtual void compact();
@@ -450,6 +453,15 @@ public:
   // accordingly".
   virtual HeapWord* forward(oop q, size_t size, CompactPoint* cp,
                     HeapWord* compact_top);
+  // (DCEVM) same as forwad, but can rescue objects. Invoked only during
+  // redefinition runs
+  HeapWord* forward_with_rescue(HeapWord* q, size_t size, CompactPoint* cp,
+                                HeapWord* compact_top);
+
+  HeapWord* forward_rescued(CompactPoint* cp, HeapWord* compact_top);
+
+  // (tw) Compute new compact top without actually forwarding the object.
+  virtual HeapWord* forward_compact_top(size_t size, CompactPoint* cp, HeapWord* compact_top);
 
   // Return a size with adjustments as required of the space.
   virtual size_t adjust_object_size_v(size_t size) const { return size; }
@@ -486,12 +498,12 @@ protected:
 
   // Frequently calls obj_size().
   template <class SpaceType>
-  static inline void scan_and_compact(SpaceType* space);
+  static inline void scan_and_compact(SpaceType* space, bool redefinition_run);
 
   // Frequently calls scanned_block_is_obj() and scanned_block_size().
   // Requires the scan_limit() function.
   template <class SpaceType>
-  static inline void scan_and_forward(SpaceType* space, CompactPoint* cp);
+  static inline void scan_and_forward(SpaceType* space, CompactPoint* cp, bool redefinition_run);
 };
 
 class GenSpaceMangler;
@@ -502,7 +514,7 @@ class ContiguousSpace: public CompactibleSpace {
   friend class VMStructs;
   // Allow scan_and_forward function to call (private) overrides for auxiliary functions on this class
   template <typename SpaceType>
-  friend void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp);
+  friend void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp, bool redefinition_run);
 
  private:
   // Auxiliary functions for scan_and_forward support.
diff --git a/src/hotspot/share/gc/shared/space.inline.hpp b/src/hotspot/share/gc/shared/space.inline.hpp
index ad22b6c4283..4394eff00c5 100644
--- a/src/hotspot/share/gc/shared/space.inline.hpp
+++ b/src/hotspot/share/gc/shared/space.inline.hpp
@@ -134,7 +134,7 @@ public:
 };
 
 template <class SpaceType>
-inline void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp) {
+inline void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* cp, bool redefinition_run) {
   // Compute the new addresses for the live objects and store it in the mark
   // Used by universe::mark_sweep_phase2()
 
@@ -172,7 +172,18 @@ inline void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* c
       // prefetch beyond cur_obj
       Prefetch::write(cur_obj, interval);
       size_t size = space->scanned_block_size(cur_obj);
-      compact_top = cp->space->forward(oop(cur_obj), size, cp, compact_top);
+
+      if (redefinition_run) {
+        compact_top = cp->space->forward_with_rescue(cur_obj, size, cp, compact_top);
+        if (first_dead == NULL && oop(cur_obj)->is_gc_marked()) {
+          /* Was moved (otherwise, forward would reset mark),
+             set first_dead to here */
+          first_dead = cur_obj;
+        }
+      } else {
+        compact_top = cp->space->forward(oop(cur_obj), size, cp, compact_top);
+      }
+
       cur_obj += size;
       end_of_live = cur_obj;
     } else {
@@ -207,6 +218,10 @@ inline void CompactibleSpace::scan_and_forward(SpaceType* space, CompactPoint* c
     }
   }
 
+  if (redefinition_run) {
+    compact_top = space->forward_rescued(cp, compact_top);
+  }
+
   assert(cur_obj == scan_limit, "just checking");
   space->_end_of_live = end_of_live;
   if (first_dead != NULL) {
@@ -293,7 +308,7 @@ inline void CompactibleSpace::clear_empty_region(SpaceType* space) {
 }
 
 template <class SpaceType>
-inline void CompactibleSpace::scan_and_compact(SpaceType* space) {
+inline void CompactibleSpace::scan_and_compact(SpaceType* space, bool redefinition_run) {
   // Copy all live objects to their new location
   // Used by MarkSweep::mark_sweep_phase4()
 
@@ -317,7 +332,7 @@ inline void CompactibleSpace::scan_and_compact(SpaceType* space) {
   if (space->_first_dead > cur_obj && !oop(cur_obj)->is_gc_marked()) {
     // All object before _first_dead can be skipped. They should not be moved.
     // A pointer to the first live object is stored at the memory location for _first_dead.
-    cur_obj = *(HeapWord**)(space->_first_dead);
+    cur_obj = space->_first_dead;
   }
 
   debug_only(HeapWord* prev_obj = NULL);
@@ -335,11 +350,35 @@ inline void CompactibleSpace::scan_and_compact(SpaceType* space) {
       size_t size = space->obj_size(cur_obj);
       HeapWord* compaction_top = (HeapWord*)oop(cur_obj)->forwardee();
 
+      if (redefinition_run &&  space->must_rescue(oop(cur_obj), oop(cur_obj)->forwardee())) {
+         space->rescue(cur_obj);
+        debug_only(Copy::fill_to_words(cur_obj, size, 0));
+        cur_obj += size;
+        continue;
+      }
+
       // prefetch beyond compaction_top
       Prefetch::write(compaction_top, copy_interval);
 
       // copy object and reinit its mark
-      assert(cur_obj != compaction_top, "everything in this pass should be moving");
+      assert(cur_obj != compaction_top || oop(cur_obj)->klass()->new_version() != NULL,
+             "everything in this pass should be moving");
+      if (redefinition_run && oop(cur_obj)->klass()->new_version() != NULL) {
+        Klass* new_version = oop(cur_obj)->klass()->new_version();
+        if (new_version->update_information() == NULL) {
+          Copy::aligned_conjoint_words(cur_obj, compaction_top, size);
+          oop(compaction_top)->set_klass(new_version);
+        } else {
+          MarkSweep::update_fields(oop(cur_obj), oop(compaction_top));
+        }
+        oop(compaction_top)->init_mark();
+        assert(oop(compaction_top)->klass() != NULL, "should have a class");
+
+        debug_only(prev_obj = cur_obj);
+        cur_obj += size;
+        continue;
+      }
+
       Copy::aligned_conjoint_words(cur_obj, compaction_top, size);
       oop(compaction_top)->init_mark_raw();
       assert(oop(compaction_top)->klass() != NULL, "should have a class");
diff --git a/src/hotspot/share/interpreter/linkResolver.cpp b/src/hotspot/share/interpreter/linkResolver.cpp
index 81998728f66..b9ccdee8cca 100644
--- a/src/hotspot/share/interpreter/linkResolver.cpp
+++ b/src/hotspot/share/interpreter/linkResolver.cpp
@@ -296,7 +296,7 @@ void LinkResolver::check_klass_accessability(Klass* ref_klass, Klass* sel_klass,
     }
   }
   Reflection::VerifyClassAccessResults vca_result =
-    Reflection::verify_class_access(ref_klass, InstanceKlass::cast(base_klass), true);
+    Reflection::verify_class_access(ref_klass->newest_version(), InstanceKlass::cast(base_klass->newest_version()), true);
   if (vca_result != Reflection::ACCESS_OK) {
     ResourceMark rm(THREAD);
     char* msg = Reflection::verify_class_access_msg(ref_klass,
@@ -572,7 +572,7 @@ void LinkResolver::check_method_accessability(Klass* ref_klass,
   // We'll check for the method name first, as that's most likely
   // to be false (so we'll short-circuit out of these tests).
   if (sel_method->name() == vmSymbols::clone_name() &&
-      sel_klass == SystemDictionary::Object_klass() &&
+      sel_klass->newest_version() == SystemDictionary::Object_klass()->newest_version() &&
       resolved_klass->is_array_klass()) {
     // We need to change "protected" to "public".
     assert(flags.is_protected(), "clone not protected?");
@@ -997,7 +997,7 @@ void LinkResolver::resolve_field(fieldDescriptor& fd,
       ResourceMark rm(THREAD);
       stringStream ss;
 
-      if (sel_klass != current_klass) {
+      if (sel_klass != current_klass && sel_klass != current_klass->active_version()) {
         ss.print("Update to %s final field %s.%s attempted from a different class (%s) than the field's declaring class",
                  is_static ? "static" : "non-static", resolved_klass->external_name(), fd.name()->as_C_string(),
                 current_klass->external_name());
@@ -1384,6 +1384,7 @@ void LinkResolver::runtime_resolve_virtual_method(CallInfo& result,
       assert(resolved_method->can_be_statically_bound(), "cannot override this method");
       selected_method = resolved_method;
     } else {
+      assert(recv_klass->is_subtype_of(resolved_method->method_holder()), "receiver and resolved method holder are inconsistent");
       selected_method = methodHandle(THREAD, recv_klass->method_at_vtable(vtable_index));
     }
   }
diff --git a/src/hotspot/share/jfr/instrumentation/jfrEventClassTransformer.cpp b/src/hotspot/share/jfr/instrumentation/jfrEventClassTransformer.cpp
index b532101c9b5..8afe8985026 100644
--- a/src/hotspot/share/jfr/instrumentation/jfrEventClassTransformer.cpp
+++ b/src/hotspot/share/jfr/instrumentation/jfrEventClassTransformer.cpp
@@ -1467,6 +1467,7 @@ static InstanceKlass* create_new_instance_klass(InstanceKlass* ik, ClassFileStre
                              NULL, // host klass
                              NULL, // cp_patches
                              ClassFileParser::INTERNAL, // internal visibility
+							 false,
                              THREAD);
   if (HAS_PENDING_EXCEPTION) {
     log_pending_exception(PENDING_EXCEPTION);
diff --git a/src/hotspot/share/memory/universe.cpp b/src/hotspot/share/memory/universe.cpp
index 4a1bd297b72..7ecb950b231 100644
--- a/src/hotspot/share/memory/universe.cpp
+++ b/src/hotspot/share/memory/universe.cpp
@@ -153,6 +153,7 @@ int             Universe::_base_vtable_size = 0;
 bool            Universe::_bootstrapping = false;
 bool            Universe::_module_initialized = false;
 bool            Universe::_fully_initialized = false;
+bool            Universe::_is_redefining_gc_run = false; // FIXME: review
 
 size_t          Universe::_heap_capacity_at_last_gc;
 size_t          Universe::_heap_used_at_last_gc = 0;
@@ -175,6 +176,44 @@ void Universe::basic_type_classes_do(void f(Klass*)) {
   f(doubleArrayKlassObj());
 }
 
+// FIXME: This method should iterate all pointers that are not within heap objects.
+void Universe::root_oops_do(OopClosure *oopClosure) {
+
+  class AlwaysTrueClosure: public BoolObjectClosure {
+  public:
+    void do_object(oop p) { ShouldNotReachHere(); }
+    bool do_object_b(oop p) { return true; }
+  };
+  AlwaysTrueClosure always_true;
+
+  Universe::oops_do(oopClosure);
+//  ReferenceProcessor::oops_do(oopClosure); (tw) check why no longer there
+  JNIHandles::oops_do(oopClosure);   // Global (strong) JNI handles
+  Threads::oops_do(oopClosure, NULL);
+  ObjectSynchronizer::oops_do(oopClosure);
+  // TODO: review, flat profiler was removed in j10
+  // FlatProfiler::oops_do(oopClosure);
+  JvmtiExport::oops_do(oopClosure);
+
+  // Now adjust pointers in remaining weak roots.  (All of which should
+  // have been cleared if they pointed to non-surviving objects.)
+  // Global (weak) JNI handles
+  JNIHandles::weak_oops_do(&always_true, oopClosure);
+
+  CodeBlobToOopClosure blobClosure(oopClosure, CodeBlobToOopClosure::FixRelocations);
+  CodeCache::blobs_do(&blobClosure);
+  StringTable::oops_do(oopClosure);
+  
+  // (DCEVM) TODO: Check if this is correct?
+  //CodeCache::scavenge_root_nmethods_oops_do(oopClosure);
+  //Management::oops_do(oopClosure);
+  //ref_processor()->weak_oops_do(&oopClosure);
+  //PSScavenge::reference_processor()->weak_oops_do(&oopClosure);
+
+  // SO_AllClasses
+  SystemDictionary::oops_do(oopClosure);
+}
+
 void Universe::oops_do(OopClosure* f, bool do_all) {
 
   f->do_oop((oop*) &_int_mirror);
diff --git a/src/hotspot/share/memory/universe.hpp b/src/hotspot/share/memory/universe.hpp
index 3ebd3654eb0..b32db16b9cf 100644
--- a/src/hotspot/share/memory/universe.hpp
+++ b/src/hotspot/share/memory/universe.hpp
@@ -52,7 +52,13 @@ class LatestMethodCache : public CHeapObj<mtClass> {
   Klass*                _klass;
   int                   _method_idnum;
 
+  static bool _is_redefining_gc_run;
+
  public:
+
+   static bool is_redefining_gc_run()               { return _is_redefining_gc_run; }
+   static void set_redefining_gc_run(bool b)        { _is_redefining_gc_run = b;    }
+
   LatestMethodCache()   { _klass = NULL; _method_idnum = -1; }
   ~LatestMethodCache()  { _klass = NULL; _method_idnum = -1; }
 
@@ -268,7 +274,12 @@ class Universe: AllStatic {
   static void calculate_verify_data(HeapWord* low_boundary, HeapWord* high_boundary) PRODUCT_RETURN;
   static void compute_verify_oop_data();
 
+  static bool _is_redefining_gc_run;
  public:
+  // Advanced class redefinition. FIXME: review?
+  static bool is_redefining_gc_run()               { return _is_redefining_gc_run; }
+  static void set_redefining_gc_run(bool b)        { _is_redefining_gc_run = b;    }
+
   // Known classes in the VM
   static Klass* boolArrayKlassObj()                 { return _boolArrayKlassObj;   }
   static Klass* byteArrayKlassObj()                 { return _byteArrayKlassObj;   }
@@ -476,6 +487,7 @@ class Universe: AllStatic {
 
   // Iteration
 
+  static void root_oops_do(OopClosure *oopClosure); // FIXME: kill...
   // Apply "f" to the addresses of all the direct heap pointers maintained
   // as static fields of "Universe".
   static void oops_do(OopClosure* f, bool do_all = false);
diff --git a/src/hotspot/share/oops/cpCache.cpp b/src/hotspot/share/oops/cpCache.cpp
index 42dbb4129ea..47040d51f0c 100644
--- a/src/hotspot/share/oops/cpCache.cpp
+++ b/src/hotspot/share/oops/cpCache.cpp
@@ -436,7 +436,8 @@ void ConstantPoolCacheEntry::set_method_handle_common(const constantPoolHandle&
   if (has_appendix) {
     const int appendix_index = f2_as_index() + _indy_resolved_references_appendix_offset;
     assert(appendix_index >= 0 && appendix_index < resolved_references->length(), "oob");
-    assert(resolved_references->obj_at(appendix_index) == NULL, "init just once");
+    // FIXME (DCEVM) relaxing for now...
+    //assert(resolved_references->obj_at(appendix_index) == NULL, "init just once");
     resolved_references->obj_at_put(appendix_index, appendix());
   }
 
@@ -444,7 +445,8 @@ void ConstantPoolCacheEntry::set_method_handle_common(const constantPoolHandle&
   if (has_method_type) {
     const int method_type_index = f2_as_index() + _indy_resolved_references_method_type_offset;
     assert(method_type_index >= 0 && method_type_index < resolved_references->length(), "oob");
-    assert(resolved_references->obj_at(method_type_index) == NULL, "init just once");
+    // FIXME (DCEVM) relaxing for now...
+    //assert(resolved_references->obj_at(method_type_index) == NULL, "init just once");
     resolved_references->obj_at_put(method_type_index, method_type());
   }
 
@@ -645,6 +647,35 @@ Method* ConstantPoolCacheEntry::get_interesting_method_entry(Klass* k) {
   // the method is in the interesting class so the entry is interesting
   return m;
 }
+
+// Enhanced RedefineClasses() API support (DCEVM):
+// Clear cached entry, let it be re-resolved
+void ConstantPoolCacheEntry::clear_entry() {
+  // Always clear for invokehandle/invokedynamic to re-resolve them
+  bool clearData = bytecode_1() == Bytecodes::_invokehandle || bytecode_1() == Bytecodes::_invokedynamic;
+  _indices = constant_pool_index();
+
+  if (clearData) {
+    if (!is_resolved_reference()) {
+      _f2 = 0;
+    }
+    // FIXME: (DCEVM) we want to clear flags, but parameter size is actually used
+    // after we return from the method, before entry is re-initialized. So let's
+    // keep parameter size the same.
+    // For example, it's used in TemplateInterpreterGenerator::generate_return_entry_for
+    // Also, we need to keep flag marking entry as one containing resolved_reference
+    _flags &= parameter_size_mask | (1 << is_resolved_ref_shift);
+    _f1 = NULL;
+  }
+}
+
+// Enhanced RedefineClasses() API support (DCEVM):
+// Clear all entries
+void ConstantPoolCache::clear_entries() {
+  for (int i = 0; i < length(); i++) {
+    entry_at(i)->clear_entry();
+  }
+}
 #endif // INCLUDE_JVMTI
 
 void ConstantPoolCacheEntry::print(outputStream* st, int index) const {
diff --git a/src/hotspot/share/oops/cpCache.hpp b/src/hotspot/share/oops/cpCache.hpp
index 2a93500b794..2a9eb978b43 100644
--- a/src/hotspot/share/oops/cpCache.hpp
+++ b/src/hotspot/share/oops/cpCache.hpp
@@ -146,13 +146,13 @@ class ConstantPoolCacheEntry {
   void set_bytecode_2(Bytecodes::Code code);
   void set_f1(Metadata* f1) {
     Metadata* existing_f1 = _f1; // read once
-    assert(existing_f1 == NULL || existing_f1 == f1, "illegal field change");
+    //assert(existing_f1 == NULL || existing_f1 == f1, "illegal field change");
     _f1 = f1;
   }
   void release_set_f1(Metadata* f1);
   void set_f2(intx f2) {
     intx existing_f2 = _f2; // read once
-    assert(existing_f2 == 0 || existing_f2 == f2, "illegal field change");
+    //assert(existing_f2 == 0 || existing_f2 == f2, "illegal field change");
     _f2 = f2;
   }
   void set_f2_as_vfinal_method(Method* f2) {
@@ -178,6 +178,8 @@ class ConstantPoolCacheEntry {
     tos_state_bits             = 4,
     tos_state_mask             = right_n_bits(tos_state_bits),
     tos_state_shift            = BitsPerInt - tos_state_bits,  // see verify_tos_state_shift below
+    // (DCEVM) We need to remember entries which has resolved reference indices as we don't want to clean them
+    is_resolved_ref_shift      = 27,
     // misc. option bits; can be any bit position in [16..27]
     is_field_entry_shift       = 26,  // (F) is it a field or a method?
     has_method_type_shift      = 25,  // (M) does the call site have a MethodType?
@@ -211,6 +213,7 @@ class ConstantPoolCacheEntry {
   void initialize_resolved_reference_index(int ref_index) {
     assert(_f2 == 0, "set once");  // note: ref_index might be zero also
     _f2 = ref_index;
+    _flags = 1 << is_resolved_ref_shift;
   }
 
   void set_field(                                // sets entry to resolved field state
@@ -360,6 +363,7 @@ class ConstantPoolCacheEntry {
   bool is_method_entry() const                   { return (_flags & (1 << is_field_entry_shift))    == 0; }
   bool is_field_entry() const                    { return (_flags & (1 << is_field_entry_shift))    != 0; }
   bool is_long() const                           { return flag_state() == ltos; }
+  bool is_resolved_reference() const             { return (_flags & (1 << is_resolved_ref_shift))   != 0; }
   bool is_double() const                         { return flag_state() == dtos; }
   TosState flag_state() const                    { assert((uint)number_of_states <= (uint)tos_state_mask+1, "");
                                                    return (TosState)((_flags >> tos_state_shift) & tos_state_mask); }
@@ -386,6 +390,10 @@ class ConstantPoolCacheEntry {
          bool* trace_name_printed);
   bool check_no_old_or_obsolete_entries();
   Method* get_interesting_method_entry(Klass* k);
+
+  // Enhanced RedefineClasses() API support (DCEVM):
+  // Clear cached entry, let it be re-resolved
+  void clear_entry();
 #endif // INCLUDE_JVMTI
 
   // Debugging & Printing
@@ -509,6 +517,10 @@ class ConstantPoolCache: public MetaspaceObj {
   void adjust_method_entries(InstanceKlass* holder, bool* trace_name_printed);
   bool check_no_old_or_obsolete_entries();
   void dump_cache();
+
+  // Enhanced RedefineClasses() API support (DCEVM):
+  // Clear all entries
+  void clear_entries();
 #endif // INCLUDE_JVMTI
 
   // RedefineClasses support
diff --git a/src/hotspot/share/oops/instanceKlass.cpp b/src/hotspot/share/oops/instanceKlass.cpp
index 9ba36f2de3f..8a262bc3735 100644
--- a/src/hotspot/share/oops/instanceKlass.cpp
+++ b/src/hotspot/share/oops/instanceKlass.cpp
@@ -832,7 +832,8 @@ bool InstanceKlass::link_class_impl(bool throw_verifyerror, TRAPS) {
       }
 #endif
       set_init_state(linked);
-      if (JvmtiExport::should_post_class_prepare()) {
+      // (DCEVM) Must check for old version in order to prevent infinite loops.
+      if (JvmtiExport::should_post_class_prepare() && old_version() == NULL /* JVMTI deadlock otherwise */) {
         Thread *thread = THREAD;
         assert(thread->is_Java_thread(), "thread->is_Java_thread()");
         JvmtiExport::post_class_prepare((JavaThread *) thread, this);
@@ -912,7 +913,9 @@ void InstanceKlass::initialize_impl(TRAPS) {
     // If we were to use wait() instead of waitInterruptibly() then
     // we might end up throwing IE from link/symbol resolution sites
     // that aren't expected to throw.  This would wreak havoc.  See 6320309.
-    while(is_being_initialized() && !is_reentrant_initialization(self)) {
+    // (DCEVM) Wait also for the old class version to be fully initialized.
+    while((is_being_initialized() && !is_reentrant_initialization(self))
+        || (old_version() != NULL && InstanceKlass::cast(old_version())->is_being_initialized())) {
         wait = true;
       ol.waitUninterruptibly(CHECK);
     }
@@ -1194,6 +1197,18 @@ bool InstanceKlass::implements_interface(Klass* k) const {
   return false;
 }
 
+bool InstanceKlass::implements_interface_any_version(Klass* k) const {
+  k = k->newest_version();
+  if (this->newest_version() == k) return true;
+  assert(k->is_interface(), "should be an interface class");
+  for (int i = 0; i < transitive_interfaces()->length(); i++) {
+    if (transitive_interfaces()->at(i)->newest_version() == k) {
+      return true;
+    }
+  }
+  return false;
+}
+
 bool InstanceKlass::is_same_or_direct_interface(Klass *k) const {
   // Verify direct super interface
   if (this == k) return true;
@@ -1464,6 +1479,23 @@ void InstanceKlass::methods_do(void f(Method* method)) {
   }
 }
 
+/**
+  Update information contains mapping of fields from old class to the new class.
+  Info is stored on HEAP, you need to call clear_update_information to free the space.
+*/
+void InstanceKlass::store_update_information(GrowableArray<int> &values) {
+  int *arr = NEW_C_HEAP_ARRAY(int, values.length(), mtClass);
+  for (int i = 0; i < values.length(); i++) {
+    arr[i] = values.at(i);
+  }
+  set_update_information(arr);
+}
+
+void InstanceKlass::clear_update_information() {
+  FREE_C_HEAP_ARRAY(int, update_information());
+  set_update_information(NULL);
+}
+
 
 void InstanceKlass::do_local_static_fields(FieldClosure* cl) {
   for (JavaFieldStream fs(this); !fs.done(); fs.next()) {
@@ -2118,8 +2150,26 @@ void InstanceKlass::add_dependent_nmethod(nmethod* nm) {
   dependencies().add_dependent_nmethod(nm);
 }
 
+bool InstanceKlass::update_jmethod_id(Method* method, jmethodID newMethodID) {
+  size_t idnum = (size_t)method->method_idnum();
+  jmethodID* jmeths = methods_jmethod_ids_acquire();
+  size_t length;                                // length assigned as debugging crumb
+  jmethodID id = NULL;
+  if (jmeths != NULL &&                         // If there is a cache
+      (length = (size_t)jmeths[0]) > idnum) {   // and if it is long enough,
+    jmeths[idnum+1] = newMethodID;              // Set method id (may be NULL)
+    return true;
+  }
+  return false;
+}
+
 void InstanceKlass::remove_dependent_nmethod(nmethod* nm, bool delete_immediately) {
   dependencies().remove_dependent_nmethod(nm, delete_immediately);
+  // (DCEVM) Hack as dependencies get wrong version of Klass*
+//  if (this->old_version() != NULL) {
+//    InstanceKlass::cast(this->old_version())->remove_dependent_nmethod(nm, true);
+//    return;
+//  }
 }
 
 #ifndef PRODUCT
@@ -3532,7 +3582,7 @@ void InstanceKlass::verify_on(outputStream* st) {
     }
 
     guarantee(sib->is_klass(), "should be klass");
-    guarantee(sib->super() == super, "siblings should have same superklass");
+    guarantee(sib->super() == super || super->newest_version() == SystemDictionary::Object_klass(), "siblings should have same superklass");
   }
 
   // Verify implementor fields requires the Compile_lock, but this is sometimes
diff --git a/src/hotspot/share/oops/instanceKlass.hpp b/src/hotspot/share/oops/instanceKlass.hpp
index 44541719e62..2cc98b636f1 100644
--- a/src/hotspot/share/oops/instanceKlass.hpp
+++ b/src/hotspot/share/oops/instanceKlass.hpp
@@ -118,6 +118,7 @@ class InstanceKlass: public Klass {
   friend class JVMCIVMStructs;
   friend class ClassFileParser;
   friend class CompileReplay;
+  friend class VM_EnhancedRedefineClasses;
 
  public:
   static const KlassID ID = InstanceKlassID;
@@ -932,6 +933,7 @@ public:
                 size_t *length_p, jmethodID* id_p);
   void ensure_space_for_methodids(int start_offset = 0);
   jmethodID jmethod_id_or_null(Method* method);
+  bool update_jmethod_id(Method* method, jmethodID newMethodID);
 
   // annotations support
   Annotations* annotations() const          { return _annotations; }
@@ -1004,6 +1006,7 @@ public:
 
   // subclass/subinterface checks
   bool implements_interface(Klass* k) const;
+  bool implements_interface_any_version(Klass* k) const;
   bool is_same_or_direct_interface(Klass* k) const;
 
 #ifdef ASSERT
@@ -1037,6 +1040,10 @@ public:
   void do_nonstatic_fields(FieldClosure* cl); // including inherited fields
   void do_local_static_fields(void f(fieldDescriptor*, Handle, TRAPS), Handle, TRAPS);
 
+  // Advanced class redefinition: FIXME: why here?
+  void store_update_information(GrowableArray<int> &values);
+  void clear_update_information();
+
   void methods_do(void f(Method* method));
   void array_klasses_do(void f(Klass* k));
   void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);
diff --git a/src/hotspot/share/oops/klass.cpp b/src/hotspot/share/oops/klass.cpp
index 01c5ed5bd46..f38874340be 100644
--- a/src/hotspot/share/oops/klass.cpp
+++ b/src/hotspot/share/oops/klass.cpp
@@ -197,7 +197,13 @@ void* Klass::operator new(size_t size, ClassLoaderData* loader_data, size_t word
 Klass::Klass(KlassID id) : _id(id),
                            _prototype_header(markOopDesc::prototype()),
                            _shared_class_path_index(-1),
-                           _java_mirror(NULL) {
+                           _java_mirror(NULL),
+                           _new_version(NULL),
+                           _old_version(NULL),
+                           _is_redefining(false),
+                           _is_copying_backwards(false),
+                           _redefinition_flags(Klass::NoRedefinition),
+                           _update_information(NULL) {
   CDS_ONLY(_shared_class_flags = 0;)
   CDS_JAVA_HEAP_ONLY(_archived_mirror = 0;)
   _primary_supers[0] = this;
@@ -393,6 +399,27 @@ void Klass::append_to_sibling_list() {
   debug_only(verify();)
 }
 
+void Klass::remove_from_sibling_list() {
+  debug_only(verify();)
+
+  // remove ourselves to superklass' subklass list
+  InstanceKlass* super = superklass();
+  if (super == NULL) return;        // special case: class Object
+  if (super->subklass() == this) {
+    // this klass is the first subklass
+    super->set_subklass(next_sibling());
+  } else {
+    Klass* sib = super->subklass();
+    assert(sib != NULL, "cannot find this class in sibling list!");
+    while (sib->next_sibling() != this) {
+      sib = sib->next_sibling();
+      assert(sib != NULL, "cannot find this class in sibling list!");
+    }
+    sib->set_next_sibling(next_sibling());
+  }
+  debug_only(verify();)
+}
+
 oop Klass::holder_phantom() const {
   return class_loader_data()->holder_phantom();
 }
diff --git a/src/hotspot/share/oops/klass.hpp b/src/hotspot/share/oops/klass.hpp
index b77a19deb38..5573b10f021 100644
--- a/src/hotspot/share/oops/klass.hpp
+++ b/src/hotspot/share/oops/klass.hpp
@@ -165,6 +165,18 @@ class Klass : public Metadata {
   // vtable length
   int _vtable_len;
 
+  // Advanced class redefinition
+
+  // Old version (used in advanced class redefinition)
+  Klass*      _old_version;
+  // New version (used in advanced class redefinition)
+  Klass*      _new_version;
+
+  int         _redefinition_flags;     // Level of class redefinition
+  bool        _is_redefining;
+  int*        _update_information;
+  bool        _is_copying_backwards;   // Does the class need to copy fields backwards? => possibly overwrite itself?
+
 private:
   // This is an index into FileMapHeader::_shared_path_table[], to
   // associate this class with the JAR file where it's loaded from during
@@ -289,6 +301,7 @@ protected:
   Klass* next_sibling() const          { return _next_sibling; }
   InstanceKlass* superklass() const;
   void append_to_sibling_list();           // add newly created receiver to superklass' subklass list
+  void remove_from_sibling_list();         // enhanced class redefinition
 
   void set_next_link(Klass* k) { _next_link = k; }
   Klass* next_link() const { return _next_link; }   // The next klass defined by the class loader.
@@ -329,11 +342,45 @@ protected:
   virtual ModuleEntry* module() const = 0;
   virtual PackageEntry* package() const = 0;
 
+  // Advanced class redefinition
+  Klass* old_version() const             { return _old_version; }
+  void set_old_version(Klass* klass)     { assert(_old_version == NULL || klass == NULL, "Old version can only be set once!"); _old_version = klass; }
+  Klass* new_version() const             { return _new_version; }
+  void set_new_version(Klass* klass)     { assert(_new_version == NULL || klass == NULL, "New version can only be set once!"); _new_version = klass; }
+  bool is_redefining() const             { return _is_redefining; }
+  void set_redefining(bool b)            { _is_redefining = b; }
+  int redefinition_flags() const         { return _redefinition_flags; }
+  bool check_redefinition_flag(int flags) const { return (_redefinition_flags & flags) != 0; }
+  void clear_redefinition_flag(int flag) { _redefinition_flags &= ~flag; }
+  void set_redefinition_flag(int flag)   { _redefinition_flags |= flag; }
+  void set_redefinition_flags(int flags) { _redefinition_flags = flags; }
+
+  const Klass* newest_version() const    { return _new_version == NULL ? this : _new_version->newest_version(); }
+        Klass* newest_version()          { return _new_version == NULL ? this : _new_version->newest_version(); }
+
+  const Klass* active_version() const   { return _new_version == NULL || _new_version->is_redefining() ? this : _new_version->active_version(); }
+        Klass* active_version()         { return _new_version == NULL || _new_version->is_redefining() ? this : _new_version->active_version(); }
+
+  // update information
+  int *update_information() const        { return _update_information; }
+  void set_update_information(int *info) { _update_information = info; }
+  bool is_copying_backwards() const      { return _is_copying_backwards; }
+  void set_copying_backwards(bool b)     { _is_copying_backwards = b; }
+
  protected:                                // internal accessors
   void     set_subklass(Klass* s);
   void     set_next_sibling(Klass* s);
 
  public:
+   enum RedefinitionFlags {
+     NoRedefinition,                             // This class is not redefined at all!
+     ModifyClass = 1,                            // There are changes to the class meta data.
+     ModifyClassSize = ModifyClass << 1,         // The size of the class meta data changes.
+     ModifyInstances = ModifyClassSize << 1,     // There are change to the instance format.
+     ModifyInstanceSize = ModifyInstances << 1,  // The size of instances changes.
+     RemoveSuperType = ModifyInstanceSize << 1,  // A super type of this class is removed.
+     MarkedAsAffected = RemoveSuperType << 1     // This class has been marked as an affected class.
+   };
 
   // Compiler support
   static ByteSize super_offset()                 { return in_ByteSize(offset_of(Klass, _super)); }
diff --git a/src/hotspot/share/oops/method.cpp b/src/hotspot/share/oops/method.cpp
index 7ca9b4b90ca..bee69f9cec6 100644
--- a/src/hotspot/share/oops/method.cpp
+++ b/src/hotspot/share/oops/method.cpp
@@ -1405,6 +1405,8 @@ methodHandle Method::clone_with_new_data(const methodHandle& m, u_char* new_code
 
   // Reset correct method/const method, method size, and parameter info
   newm->set_constMethod(newcm);
+  newm->set_new_version(newm->new_version());
+  newm->set_old_version(newm->old_version());
   newm->constMethod()->set_code_size(new_code_length);
   newm->constMethod()->set_constMethod_size(new_const_method_size);
   assert(newm->code_size() == new_code_length, "check");
@@ -2099,6 +2101,10 @@ void Method::ensure_jmethod_ids(ClassLoaderData* loader_data, int capacity) {
 
 // Add a method id to the jmethod_ids
 jmethodID Method::make_jmethod_id(ClassLoaderData* loader_data, Method* m) {
+  // FIXME: (DCEVM) ???
+  if (m != m->newest_version()) {
+    m = m->newest_version();
+  }
   ClassLoaderData* cld = loader_data;
 
   if (!SafepointSynchronize::is_at_safepoint()) {
diff --git a/src/hotspot/share/oops/method.hpp b/src/hotspot/share/oops/method.hpp
index 18c706187df..4533476ff8f 100644
--- a/src/hotspot/share/oops/method.hpp
+++ b/src/hotspot/share/oops/method.hpp
@@ -78,6 +78,9 @@ class Method : public Metadata {
   MethodCounters*   _method_counters;
   AccessFlags       _access_flags;               // Access flags
   int               _vtable_index;               // vtable index of this method (see VtableIndexFlag)
+  // (DCEVM) Newer version of method available?
+  Method*           _new_version;
+  Method*           _old_version;
                                                  // note: can have vtables with >2**16 elements (because of inheritance)
   u2                _intrinsic_id;               // vmSymbols::intrinsic_id (0 == _none)
 
@@ -154,6 +157,23 @@ class Method : public Metadata {
   int name_index() const                         { return constMethod()->name_index();         }
   void set_name_index(int index)                 { constMethod()->set_name_index(index);       }
 
+  Method* new_version() const                    { return _new_version; }
+  void set_new_version(Method* m)                { _new_version = m; }
+  Method* newest_version()                       { return (_new_version == NULL) ? this : _new_version->newest_version(); }
+
+  Method* old_version() const                    { return _old_version; }
+  void set_old_version(Method* m) {
+    /*if (m == NULL) {
+      _old_version = NULL;
+      return;
+    }*/
+
+    assert(_old_version == NULL, "may only be set once");
+    assert(this->code_size() == m->code_size(), "must have same code length");
+    _old_version = m;
+  }
+  const Method* oldest_version() const           { return (_old_version == NULL) ? this : _old_version->oldest_version(); }
+
   // signature
   Symbol* signature() const                      { return constants()->symbol_at(signature_index()); }
   int signature_index() const                    { return constMethod()->signature_index();         }
diff --git a/src/hotspot/share/prims/jni.cpp b/src/hotspot/share/prims/jni.cpp
index aed6075b52a..8b0ae584cf4 100644
--- a/src/hotspot/share/prims/jni.cpp
+++ b/src/hotspot/share/prims/jni.cpp
@@ -353,6 +353,7 @@ JNI_ENTRY(jclass, jni_DefineClass(JNIEnv *env, const char *name, jobject loaderR
                                                    class_loader,
                                                    Handle(),
                                                    &st,
+                                                   NULL,
                                                    CHECK_NULL);
 
   if (log_is_enabled(Debug, class, resolve) && k != NULL) {
diff --git a/src/hotspot/share/prims/jvm.cpp b/src/hotspot/share/prims/jvm.cpp
index 797b022d5e5..eb4b7622820 100644
--- a/src/hotspot/share/prims/jvm.cpp
+++ b/src/hotspot/share/prims/jvm.cpp
@@ -927,6 +927,7 @@ static jclass jvm_define_class_common(JNIEnv *env, const char *name,
                                                    class_loader,
                                                    protection_domain,
                                                    &st,
+                                                   NULL,
                                                    CHECK_NULL);
 
   if (log_is_enabled(Debug, class, resolve) && k != NULL) {
diff --git a/src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.cpp b/src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.cpp
new file mode 100644
index 00000000000..83c0952de37
--- /dev/null
+++ b/src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.cpp
@@ -0,0 +1,2255 @@
+/*
+ * Copyright (c) 2003, 2016, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include "precompiled.hpp"
+#include "aot/aotLoader.hpp"
+#include "classfile/classFileStream.hpp"
+#include "classfile/metadataOnStackMark.hpp"
+#include "classfile/systemDictionary.hpp"
+#include "classfile/verifier.hpp"
+#include "interpreter/oopMapCache.hpp"
+#include "interpreter/rewriter.hpp"
+#include "logging/logStream.hpp"
+#include "memory/metadataFactory.hpp"
+#include "memory/metaspaceShared.hpp"
+#include "memory/resourceArea.hpp"
+#include "memory/iterator.inline.hpp"
+#include "gc/serial/markSweep.hpp" // FIXME: other GC?
+#include "oops/fieldStreams.hpp"
+#include "oops/klassVtable.hpp"
+#include "oops/oop.inline.hpp"
+#include "oops/constantPool.inline.hpp"
+#include "prims/jvmtiImpl.hpp"
+#include "prims/jvmtiClassFileReconstituter.hpp"
+#include "prims/jvmtiEnhancedRedefineClasses.hpp"
+#include "prims/methodComparator.hpp"
+#include "prims/resolvedMethodTable.hpp"
+#include "runtime/deoptimization.hpp"
+#include "runtime/jniHandles.inline.hpp"
+#include "runtime/relocator.hpp"
+#include "utilities/bitMap.inline.hpp"
+#include "prims/jvmtiThreadState.inline.hpp"
+#include "utilities/events.hpp"
+#include "oops/constantPool.inline.hpp"
+
+Array<Method*>* VM_EnhancedRedefineClasses::_old_methods = NULL;
+Array<Method*>* VM_EnhancedRedefineClasses::_new_methods = NULL;
+Method**  VM_EnhancedRedefineClasses::_matching_old_methods = NULL;
+Method**  VM_EnhancedRedefineClasses::_matching_new_methods = NULL;
+Method**  VM_EnhancedRedefineClasses::_deleted_methods      = NULL;
+Method**  VM_EnhancedRedefineClasses::_added_methods        = NULL;
+int         VM_EnhancedRedefineClasses::_matching_methods_length = 0;
+int         VM_EnhancedRedefineClasses::_deleted_methods_length  = 0;
+int         VM_EnhancedRedefineClasses::_added_methods_length    = 0;
+Klass*      VM_EnhancedRedefineClasses::_the_class_oop = NULL;
+
+/**
+ * Create new instance of enhanced class redefiner.
+ *
+ * This class implements VM_GC_Operation - the usual usage should be:
+ *     VM_EnhancedRedefineClasses op(class_count, class_definitions, jvmti_class_load_kind_redefine);
+ *     VMThread::execute(&op);
+ * Which
+ *
+ * @param class_count size of class_defs
+ * @param class_defs class definition - either new class or redefined class
+ *               note that this is not the final array of classes to be redefined
+ *               we need to scan for all affected classes (e.g. subclasses) and
+ *               caculcate redefinition for them as well.
+ * @param class_load_kind always jvmti_class_load_kind_redefine
+ */
+VM_EnhancedRedefineClasses::VM_EnhancedRedefineClasses(jint class_count, const jvmtiClassDefinition *class_defs, JvmtiClassLoadKind class_load_kind) :
+        VM_GC_Operation(Universe::heap()->total_collections(), GCCause::_heap_inspection, Universe::heap()->total_full_collections(), true) {
+  _affected_klasses = NULL;
+  _class_count = class_count;
+  _class_defs = class_defs;
+  _class_load_kind = class_load_kind;
+  _res = JVMTI_ERROR_NONE;
+  _any_class_has_resolved_methods = false;
+}
+
+static inline InstanceKlass* get_ik(jclass def) {
+  oop mirror = JNIHandles::resolve_non_null(def);
+  return InstanceKlass::cast(java_lang_Class::as_Klass(mirror));
+}
+
+/**
+ * Start the redefinition:
+ * - Load new class definitions - @see load_new_class_versions
+ * - Start mark&sweep GC.
+ * @return true if success, otherwise all chnages are rollbacked.
+ */
+bool VM_EnhancedRedefineClasses::doit_prologue() {
+
+  if (_class_count == 0) {
+    _res = JVMTI_ERROR_NONE;
+    return false;
+  }
+  if (_class_defs == NULL) {
+    _res = JVMTI_ERROR_NULL_POINTER;
+    return false;
+  }
+  for (int i = 0; i < _class_count; i++) {
+    if (_class_defs[i].klass == NULL) {
+      _res = JVMTI_ERROR_INVALID_CLASS;
+      return false;
+    }
+    if (_class_defs[i].class_byte_count == 0) {
+      _res = JVMTI_ERROR_INVALID_CLASS_FORMAT;
+      return false;
+    }
+    if (_class_defs[i].class_bytes == NULL) {
+      _res = JVMTI_ERROR_NULL_POINTER;
+      return false;
+    }
+
+    // classes for primitives and arrays and vm anonymous classes cannot be redefined
+    // check here so following code can assume these classes are InstanceKlass
+    oop mirror = JNIHandles::resolve_non_null(_class_defs[i].klass);
+    if (!is_modifiable_class(mirror)) {
+      _res = JVMTI_ERROR_UNMODIFIABLE_CLASS;
+      return false;
+    }
+  }
+
+  // Start timer after all the sanity checks; not quite accurate, but
+  // better than adding a bunch of stop() calls.
+  if (log_is_enabled(Info, redefine, class, timer)) {
+    _timer_vm_op_prologue.start();
+  }
+
+  // We first load new class versions in the prologue, because somewhere down the
+  // call chain it is required that the current thread is a Java thread.
+  _res = load_new_class_versions(Thread::current());
+
+  // prepare GC, lock heap
+  if (_res == JVMTI_ERROR_NONE && !VM_GC_Operation::doit_prologue()) {
+    _res = JVMTI_ERROR_INTERNAL;
+  }
+
+  if (_res != JVMTI_ERROR_NONE) {
+    rollback();
+    // TODO free any successfully created classes
+    /*for (int i = 0; i < _class_count; i++) {
+      if (_new_classes[i] != NULL) {
+        ClassLoaderData* cld = _new_classes[i]->class_loader_data();
+        // Free the memory for this class at class unloading time.  Not before
+        // because CMS might think this is still live.
+        cld->add_to_deallocate_list(InstanceKlass::cast(_new_classes[i]));
+      }
+    }*/
+    delete _new_classes;
+    _new_classes = NULL;
+    delete _affected_klasses;
+    _affected_klasses = NULL;
+
+    _timer_vm_op_prologue.stop();
+    return false;
+  }
+
+  _timer_vm_op_prologue.stop();
+  return true;
+}
+
+/**
+ * Closer for static fields - copy value from old class to the new class.
+ */
+class FieldCopier : public FieldClosure {
+  public:
+  void do_field(fieldDescriptor* fd) {
+    InstanceKlass* cur = InstanceKlass::cast(fd->field_holder());
+    oop cur_oop = cur->java_mirror();
+
+    InstanceKlass* old = InstanceKlass::cast(cur->old_version());
+    oop old_oop = old->java_mirror();
+
+    fieldDescriptor result;
+    bool found = old->find_local_field(fd->name(), fd->signature(), &result);
+    if (found && result.is_static()) {
+      log_trace(redefine, class, obsolete, metadata)("Copying static field value for field %s old_offset=%d new_offset=%d",
+                                               fd->name()->as_C_string(), result.offset(), fd->offset());
+      memcpy(cur_oop->obj_field_addr_raw<HeapWord>(fd->offset()),
+             old_oop->obj_field_addr_raw<HeapWord>(result.offset()),
+             type2aelembytes(fd->field_type()));
+
+      // Static fields may have references to java.lang.Class
+      if (fd->field_type() == T_OBJECT) {
+         oop oop = cur_oop->obj_field(fd->offset());
+         if (oop != NULL && oop->is_instance() && InstanceKlass::cast(oop->klass())->is_mirror_instance_klass()) {
+            Klass* klass = java_lang_Class::as_Klass(oop);
+            if (klass != NULL && klass->is_instance_klass()) {
+              assert(oop == InstanceKlass::cast(klass)->java_mirror(), "just checking");
+              if (klass->new_version() != NULL) {
+                oop = InstanceKlass::cast(klass->new_version())->java_mirror();
+                cur_oop->obj_field_put(fd->offset(), oop);
+              }
+            }
+         }
+        }
+      }
+    }
+};
+
+
+// TODO: review...
+void VM_EnhancedRedefineClasses::mark_as_scavengable(nmethod* nm) {
+  if (!nm->on_scavenge_root_list()) {
+    CodeCache::add_scavenge_root_nmethod(nm);
+  }
+}
+
+// TODO comment
+struct StoreBarrier {
+  // TODO: j10 review change ::oop_store -> HeapAccess<>::oop_store
+  template <class T> static void oop_store(T* p, oop v) { HeapAccess<>::oop_store(p, v); }
+};
+
+
+// TODO comment
+struct StoreNoBarrier {
+  template <class T> static void oop_store(T* p, oop v) { RawAccess<IS_NOT_NULL>::oop_store(p, v); }
+};
+
+/**
+  Closure to scan all heap objects and update method handles
+*/
+template <class S>
+class ChangePointersOopClosure : public BasicOopIterateClosure {
+  // import java_lang_invoke_MemberName.*
+  enum {
+    REFERENCE_KIND_SHIFT = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,
+    REFERENCE_KIND_MASK  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,
+  };
+
+
+  bool update_member_name(oop obj) {
+    int flags    =       java_lang_invoke_MemberName::flags(obj);
+    int ref_kind =       (flags >> REFERENCE_KIND_SHIFT) & REFERENCE_KIND_MASK;
+    if (MethodHandles::ref_kind_is_method(ref_kind)) {
+      Method* m = (Method*) java_lang_invoke_MemberName::vmtarget(obj);
+      if (m != NULL && m->method_holder()->new_version() != NULL) {
+        // Let's try to re-resolve method
+        InstanceKlass* newest = InstanceKlass::cast(m->method_holder()->newest_version());
+        Method* new_method = newest->find_method(m->name(), m->signature());
+
+        if (new_method != NULL) {
+          // Note: we might set NULL at this point, which should force AbstractMethodError at runtime
+          Thread *thread = Thread::current();
+          CallInfo info(new_method, newest, thread);
+          Handle objHandle(thread, obj);  // TODO : review thread
+          MethodHandles::init_method_MemberName(objHandle, info);
+        } else {
+          java_lang_invoke_MemberName::set_method(obj, NULL);
+        }
+      }
+    } else if (MethodHandles::ref_kind_is_field(ref_kind)) {
+      Klass* k = (Klass*) java_lang_invoke_MemberName::vmtarget(obj);
+      if (k == NULL) {
+        return false; // Was cleared before, this MemberName is invalid.
+      }
+
+      if (k != NULL && k->new_version() != NULL) {
+        // Let's try to re-resolve field
+        fieldDescriptor fd;
+        int offset = java_lang_invoke_MemberName::vmindex(obj);
+        bool is_static = MethodHandles::ref_kind_is_static(ref_kind);
+        InstanceKlass* ik = InstanceKlass::cast(k);
+        if (ik->find_local_field_from_offset(offset, is_static, &fd)) {
+          InstanceKlass* newest = InstanceKlass::cast(k->newest_version());
+          fieldDescriptor fd_new;
+          if (newest->find_local_field(fd.name(), fd.signature(), &fd_new)) {
+            Handle objHandle(Thread::current(), obj);  // TODO : review thread
+            MethodHandles::init_field_MemberName(objHandle, fd_new, MethodHandles::ref_kind_is_setter(ref_kind));
+          } else {
+            // Matching field is not found in new version, not much we can do here.
+            // JVM will crash once faulty MH is invoked.
+            // However, to avoid that all DMH's using this faulty MH are cleared (set to NULL)
+            // Eventually, we probably want to replace them with something more meaningful,
+            // like instance throwing NoSuchFieldError or DMH that will resort to dynamic
+            // field resolution (with possibility of type conversion)
+            java_lang_invoke_MemberName::set_method(obj, NULL);
+            java_lang_invoke_MemberName::set_vmindex(obj, 0);
+            return false;
+          }
+        }
+      }
+    }
+    return true;
+  }
+
+  bool update_direct_method_handle(oop obj) {
+    // Always update member name first.
+    oop mem_name = java_lang_invoke_DirectMethodHandle::member(obj);
+    if (!update_member_name(mem_name)) {
+      return false;
+    }
+
+    // Here we rely on DirectMethodHandle implementation.
+    // The current implementation caches field offset in $StaticAccessor/$Accessor
+    int flags    =       java_lang_invoke_MemberName::flags(mem_name);
+    int ref_kind =       (flags >> REFERENCE_KIND_SHIFT) & REFERENCE_KIND_MASK;
+    if (MethodHandles::ref_kind_is_field(ref_kind)) {
+      // Note: we don't care about staticBase field (which is java.lang.Class)
+      // It should be processed during normal object update.
+      // Update offset in StaticAccessor
+      int offset = java_lang_invoke_MemberName::vmindex(mem_name);
+      if (offset != 0) { // index of 0 means that field no longer exist
+        if (java_lang_invoke_DirectMethodHandle_StaticAccessor::is_instance(obj)) {
+          java_lang_invoke_DirectMethodHandle_StaticAccessor::set_static_offset(obj, offset);
+        } else if (java_lang_invoke_DirectMethodHandle_Accessor::is_instance(obj)) {
+          java_lang_invoke_DirectMethodHandle_Accessor::set_field_offset(obj, offset);
+        }
+      }
+    }
+    return true;
+  }
+
+  // Forward pointers to InstanceKlass and mirror class to new versions
+  template <class T>
+  inline void do_oop_work(T* p) {
+    oop obj = RawAccess<>::oop_load(p);
+    if (obj == NULL) {
+      return;
+    }
+    if (obj->is_instance() && InstanceKlass::cast(obj->klass())->is_mirror_instance_klass()) {
+      Klass* klass = java_lang_Class::as_Klass(obj);
+      if (klass != NULL && klass->is_instance_klass()) {
+        assert(obj == InstanceKlass::cast(klass)->java_mirror(), "just checking");
+        if (klass->new_version() != NULL) {
+          obj = InstanceKlass::cast(klass->new_version())->java_mirror();
+          S::oop_store(p, obj);
+        }
+      }
+    }
+
+    // JSR 292 support, uptade java.lang.invoke.MemberName instances
+    if (java_lang_invoke_MemberName::is_instance(obj)) {
+      update_member_name(obj);
+    } else if (java_lang_invoke_DirectMethodHandle::is_instance(obj)) {
+      if (!update_direct_method_handle(obj)) {
+        // DMH is no longer valid, replace it with null reference.
+        // See note above. We probably want to replace this with something more meaningful.
+        S::oop_store(p, NULL);
+      }
+    }
+  }
+
+  virtual void do_oop(oop* o) {
+    do_oop_work(o);
+  }
+
+  virtual void do_oop(narrowOop* o) {
+    do_oop_work(o);
+  }
+};
+
+/**
+ * Closure to scan all objects on heap for objects of changed classes
+ *   - if the fields are compatible, only update class definition reference
+ *   - otherwise if the new object size is smaller then old size, reshufle
+ *          the fields and fill the gap with "dead_space"
+ *   - otherwise set the _needs_instance_update flag, we need to do full GC
+ *          and reshuffle object positions durring mark&sweep
+ */
+class ChangePointersObjectClosure : public ObjectClosure {
+  private:
+
+  OopIterateClosure *_closure;
+  bool _needs_instance_update;
+  oop _tmp_obj;
+  int _tmp_obj_size;
+
+public:
+  ChangePointersObjectClosure(OopIterateClosure *closure) : _closure(closure), _needs_instance_update(false), _tmp_obj(NULL), _tmp_obj_size(0) {}
+
+  bool needs_instance_update() {
+    return _needs_instance_update;
+  }
+
+  void copy_to_tmp(oop o) {
+    int size = o->size();
+    if (_tmp_obj_size < size) {
+      _tmp_obj_size = size;
+      _tmp_obj = (oop)resource_allocate_bytes(size * HeapWordSize);
+    }
+    Copy::aligned_disjoint_words((HeapWord*)o, (HeapWord*)_tmp_obj, size);
+  }
+
+  virtual void do_object(oop obj) {
+    if (obj->is_instance() && InstanceKlass::cast(obj->klass())->is_mirror_instance_klass()) {
+      // static fields may have references to old java.lang.Class instances, update them
+      // at the same time, we don't want to update other oops in the java.lang.Class
+      // Causes SIGSEGV?
+      //instanceMirrorKlass::oop_fields_iterate(obj, _closure);
+    } else {
+      obj->oop_iterate(_closure);
+    }
+
+    if (obj->klass()->new_version() != NULL) {
+      Klass* new_klass = obj->klass()->new_version();
+
+      if (new_klass->update_information() != NULL) {
+        int size_diff = obj->size() - obj->size_given_klass(new_klass);
+
+        // Either new size is bigger or gap is to small to be filled
+        if (size_diff < 0 || (size_diff > 0 && (size_t) size_diff < CollectedHeap::min_fill_size())) {
+          // We need an instance update => set back to old klass
+          _needs_instance_update = true;
+        } else {
+          oop src = obj;
+          if (new_klass->is_copying_backwards()) {
+            copy_to_tmp(obj);
+            src = _tmp_obj;
+          }
+          src->set_klass(obj->klass()->new_version());
+          //  FIXME: instance updates...
+          //guarantee(false, "instance updates!");
+          MarkSweep::update_fields(obj, src, new_klass->update_information());
+
+          if (size_diff > 0) {
+            HeapWord* dead_space = ((HeapWord *)obj) + obj->size();
+            CollectedHeap::fill_with_object(dead_space, size_diff);
+          }
+        }
+      } else {
+        obj->set_klass(obj->klass()->new_version());
+      }
+    }
+  }
+};
+
+
+/**
+  Main transformation method - runs in VM thread.
+
+  - UseSharedSpaces - TODO what does it mean?
+  - for each sratch class call redefine_single_class
+  - clear code cache (flush_dependent_code)
+  - iterate the heap and update object defintions, check it old/new class fields
+       are compatible. If new class size is smaller then old, it can be solved directly here.
+  - iterate the heap and update method handles to new version
+  - Swap marks to have same hashcodes
+  - copy static fields
+  - notify JVM of the modification
+*/
+void VM_EnhancedRedefineClasses::doit() {
+  Thread *thread = Thread::current();
+
+  if (UseSharedSpaces) {
+    // Sharing is enabled so we remap the shared readonly space to
+    // shared readwrite, private just in case we need to redefine
+    // a shared class. We do the remap during the doit() phase of
+    // the safepoint to be safer.
+    if (!MetaspaceShared::remap_shared_readonly_as_readwrite()) {
+      log_info(redefine, class, load)("failed to remap shared readonly space to readwrite, private");
+      _res = JVMTI_ERROR_INTERNAL;
+      return;
+    }
+  }
+
+  // Mark methods seen on stack and everywhere else so old methods are not
+  // cleaned up if they're on the stack.
+  MetadataOnStackMark md_on_stack(true);
+  HandleMark hm(thread);   // make sure any handles created are deleted
+                           // before the stack walk again.
+
+  for (int i = 0; i < _new_classes->length(); i++) {
+    redefine_single_class(_new_classes->at(i), thread);
+  }
+
+  // Deoptimize all compiled code that depends on this class (do only once, because it clears whole cache)
+  flush_dependent_code(NULL, thread);
+
+  // JSR-292 support
+  if (_any_class_has_resolved_methods) {
+    bool trace_name_printed = false;
+    ResolvedMethodTable::adjust_method_entries(&trace_name_printed);
+  }
+
+  ChangePointersOopClosure<StoreNoBarrier> oopClosureNoBarrier;
+  ChangePointersOopClosure<StoreBarrier> oopClosure;
+  ChangePointersObjectClosure objectClosure(&oopClosure);
+
+  log_trace(redefine, class, obsolete, metadata)("Before updating instances");
+  {
+    // Since we may update oops inside nmethod's code blob to point to java.lang.Class in new generation, we need to
+    // make sure such references are properly recognized by GC. For that, If ScavengeRootsInCode is true, we need to
+    // mark such nmethod's as "scavengable".
+    // For now, mark all nmethod's as scavengable that are not scavengable already
+    if (ScavengeRootsInCode) {
+      CodeCache::nmethods_do(mark_as_scavengable);
+    }
+
+    Universe::heap()->ensure_parsability(false);
+    Universe::heap()->object_iterate(&objectClosure);
+    Universe::root_oops_do(&oopClosureNoBarrier);
+  }
+  log_trace(redefine, class, obsolete, metadata)("After updating instances");
+
+  for (int i = 0; i < _new_classes->length(); i++) {
+    InstanceKlass* cur = InstanceKlass::cast(_new_classes->at(i));
+    InstanceKlass* old = InstanceKlass::cast(cur->old_version());
+
+    // Swap marks to have same hashcodes
+    markOop cur_mark = cur->prototype_header();
+    markOop old_mark = old->prototype_header();
+    cur->set_prototype_header(old_mark);
+    old->set_prototype_header(cur_mark);
+
+    //swap_marks(cur, old);
+    cur_mark = cur->java_mirror()->mark();
+    old_mark = old->java_mirror()->mark();
+    cur->java_mirror()->set_mark(old_mark);
+    old->java_mirror()->set_mark(cur_mark);
+
+
+      // Revert pool holder for old version of klass (it was updated by one of ours closure!)
+    old->constants()->set_pool_holder(old);
+
+    Klass* array_klasses = old->array_klasses();
+    if (array_klasses != NULL) {
+      assert(cur->array_klasses() == NULL, "just checking");
+
+      // Transfer the array classes, otherwise we might get cast exceptions when casting array types.
+      // Also, set array klasses element klass.
+      cur->set_array_klasses(array_klasses);
+      ObjArrayKlass::cast(array_klasses)->set_element_klass(cur);
+      java_lang_Class::release_set_array_klass(cur->java_mirror(), array_klasses);
+      java_lang_Class::set_component_mirror(array_klasses->java_mirror(), cur->java_mirror());
+    }
+
+    // Initialize the new class! Special static initialization that does not execute the
+    // static constructor but copies static field values from the old class if name
+    // and signature of a static field match.
+    FieldCopier copier;
+    cur->do_local_static_fields(&copier); // TODO (tw): What about internal static fields??
+    //java_lang_Class::set_klass(old->java_mirror(), cur); // FIXME-isd (from JDK8): is that correct?
+    //FIXME-isd (from JDK8): do we need this: ??? old->set_java_mirror(cur->java_mirror());
+
+    // Transfer init state
+    InstanceKlass::ClassState state = old->init_state();
+    if (state > InstanceKlass::linked) {
+      cur->set_init_state(state);
+    }
+  }
+
+//  if (objectClosure.needs_instance_update()) {
+    // Do a full garbage collection to update the instance sizes accordingly
+    Universe::set_redefining_gc_run(true);
+    notify_gc_begin(true);
+    Universe::heap()->collect_as_vm_thread(GCCause::_heap_inspection);
+    notify_gc_end();
+    Universe::set_redefining_gc_run(false);
+//  }
+
+  // Unmark Klass*s as "redefining"
+  for (int i = 0; i < _new_classes->length(); i++) {
+    InstanceKlass* cur = _new_classes->at(i);
+    cur->set_redefining(false);
+    cur->clear_update_information();
+  }
+
+  // TODO: explain...
+  SystemDictionary::update_constraints_after_redefinition();
+
+  // TODO: explain...
+  ciObjectFactory::resort_shared_ci_metadata();
+
+  // FIXME - check if it was in JDK8. Copied from standard JDK9 hotswap.
+  //MethodDataCleaner clean_weak_method_links;
+  //ClassLoaderDataGraph::classes_do(&clean_weak_method_links);
+
+  // Disable any dependent concurrent compilations
+  SystemDictionary::notice_modification();
+
+  // Set flag indicating that some invariants are no longer true.
+  // See jvmtiExport.hpp for detailed explanation.
+  JvmtiExport::set_has_redefined_a_class();
+
+  // check_class() is optionally called for product bits, but is
+  // always called for non-product bits.
+#ifdef PRODUCT
+  if (log_is_enabled(Trace, redefine, class, obsolete, metadata)) {
+#endif
+    log_trace(redefine, class, obsolete, metadata)("calling check_class");
+    CheckClass check_class(thread);
+    ClassLoaderDataGraph::classes_do(&check_class);
+#ifdef PRODUCT
+  }
+#endif
+}
+
+/**
+ * Cleanup - runs in JVM thread
+ *  - free used memory
+ *  - end GC
+ */
+void VM_EnhancedRedefineClasses::doit_epilogue() {
+  VM_GC_Operation::doit_epilogue();
+
+  if (_new_classes != NULL) {
+    delete _new_classes;
+  }
+  _new_classes = NULL;
+  if (_affected_klasses != NULL) {
+    delete _affected_klasses;
+  }
+  _affected_klasses = NULL;
+
+  // Reset the_class_oop to null for error printing.
+  _the_class_oop = NULL;
+
+  if (log_is_enabled(Info, redefine, class, timer)) {
+    // Used to have separate timers for "doit" and "all", but the timer
+    // overhead skewed the measurements.
+    jlong doit_time = _timer_rsc_phase1.milliseconds() +
+                      _timer_rsc_phase2.milliseconds();
+    jlong all_time = _timer_vm_op_prologue.milliseconds() + doit_time;
+
+    log_info(redefine, class, timer)
+      ("vm_op: all=" JLONG_FORMAT "  prologue=" JLONG_FORMAT "  doit=" JLONG_FORMAT,
+       all_time, _timer_vm_op_prologue.milliseconds(), doit_time);
+    log_info(redefine, class, timer)
+      ("redefine_single_class: phase1=" JLONG_FORMAT "  phase2=" JLONG_FORMAT,
+       _timer_rsc_phase1.milliseconds(), _timer_rsc_phase2.milliseconds());
+  }
+}
+
+/**
+ * Exclude java primitives and arrays from redefinition
+ * @param klass_mirror  pointer to the klass
+ * @return true if is modifiable
+ */
+bool VM_EnhancedRedefineClasses::is_modifiable_class(oop klass_mirror) {
+  // classes for primitives cannot be redefined
+  if (java_lang_Class::is_primitive(klass_mirror)) {
+    return false;
+  }
+  Klass* k = java_lang_Class::as_Klass(klass_mirror);
+  // classes for arrays cannot be redefined
+  if (k == NULL || !k->is_instance_klass()) {
+    return false;
+  }
+
+  // Cannot redefine or retransform an anonymous class.
+  if (InstanceKlass::cast(k)->is_anonymous()) {
+    return false;
+  }
+  return true;
+}
+
+/**
+  Load and link new classes (either redefined or affected by redefinition - subclass, ...)
+
+  - find sorted affected classes
+  - resolve new class
+  - calculate redefine flags (field change, method change, supertype change, ...)
+  - calculate modified fields and mapping to old fields
+  - link new classes
+
+  The result is sotred in _affected_klasses(old definitions) and _new_classes(new definitions) arrays.
+*/
+jvmtiError VM_EnhancedRedefineClasses::load_new_class_versions(TRAPS) {
+
+  _affected_klasses = new (ResourceObj::C_HEAP, mtInternal) GrowableArray<Klass*>(_class_count, true);
+  _new_classes = new (ResourceObj::C_HEAP, mtInternal) GrowableArray<InstanceKlass*>(_class_count, true);
+
+  ResourceMark rm(THREAD);
+
+  // Retrieve an array of all classes that need to be redefined into _affected_klasses
+  jvmtiError err = find_sorted_affected_classes(THREAD);
+  if (err != JVMTI_ERROR_NONE) {
+    return err;
+  }
+
+  // thread local state - used to transfer class_being_redefined object to SystemDictonery::resolve_from_stream
+  JvmtiThreadState *state = JvmtiThreadState::state_for(JavaThread::current());
+  // state can only be NULL if the current thread is exiting which
+  // should not happen since we're trying to do a RedefineClasses
+  guarantee(state != NULL, "exiting thread calling load_new_class_versions");
+
+  _max_redefinition_flags = Klass::NoRedefinition;
+
+  for (int i = 0; i < _affected_klasses->length(); i++) {
+    // Create HandleMark so that any handles created while loading new class
+    // versions are deleted. Constant pools are deallocated while merging
+    // constant pools
+    HandleMark hm(THREAD);
+    InstanceKlass* the_class = InstanceKlass::cast(_affected_klasses->at(i));
+    Symbol*  the_class_sym = the_class->name();
+
+    // Ensure class is linked before redefine
+    if (!the_class->is_linked()) {
+      the_class->link_class(THREAD);
+      if (HAS_PENDING_EXCEPTION) {
+        Symbol* ex_name = PENDING_EXCEPTION->klass()->name();
+        log_info(redefine, class, load, exceptions)("link_class exception: '%s'", ex_name->as_C_string());
+        CLEAR_PENDING_EXCEPTION;
+        if (ex_name == vmSymbols::java_lang_OutOfMemoryError()) {
+          return JVMTI_ERROR_OUT_OF_MEMORY;
+        } else {
+          return JVMTI_ERROR_INTERNAL;
+        }
+      }
+    }
+
+    log_debug(redefine, class, load)
+      ("loading name=%s kind=%d (avail_mem=" UINT64_FORMAT "K)",
+       the_class->external_name(), _class_load_kind, os::available_memory() >> 10);
+
+    // class bytes...
+    const unsigned char* class_bytes;
+    jint class_byte_count;
+    jvmtiError error;
+    jboolean not_changed;
+    if ((error = find_class_bytes(the_class, &class_bytes, &class_byte_count, &not_changed)) != JVMTI_ERROR_NONE) {
+      log_info(redefine, class, load, exceptions)("error finding class bytes: %d", (int) error);
+      return error;
+    }
+    assert(class_bytes != NULL && class_byte_count != 0, "class bytes should be defined at this point!");
+
+    ClassFileStream st((u1*)class_bytes,
+                       class_byte_count,
+                       "__VM_EnhancedRedefineClasses__",
+                       ClassFileStream::verify);
+
+    // Parse the stream.
+    Handle the_class_loader(THREAD, the_class->class_loader());
+    Handle protection_domain(THREAD, the_class->protection_domain());
+    // Set redefined class handle in JvmtiThreadState class.
+    // This redefined class is sent to agent event handler for class file
+    // load hook event.
+    state->set_class_being_redefined(the_class, _class_load_kind);
+
+    InstanceKlass* k = SystemDictionary::resolve_from_stream(the_class_sym,
+                                                the_class_loader,
+                                                protection_domain,
+                                                &st,
+                                                the_class,
+                                                THREAD);
+    // Clear class_being_redefined just to be sure.
+    state->clear_class_being_redefined();
+
+    if (HAS_PENDING_EXCEPTION) {
+      Symbol* ex_name = PENDING_EXCEPTION->klass()->name();
+      log_info(redefine, class, load, exceptions)("parse_stream exception: '%s'", ex_name->as_C_string());
+      CLEAR_PENDING_EXCEPTION;
+
+      if (ex_name == vmSymbols::java_lang_UnsupportedClassVersionError()) {
+        return JVMTI_ERROR_UNSUPPORTED_VERSION;
+      } else if (ex_name == vmSymbols::java_lang_ClassFormatError()) {
+        return JVMTI_ERROR_INVALID_CLASS_FORMAT;
+      } else if (ex_name == vmSymbols::java_lang_ClassCircularityError()) {
+        return JVMTI_ERROR_CIRCULAR_CLASS_DEFINITION;
+      } else if (ex_name == vmSymbols::java_lang_NoClassDefFoundError()) {
+        // The message will be "XXX (wrong name: YYY)"
+        return JVMTI_ERROR_NAMES_DONT_MATCH;
+      } else if (ex_name == vmSymbols::java_lang_OutOfMemoryError()) {
+        return JVMTI_ERROR_OUT_OF_MEMORY;
+      } else {  // Just in case more exceptions can be thrown..
+        return JVMTI_ERROR_FAILS_VERIFICATION;
+      }
+    }
+
+    InstanceKlass* new_class = k;
+    the_class->set_new_version(new_class);
+    _new_classes->append(new_class);
+
+    int redefinition_flags = Klass::NoRedefinition;
+    if (not_changed) {
+      redefinition_flags = Klass::NoRedefinition;
+    } else {
+      redefinition_flags = calculate_redefinition_flags(new_class);
+      if (redefinition_flags >= Klass::RemoveSuperType) {
+        return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_HIERARCHY_CHANGED;
+      }
+    }
+
+    if (new_class->super() != NULL) {
+      redefinition_flags = redefinition_flags | new_class->super()->redefinition_flags();
+    }
+
+    for (int j = 0; j < new_class->local_interfaces()->length(); j++) {
+      redefinition_flags = redefinition_flags | (new_class->local_interfaces()->at(j))->redefinition_flags();
+    }
+
+    new_class->set_redefinition_flags(redefinition_flags);
+
+    _max_redefinition_flags = _max_redefinition_flags | redefinition_flags;
+
+    if ((redefinition_flags & Klass::ModifyInstances) != 0) {
+       calculate_instance_update_information(_new_classes->at(i));
+    } else {
+      // Fields were not changed, transfer special flags only
+      assert(new_class->layout_helper() >> 1 == new_class->old_version()->layout_helper() >> 1, "must be equal");
+      assert(new_class->fields()->length() == InstanceKlass::cast(new_class->old_version())->fields()->length(), "must be equal");
+
+      JavaFieldStream old_fs(the_class);
+      JavaFieldStream new_fs(new_class);
+      for (; !old_fs.done() && !new_fs.done(); old_fs.next(), new_fs.next()) {
+        AccessFlags flags = new_fs.access_flags();
+        flags.set_is_field_modification_watched(old_fs.access_flags().is_field_modification_watched());
+        flags.set_is_field_access_watched(old_fs.access_flags().is_field_access_watched());
+        flags.set_has_field_initialized_final_update(old_fs.access_flags().has_field_initialized_final_update());
+        new_fs.set_access_flags(flags);
+      }
+    }
+
+    log_debug(redefine, class, load)
+      ("loaded name=%s (avail_mem=" UINT64_FORMAT "K)", the_class->external_name(), os::available_memory() >> 10);
+  }
+
+  // Link and verify new classes _after_ all classes have been updated in the system dictionary!
+  for (int i = 0; i < _affected_klasses->length(); i++) {
+    Klass* the_class = _affected_klasses->at(i);
+    assert (the_class->new_version() != NULL, "new version must be present");
+    InstanceKlass* new_class(InstanceKlass::cast(the_class->new_version()));
+
+    new_class->link_class(THREAD);
+
+    if (HAS_PENDING_EXCEPTION) {
+      Symbol* ex_name = PENDING_EXCEPTION->klass()->name();
+      log_info(redefine, class, load, exceptions)("link_class exception: '%s'", new_class->name()->as_C_string());
+      CLEAR_PENDING_EXCEPTION;
+      if (ex_name == vmSymbols::java_lang_OutOfMemoryError()) {
+        return JVMTI_ERROR_OUT_OF_MEMORY;
+      } else {
+        return JVMTI_ERROR_INTERNAL;
+      }
+    }
+  }
+  return JVMTI_ERROR_NONE;
+}
+
+/**
+  Calculated the difference between new and old class  (field change, method change, supertype change, ...).
+*/
+int VM_EnhancedRedefineClasses::calculate_redefinition_flags(InstanceKlass* new_class) {
+  int result = Klass::NoRedefinition;
+  log_info(redefine, class, load)("Comparing different class versions of class %s",new_class->name()->as_C_string());
+
+  assert(new_class->old_version() != NULL, "must have old version");
+  InstanceKlass* the_class = InstanceKlass::cast(new_class->old_version());
+
+  // Check whether class is in the error init state.
+  if (the_class->is_in_error_state()) {
+    // TBD #5057930: special error code is needed in 1.6
+    //result = Klass::union_redefinition_level(result, Klass::Invalid);
+  }
+
+  int i;
+
+  // Check superclasses
+  assert(new_class->super() == NULL || new_class->super()->new_version() == NULL, "superclass must be of newest version");
+  if (the_class->super() != new_class->super()) {
+    // Super class changed
+    Klass* cur_klass = the_class->super();
+    while (cur_klass != NULL) {
+      if (!new_class->is_subclass_of(cur_klass->newest_version())) {
+        log_info(redefine, class, load)("removed super class %s", cur_klass->name()->as_C_string());
+        result = result | Klass::RemoveSuperType | Klass::ModifyInstances | Klass::ModifyClass;
+      }
+      cur_klass = cur_klass->super();
+    }
+
+    cur_klass = new_class->super();
+    while (cur_klass != NULL) {
+      if (!the_class->is_subclass_of(cur_klass->old_version())) {
+        log_info(redefine, class, load)("added super class %s", cur_klass->name()->as_C_string());
+        result = result | Klass::ModifyClass | Klass::ModifyInstances;
+      }
+      cur_klass = cur_klass->super();
+    }
+  }
+
+  // Check interfaces
+
+  // Interfaces removed?
+  Array<Klass*>* old_interfaces = the_class->transitive_interfaces();
+  for (i = 0; i < old_interfaces->length(); i++) {
+    InstanceKlass* old_interface = InstanceKlass::cast(old_interfaces->at(i));
+    if (!new_class->implements_interface_any_version(old_interface)) {
+      result = result | Klass::RemoveSuperType | Klass::ModifyClass;
+      log_info(redefine, class, load)("removed interface %s", old_interface->name()->as_C_string());
+    }
+  }
+
+  // Interfaces added?
+  Array<Klass*>* new_interfaces = new_class->transitive_interfaces();
+  for (i = 0; i<new_interfaces->length(); i++) {
+    if (!the_class->implements_interface_any_version(new_interfaces->at(i))) {
+      result = result | Klass::ModifyClass;
+      log_info(redefine, class, load)("added interface %s", new_interfaces->at(i)->name()->as_C_string());
+    }
+  }
+
+  // Check whether class modifiers are the same.
+  jushort old_flags = (jushort) the_class->access_flags().get_flags();
+  jushort new_flags = (jushort) new_class->access_flags().get_flags();
+  if (old_flags != new_flags) {
+    // FIXME: Can this have any effects?
+  }
+
+  // Check if the number, names, types and order of fields declared in these classes
+  // are the same.
+  JavaFieldStream old_fs(the_class);
+  JavaFieldStream new_fs(new_class);
+  for (; !old_fs.done() && !new_fs.done(); old_fs.next(), new_fs.next()) {
+    // access
+    old_flags = old_fs.access_flags().as_short();
+    new_flags = new_fs.access_flags().as_short();
+    if ((old_flags ^ new_flags) & JVM_RECOGNIZED_FIELD_MODIFIERS) {
+      // FIXME: can this have any effect?
+    }
+    // offset
+    if (old_fs.offset() != new_fs.offset()) {
+      result = result | Klass::ModifyInstances;
+    }
+    // name and signature
+    Symbol* name_sym1 = the_class->constants()->symbol_at(old_fs.name_index());
+    Symbol* sig_sym1 = the_class->constants()->symbol_at(old_fs.signature_index());
+    Symbol* name_sym2 = new_class->constants()->symbol_at(new_fs.name_index());
+    Symbol* sig_sym2 = new_class->constants()->symbol_at(new_fs.signature_index());
+    if (name_sym1 != name_sym2 || sig_sym1 != sig_sym2) {
+      result = result | Klass::ModifyInstances;
+    }
+  }
+
+  // If both streams aren't done then we have a differing number of
+  // fields.
+  if (!old_fs.done() || !new_fs.done()) {
+    result = result | Klass::ModifyInstances;
+  }
+
+  // Do a parallel walk through the old and new methods. Detect
+  // cases where they match (exist in both), have been added in
+  // the new methods, or have been deleted (exist only in the
+  // old methods).  The class file parser places methods in order
+  // by method name, but does not order overloaded methods by
+  // signature.  In order to determine what fate befell the methods,
+  // this code places the overloaded new methods that have matching
+  // old methods in the same order as the old methods and places
+  // new overloaded methods at the end of overloaded methods of
+  // that name. The code for this order normalization is adapted
+  // from the algorithm used in InstanceKlass::find_method().
+  // Since we are swapping out of order entries as we find them,
+  // we only have to search forward through the overloaded methods.
+  // Methods which are added and have the same name as an existing
+  // method (but different signature) will be put at the end of
+  // the methods with that name, and the name mismatch code will
+  // handle them.
+  Array<Method*>* k_old_methods(the_class->methods());
+  Array<Method*>* k_new_methods(new_class->methods());
+  int n_old_methods = k_old_methods->length();
+  int n_new_methods = k_new_methods->length();
+  Thread* thread = Thread::current();
+
+  int ni = 0;
+  int oi = 0;
+  while (true) {
+    Method* k_old_method;
+    Method* k_new_method;
+    enum { matched, added, deleted, undetermined } method_was = undetermined;
+
+    if (oi >= n_old_methods) {
+      if (ni >= n_new_methods) {
+        break; // we've looked at everything, done
+      }
+      // New method at the end
+      k_new_method = k_new_methods->at(ni);
+      method_was = added;
+    } else if (ni >= n_new_methods) {
+      // Old method, at the end, is deleted
+      k_old_method = k_old_methods->at(oi);
+      method_was = deleted;
+    } else {
+      // There are more methods in both the old and new lists
+      k_old_method = k_old_methods->at(oi);
+      k_new_method = k_new_methods->at(ni);
+      if (k_old_method->name() != k_new_method->name()) {
+        // Methods are sorted by method name, so a mismatch means added
+        // or deleted
+        if (k_old_method->name()->fast_compare(k_new_method->name()) > 0) {
+          method_was = added;
+        } else {
+          method_was = deleted;
+        }
+      } else if (k_old_method->signature() == k_new_method->signature()) {
+        // Both the name and signature match
+        method_was = matched;
+      } else {
+        // The name matches, but the signature doesn't, which means we have to
+        // search forward through the new overloaded methods.
+        int nj;  // outside the loop for post-loop check
+        for (nj = ni + 1; nj < n_new_methods; nj++) {
+          Method* m = k_new_methods->at(nj);
+          if (k_old_method->name() != m->name()) {
+            // reached another method name so no more overloaded methods
+            method_was = deleted;
+            break;
+          }
+          if (k_old_method->signature() == m->signature()) {
+            // found a match so swap the methods
+            k_new_methods->at_put(ni, m);
+            k_new_methods->at_put(nj, k_new_method);
+            k_new_method = m;
+            method_was = matched;
+            break;
+          }
+        }
+
+        if (nj >= n_new_methods) {
+          // reached the end without a match; so method was deleted
+          method_was = deleted;
+        }
+      }
+    }
+
+    switch (method_was) {
+    case matched:
+      // methods match, be sure modifiers do too
+      old_flags = (jushort) k_old_method->access_flags().get_flags();
+      new_flags = (jushort) k_new_method->access_flags().get_flags();
+      if ((old_flags ^ new_flags) & ~(JVM_ACC_NATIVE)) {
+        // TODO Can this have any effects? Probably yes on vtables?
+        result = result | Klass::ModifyClass;
+      }
+      {
+        u2 new_num = k_new_method->method_idnum();
+        u2 old_num = k_old_method->method_idnum();
+        if (new_num != old_num) {
+        Method* idnum_owner = new_class->method_with_idnum(old_num);
+          if (idnum_owner != NULL) {
+            // There is already a method assigned this idnum -- switch them
+            // Take current and original idnum from the new_method
+            idnum_owner->set_method_idnum(new_num);
+            idnum_owner->set_orig_method_idnum(k_new_method->orig_method_idnum());
+          }
+          // Take current and original idnum from the old_method
+          k_new_method->set_method_idnum(old_num);
+          k_new_method->set_orig_method_idnum(k_old_method->orig_method_idnum());
+          if (thread->has_pending_exception()) {
+            return JVMTI_ERROR_OUT_OF_MEMORY;
+          }
+        }
+      }
+      log_trace(redefine, class, normalize)
+        ("Method matched: new: %s [%d] == old: %s [%d]",
+         k_new_method->name_and_sig_as_C_string(), ni, k_old_method->name_and_sig_as_C_string(), oi);
+      // advance to next pair of methods
+      ++oi;
+      ++ni;
+      break;
+    case added:
+      // method added, see if it is OK
+      new_flags = (jushort) k_new_method->access_flags().get_flags();
+      if ((new_flags & JVM_ACC_PRIVATE) == 0
+           // hack: private should be treated as final, but alas
+          || (new_flags & (JVM_ACC_FINAL|JVM_ACC_STATIC)) == 0
+         ) {
+        // new methods must be private
+        result = result | Klass::ModifyClass;
+      }
+      {
+        u2 num = new_class->next_method_idnum();
+        if (num == ConstMethod::UNSET_IDNUM) {
+          // cannot add any more methods
+        result = result | Klass::ModifyClass;
+        }
+        u2 new_num = k_new_method->method_idnum();
+        Method* idnum_owner = new_class->method_with_idnum(num);
+        if (idnum_owner != NULL) {
+          // There is already a method assigned this idnum -- switch them
+          // Take current and original idnum from the new_method
+          idnum_owner->set_method_idnum(new_num);
+          idnum_owner->set_orig_method_idnum(k_new_method->orig_method_idnum());
+        }
+        k_new_method->set_method_idnum(num);
+        k_new_method->set_orig_method_idnum(num);
+        if (thread->has_pending_exception()) {
+          return JVMTI_ERROR_OUT_OF_MEMORY;
+        }
+      }
+      log_trace(redefine, class, normalize)
+        ("Method added: new: %s [%d]", k_new_method->name_and_sig_as_C_string(), ni);
+      ++ni; // advance to next new method
+      break;
+    case deleted:
+      // method deleted, see if it is OK
+      old_flags = (jushort) k_old_method->access_flags().get_flags();
+      if ((old_flags & JVM_ACC_PRIVATE) == 0
+           // hack: private should be treated as final, but alas
+          || (old_flags & (JVM_ACC_FINAL|JVM_ACC_STATIC)) == 0
+         ) {
+        // deleted methods must be private
+        result = result | Klass::ModifyClass;
+      }
+      log_trace(redefine, class, normalize)
+        ("Method deleted: old: %s [%d]", k_old_method->name_and_sig_as_C_string(), oi);
+      ++oi; // advance to next old method
+      break;
+    default:
+      ShouldNotReachHere();
+    }
+  }
+
+  if (new_class->size() != new_class->old_version()->size()) {
+    result |= Klass::ModifyClassSize;
+  }
+
+  if (new_class->size_helper() != (InstanceKlass::cast((new_class->old_version()))->size_helper())) {
+    result |= Klass::ModifyInstanceSize;
+  }
+
+  // TODO Check method bodies to be able to return NoChange?
+  return result;
+}
+
+
+/** 
+  Searches for the class bytecode of the given class and returns it as a byte array.
+  
+  @param the_class definition of a class, either existing class or new_class
+  @param class_bytes - if the class is redefined, it contains new class definition, otherwise just original class bytecode.
+  @param class_byte_count - size of class_bytes
+  @param not_changed - new_class not available or same as current class
+*/
+jvmtiError VM_EnhancedRedefineClasses::find_class_bytes(InstanceKlass* the_class, const unsigned char **class_bytes, jint *class_byte_count, jboolean *not_changed) {
+
+  *not_changed = false;
+
+  // Search for the index in the redefinition array that corresponds to the current class
+  int i;
+  for (i = 0; i < _class_count; i++) {
+    if (the_class == get_ik(_class_defs[i].klass))
+      break;
+  }
+
+  if (i == _class_count) {
+    *not_changed = true;
+
+    // Redefine with same bytecodes. This is a class that is only indirectly affected by redefinition,
+    // so the user did not specify a different bytecode for that class.
+    if (the_class->get_cached_class_file_bytes() == NULL) {
+      // Not cached, we need to reconstitute the class file from the
+      // VM representation. We don't attach the reconstituted class
+      // bytes to the InstanceKlass here because they have not been
+      // validated and we're not at a safepoint.
+      JvmtiClassFileReconstituter reconstituter(the_class);
+      if (reconstituter.get_error() != JVMTI_ERROR_NONE) {
+        return reconstituter.get_error();
+      }
+
+      *class_byte_count = (jint)reconstituter.class_file_size();
+      *class_bytes      = (unsigned char*) reconstituter.class_file_bytes();
+    } else {
+      // it is cached, get it from the cache
+      *class_byte_count = the_class->get_cached_class_file_len();
+      *class_bytes      = the_class->get_cached_class_file_bytes();
+    }
+  } else {
+    // Redefine with bytecodes at index j
+    *class_bytes = _class_defs[i].class_bytes;
+    *class_byte_count = _class_defs[i].class_byte_count;
+  }
+
+  return JVMTI_ERROR_NONE;
+}
+
+/**
+  Calculate difference between non static fields of old and new class and store the info into new class:
+     instanceKlass->store_update_information
+     instanceKlass->copy_backwards
+*/
+void VM_EnhancedRedefineClasses::calculate_instance_update_information(Klass* new_version) {
+
+  class CalculateFieldUpdates : public FieldClosure {
+
+  private:
+    InstanceKlass* _old_ik;
+    GrowableArray<int> _update_info;
+    int _position;
+    bool _copy_backwards;
+
+  public:
+
+    bool does_copy_backwards() {
+      return _copy_backwards;
+    }
+
+    CalculateFieldUpdates(InstanceKlass* old_ik) :
+        _old_ik(old_ik), _position(instanceOopDesc::base_offset_in_bytes()), _copy_backwards(false) {
+      _update_info.append(_position);
+      _update_info.append(0);
+    }
+
+    GrowableArray<int> &finish() {
+      _update_info.append(0);
+      return _update_info;
+    }
+
+    void do_field(fieldDescriptor* fd) {
+      int alignment = fd->offset() - _position;
+      if (alignment > 0) {
+        // This field was aligned, so we need to make sure that we fill the gap
+        fill(alignment);
+      }
+
+      assert(_position == fd->offset(), "must be correct offset!");
+
+      fieldDescriptor old_fd;
+      if (_old_ik->find_field(fd->name(), fd->signature(), false, &old_fd) != NULL) {
+        // Found field in the old class, copy
+        copy(old_fd.offset(), type2aelembytes(fd->field_type()));
+
+        if (old_fd.offset() < fd->offset()) {
+          _copy_backwards = true;
+        }
+
+        // Transfer special flags
+        fd->set_is_field_modification_watched(old_fd.is_field_modification_watched());
+        fd->set_is_field_access_watched(old_fd.is_field_access_watched());
+      } else {
+        // New field, fill
+        fill(type2aelembytes(fd->field_type()));
+      }
+   }
+
+  private:
+    void fill(int size) {
+      if (_update_info.length() > 0 && _update_info.at(_update_info.length() - 1) < 0) {
+        (*_update_info.adr_at(_update_info.length() - 1)) -= size;
+      } else {
+        _update_info.append(-size);
+      }
+      _position += size;
+    }
+
+    void copy(int offset, int size) {
+      int prev_end = -1;
+      if (_update_info.length() > 0 && _update_info.at(_update_info.length() - 1) > 0) {
+        prev_end = _update_info.at(_update_info.length() - 2) + _update_info.at(_update_info.length() - 1);
+      }
+
+      if (prev_end == offset) {
+        (*_update_info.adr_at(_update_info.length() - 2)) += size;
+      } else {
+        _update_info.append(size);
+        _update_info.append(offset);
+      }
+
+      _position += size;
+    }
+  };
+
+  InstanceKlass* ik = InstanceKlass::cast(new_version);
+  InstanceKlass* old_ik = InstanceKlass::cast(new_version->old_version());
+
+  //
+  CalculateFieldUpdates cl(old_ik);
+  ik->do_nonstatic_fields(&cl);
+
+  GrowableArray<int> result = cl.finish();
+  ik->store_update_information(result);
+  ik->set_copying_backwards(cl.does_copy_backwards());
+/* TODO logging
+  if (RC_TRACE_ENABLED(0x00000001)) {
+    RC_TRACE(0x00000001, ("Instance update information for %s:", new_version->name()->as_C_string()));
+    if (cl.does_copy_backwards()) {
+      RC_TRACE(0x00000001, ("\tDoes copy backwards!"));
+    }
+    for (int i=0; i<result.length(); i++) {
+      int curNum = result.at(i);
+      if (curNum < 0) {
+        RC_TRACE(0x00000001, ("\t%d CLEAN", curNum));
+      } else if (curNum > 0) {
+        RC_TRACE(0x00000001, ("\t%d COPY from %d", curNum, result.at(i + 1)));
+        i++;
+      } else {
+        RC_TRACE(0x00000001, ("\tEND"));
+      }
+    }
+  }*/
+}
+
+/**
+  Rollback all changes - clear new classes from the system dictionary, return old classes to directory, free memory.
+*/
+void VM_EnhancedRedefineClasses::rollback() {
+  log_info(redefine, class, load)("Rolling back redefinition, result=%d", _res);
+  ClassLoaderDataGraph::rollback_redefinition();
+
+  for (int i = 0; i < _new_classes->length(); i++) {
+    SystemDictionary::remove_from_hierarchy(_new_classes->at(i));
+  }
+
+  for (int i = 0; i < _new_classes->length(); i++) {
+    InstanceKlass* new_class = _new_classes->at(i);
+    new_class->set_redefining(false);
+    new_class->old_version()->set_new_version(NULL);
+    new_class->set_old_version(NULL);
+  }
+  _new_classes->clear();
+}
+
+
+// Rewrite faster byte-codes back to their slower equivalent. Undoes rewriting happening in templateTable_xxx.cpp
+// The reason is that once we zero cpool caches, we need to re-resolve all entries again. Faster bytecodes do not
+// do that, they assume that cache entry is resolved already.
+void VM_EnhancedRedefineClasses::unpatch_bytecode(Method* method) {
+  RawBytecodeStream bcs(method);
+  Bytecodes::Code code;
+  Bytecodes::Code java_code;
+  while (!bcs.is_last_bytecode()) {
+    code = bcs.raw_next();
+
+    // dcevm : workaround check _illegal in case of lambda methods etc.
+    // TODO: skip lambda/intrinsic before while loop?  (method()->is_method_handle_intrinsic() || method()->is_compiled_lambda_form())
+    if (code == Bytecodes::_illegal) {
+      return;
+    }
+
+    address bcp = bcs.bcp();
+
+    if (code == Bytecodes::_breakpoint) {
+      int bci = method->bci_from(bcp);
+      code = method->orig_bytecode_at(bci);
+      java_code = Bytecodes::java_code(code);
+      if (code != java_code &&
+           (java_code == Bytecodes::_getfield ||
+            java_code == Bytecodes::_putfield ||
+            java_code == Bytecodes::_aload_0)) {
+        // Let breakpoint table handling unpatch bytecode
+        method->set_orig_bytecode_at(bci, java_code);
+      }
+    } else {
+      java_code = Bytecodes::java_code(code);
+      if (code != java_code &&
+           (java_code == Bytecodes::_getfield ||
+            java_code == Bytecodes::_putfield ||
+            java_code == Bytecodes::_aload_0)) {
+        *bcp = java_code;
+      }
+    }
+
+    // Additionally, we need to unpatch bytecode at bcp+1 for fast_xaccess (which would be fast field access)
+    if (code == Bytecodes::_fast_iaccess_0 || code == Bytecodes::_fast_aaccess_0 || code == Bytecodes::_fast_faccess_0) {
+      Bytecodes::Code code2 = Bytecodes::code_or_bp_at(bcp + 1);
+      assert(code2 == Bytecodes::_fast_igetfield ||
+             code2 == Bytecodes::_fast_agetfield ||
+             code2 == Bytecodes::_fast_fgetfield, "");
+        *(bcp + 1) = Bytecodes::java_code(code2);
+      }
+    }
+  }
+
+// Unevolving classes may point to methods of the_class directly
+// from their constant pool caches, itables, and/or vtables. We
+// use the ClassLoaderDataGraph::classes_do() facility and this helper
+// to fix up these pointers.
+// Adjust cpools and vtables closure
+void VM_EnhancedRedefineClasses::ClearCpoolCacheAndUnpatch::do_klass(Klass* k) {
+  // This is a very busy routine. We don't want too much tracing
+  // printed out.
+  bool trace_name_printed = false;
+  InstanceKlass *the_class = InstanceKlass::cast(_the_class_oop);
+
+  // If the class being redefined is java.lang.Object, we need to fix all
+  // array class vtables also
+  if (k->is_array_klass() && _the_class_oop == SystemDictionary::Object_klass()) {
+    k->vtable().adjust_method_entries(the_class, &trace_name_printed);
+  } else if (k->is_instance_klass()) {
+    HandleMark hm(_thread);
+    InstanceKlass *ik = InstanceKlass::cast(k);
+
+    // HotSpot specific optimization! HotSpot does not currently
+    // support delegation from the bootstrap class loader to a
+    // user-defined class loader. This means that if the bootstrap
+    // class loader is the initiating class loader, then it will also
+    // be the defining class loader. This also means that classes
+    // loaded by the bootstrap class loader cannot refer to classes
+    // loaded by a user-defined class loader. Note: a user-defined
+    // class loader can delegate to the bootstrap class loader.
+    //
+    // If the current class being redefined has a user-defined class
+    // loader as its defining class loader, then we can skip all
+    // classes loaded by the bootstrap class loader.
+    bool is_user_defined =
+            InstanceKlass::cast(_the_class_oop)->class_loader() != NULL;
+    if (is_user_defined && ik->class_loader() == NULL) {
+       return;
+    }
+
+    // Fix the vtable embedded in the_class and subclasses of the_class,
+    // if one exists. We discard scratch_class and we don't keep an
+    // InstanceKlass around to hold obsolete methods so we don't have
+    // any other InstanceKlass embedded vtables to update. The vtable
+    // holds the Method*s for virtual (but not final) methods.
+    // Default methods, or concrete methods in interfaces are stored
+    // in the vtable, so if an interface changes we need to check
+    // adjust_method_entries() for every InstanceKlass, which will also
+    // adjust the default method vtable indices.
+    // We also need to adjust any default method entries that are
+    // not yet in the vtable, because the vtable setup is in progress.
+    // This must be done after we adjust the default_methods and
+    // default_vtable_indices for methods already in the vtable.
+    // If redefining Unsafe, walk all the vtables looking for entries.
+// FIXME - code from standard redefine - if needed, it should switch to new_class
+//    if (ik->vtable_length() > 0 && (_the_class_oop->is_interface()
+//        || _the_class_oop == SystemDictionary::internal_Unsafe_klass()
+//        || ik->is_subtype_of(_the_class_oop))) {
+//      // ik->vtable() creates a wrapper object; rm cleans it up
+//      ResourceMark rm(_thread);
+//
+//      ik->vtable()->adjust_method_entries(the_class, &trace_name_printed);
+//      ik->adjust_default_methods(the_class, &trace_name_printed);
+//    }
+
+    // If the current class has an itable and we are either redefining an
+    // interface or if the current class is a subclass of the_class, then
+    // we potentially have to fix the itable. If we are redefining an
+    // interface, then we have to call adjust_method_entries() for
+    // every InstanceKlass that has an itable since there isn't a
+    // subclass relationship between an interface and an InstanceKlass.
+    // If redefining Unsafe, walk all the itables looking for entries.
+// FIXME - code from standard redefine - if needed, it should switch to new_class
+//    if (ik->itable_length() > 0 && (_the_class_oop->is_interface()
+//        || _the_class_oop == SystemDictionary::internal_Unsafe_klass()
+//        || ik->is_subclass_of(_the_class_oop))) {
+//      // ik->itable() creates a wrapper object; rm cleans it up
+//      ResourceMark rm(_thread);
+//
+//      ik->itable()->adjust_method_entries(the_class, &trace_name_printed);
+//    }
+   
+   constantPoolHandle other_cp = constantPoolHandle(ik->constants());
+
+  // Update host klass of anonymous classes (for example, produced by lambdas) to newest version.
+  if (ik->is_anonymous() && ik->host_klass()->new_version() != NULL) {
+    ik->set_host_klass(InstanceKlass::cast(ik->host_klass()->newest_version()));
+  }
+
+  for (int i = 0; i < other_cp->length(); i++) {
+    if (other_cp->tag_at(i).is_klass()) {
+      Klass* klass = other_cp->resolved_klass_at(i);
+      if (klass->new_version() != NULL) {
+        // Constant pool entry points to redefined class -- update to the new version
+        other_cp->klass_at_put(i, klass->newest_version());
+      }
+      klass = other_cp->resolved_klass_at(i);
+      assert(klass->new_version() == NULL, "Must be new klass!");
+    }
+  }
+
+  // DCEVM - clear whole cache (instead special methods for class/method update in standard redefinition)
+  ConstantPoolCache* cp_cache = other_cp->cache();
+  if (cp_cache != NULL) {
+    cp_cache->clear_entries();
+  }
+
+  // If bytecode rewriting is enabled, we also need to unpatch bytecode to force resolution of zeroed entries
+  if (RewriteBytecodes) {
+    ik->methods_do(unpatch_bytecode);
+  }
+  }
+}
+
+// Clean method data for this class
+void VM_EnhancedRedefineClasses::MethodDataCleaner::do_klass(Klass* k) {
+  if (k->is_instance_klass()) {
+    InstanceKlass *ik = InstanceKlass::cast(k);
+    // Clean MethodData of this class's methods so they don't refer to
+    // old methods that are no longer running.
+    Array<Method*>* methods = ik->methods();
+    int num_methods = methods->length();
+    for (int index = 0; index < num_methods; ++index) {
+      if (methods->at(index)->method_data() != NULL) {
+        methods->at(index)->method_data()->clean_weak_method_links();
+      }
+    }
+  }
+}
+
+void VM_EnhancedRedefineClasses::update_jmethod_ids() {
+  for (int j = 0; j < _matching_methods_length; ++j) {
+    Method* old_method = _matching_old_methods[j];
+    jmethodID jmid = old_method->find_jmethod_id_or_null();
+    if (old_method->new_version() != NULL && jmid == NULL) {
+       // (DCEVM) Have to create jmethodID in this case
+       jmid = old_method->jmethod_id();
+    }
+
+    if (jmid != NULL) {
+      // There is a jmethodID, change it to point to the new method
+      methodHandle new_method_h(_matching_new_methods[j]);
+
+      if (old_method->new_version() == NULL) {
+        methodHandle old_method_h(_matching_old_methods[j]);
+        jmethodID new_jmethod_id = Method::make_jmethod_id(old_method_h->method_holder()->class_loader_data(), old_method_h());
+        bool result = InstanceKlass::cast(old_method_h->method_holder())->update_jmethod_id(old_method_h(), new_jmethod_id);
+      } else {
+        jmethodID mid = new_method_h->jmethod_id();
+        bool result = InstanceKlass::cast(new_method_h->method_holder())->update_jmethod_id(new_method_h(), jmid);
+      }
+
+      Method::change_method_associated_with_jmethod_id(jmid, new_method_h());
+      assert(Method::resolve_jmethod_id(jmid) == _matching_new_methods[j], "should be replaced");
+    }
+  }
+}
+
+/**
+  Set method as obsolete / old / deleted.
+*/
+void VM_EnhancedRedefineClasses::check_methods_and_mark_as_obsolete() {
+  for (int j = 0; j < _matching_methods_length; ++j/*, ++old_index*/) {
+    Method* old_method = _matching_old_methods[j];
+    Method* new_method = _matching_new_methods[j];
+
+    if (MethodComparator::methods_EMCP(old_method, new_method)) {
+      old_method->set_new_version(new_method);
+      new_method->set_old_version(old_method);
+
+      // Transfer breakpoints
+      InstanceKlass *ik = InstanceKlass::cast(old_method->method_holder());
+      for (BreakpointInfo* bp = ik->breakpoints(); bp != NULL; bp = bp->next()) {
+        if (bp->match(old_method)) {
+          assert(bp->match(new_method), "if old method is method, then new method must match too");
+          new_method->set_breakpoint(bp->bci());
+        }
+      }
+    } else {
+      // mark obsolete methods as such
+      old_method->set_is_obsolete();
+
+      // obsolete methods need a unique idnum so they become new entries in
+      // the jmethodID cache in InstanceKlass
+      assert(old_method->method_idnum() == new_method->method_idnum(), "must match");
+//      u2 num = InstanceKlass::cast(_the_class_oop)->next_method_idnum();
+//      if (num != ConstMethod::UNSET_IDNUM) {
+//        old_method->set_method_idnum(num);
+//      }
+    }
+    old_method->set_is_old();
+  }
+  for (int i = 0; i < _deleted_methods_length; ++i) {
+    Method* old_method = _deleted_methods[i];
+
+    old_method->set_is_old();
+    old_method->set_is_obsolete();
+    // FIXME: this flag was added in dcevm10 since it is required in resolvedMethodTable.cpp
+    old_method->set_is_deleted();
+  }
+}
+
+// This internal class transfers the native function registration from old methods
+// to new methods.  It is designed to handle both the simple case of unchanged
+// native methods and the complex cases of native method prefixes being added and/or
+// removed.
+// It expects only to be used during the VM_EnhancedRedefineClasses op (a safepoint).
+//
+// This class is used after the new methods have been installed in "the_class".
+//
+// So, for example, the following must be handled.  Where 'm' is a method and
+// a number followed by an underscore is a prefix.
+//
+//                                      Old Name    New Name
+// Simple transfer to new method        m       ->  m
+// Add prefix                           m       ->  1_m
+// Remove prefix                        1_m     ->  m
+// Simultaneous add of prefixes         m       ->  3_2_1_m
+// Simultaneous removal of prefixes     3_2_1_m ->  m
+// Simultaneous add and remove          1_m     ->  2_m
+// Same, caused by prefix removal only  3_2_1_m ->  3_2_m
+//
+class TransferNativeFunctionRegistration {
+ private:
+  InstanceKlass* the_class;
+  int prefix_count;
+  char** prefixes;
+
+  // Recursively search the binary tree of possibly prefixed method names.
+  // Iteration could be used if all agents were well behaved. Full tree walk is
+  // more resilent to agents not cleaning up intermediate methods.
+  // Branch at each depth in the binary tree is:
+  //    (1) without the prefix.
+  //    (2) with the prefix.
+  // where 'prefix' is the prefix at that 'depth' (first prefix, second prefix,...)
+  Method* search_prefix_name_space(int depth, char* name_str, size_t name_len,
+                                     Symbol* signature) {
+    TempNewSymbol name_symbol = SymbolTable::probe(name_str, (int)name_len);
+    if (name_symbol != NULL) {
+      Method* method = the_class->lookup_method(name_symbol, signature);
+      if (method != NULL) {
+        // Even if prefixed, intermediate methods must exist.
+        if (method->is_native()) {
+          // Wahoo, we found a (possibly prefixed) version of the method, return it.
+          return method;
+        }
+        if (depth < prefix_count) {
+          // Try applying further prefixes (other than this one).
+          method = search_prefix_name_space(depth+1, name_str, name_len, signature);
+          if (method != NULL) {
+            return method; // found
+          }
+
+          // Try adding this prefix to the method name and see if it matches
+          // another method name.
+          char* prefix = prefixes[depth];
+          size_t prefix_len = strlen(prefix);
+          size_t trial_len = name_len + prefix_len;
+          char* trial_name_str = NEW_RESOURCE_ARRAY(char, trial_len + 1);
+          strcpy(trial_name_str, prefix);
+          strcat(trial_name_str, name_str);
+          method = search_prefix_name_space(depth+1, trial_name_str, trial_len,
+                                            signature);
+          if (method != NULL) {
+            // If found along this branch, it was prefixed, mark as such
+            method->set_is_prefixed_native();
+            return method; // found
+          }
+        }
+      }
+    }
+    return NULL;  // This whole branch bore nothing
+  }
+
+  // Return the method name with old prefixes stripped away.
+  char* method_name_without_prefixes(Method* method) {
+    Symbol* name = method->name();
+    char* name_str = name->as_utf8();
+
+    // Old prefixing may be defunct, strip prefixes, if any.
+    for (int i = prefix_count-1; i >= 0; i--) {
+      char* prefix = prefixes[i];
+      size_t prefix_len = strlen(prefix);
+      if (strncmp(prefix, name_str, prefix_len) == 0) {
+        name_str += prefix_len;
+      }
+    }
+    return name_str;
+  }
+
+  // Strip any prefixes off the old native method, then try to find a
+  // (possibly prefixed) new native that matches it.
+  Method* strip_and_search_for_new_native(Method* method) {
+    ResourceMark rm;
+    char* name_str = method_name_without_prefixes(method);
+    return search_prefix_name_space(0, name_str, strlen(name_str),
+                                    method->signature());
+  }
+
+ public:
+
+  // Construct a native method transfer processor for this class.
+  TransferNativeFunctionRegistration(InstanceKlass* _the_class) {
+    assert(SafepointSynchronize::is_at_safepoint(), "sanity check");
+
+    the_class = _the_class;
+    prefixes = JvmtiExport::get_all_native_method_prefixes(&prefix_count);
+  }
+
+  // Attempt to transfer any of the old or deleted methods that are native
+  void transfer_registrations(Method** old_methods, int methods_length) {
+    for (int j = 0; j < methods_length; j++) {
+      Method* old_method = old_methods[j];
+
+      if (old_method->is_native() && old_method->has_native_function()) {
+        Method* new_method = strip_and_search_for_new_native(old_method);
+        if (new_method != NULL) {
+          // Actually set the native function in the new method.
+          // Redefine does not send events (except CFLH), certainly not this
+          // behind the scenes re-registration.
+          new_method->set_native_function(old_method->native_function(),
+                              !Method::native_bind_event_is_interesting);
+        }
+      }
+    }
+  }
+};
+
+// Don't lose the association between a native method and its JNI function.
+void VM_EnhancedRedefineClasses::transfer_old_native_function_registrations(InstanceKlass* the_class) {
+  TransferNativeFunctionRegistration transfer(the_class);
+  transfer.transfer_registrations(_deleted_methods, _deleted_methods_length);
+  transfer.transfer_registrations(_matching_old_methods, _matching_methods_length);
+}
+
+// DCEVM - it always deoptimases everything! (because it is very difficult to find only correct dependencies)
+// Deoptimize all compiled code that depends on this class.
+//
+// If the can_redefine_classes capability is obtained in the onload
+// phase then the compiler has recorded all dependencies from startup.
+// In that case we need only deoptimize and throw away all compiled code
+// that depends on the class.
+//
+// If can_redefine_classes is obtained sometime after the onload
+// phase then the dependency information may be incomplete. In that case
+// the first call to RedefineClasses causes all compiled code to be
+// thrown away. As can_redefine_classes has been obtained then
+// all future compilations will record dependencies so second and
+// subsequent calls to RedefineClasses need only throw away code
+// that depends on the class.
+//
+void VM_EnhancedRedefineClasses::flush_dependent_code(InstanceKlass* k_h, TRAPS) {
+  assert_locked_or_safepoint(Compile_lock);
+
+  // All dependencies have been recorded from startup or this is a second or
+  // subsequent use of RedefineClasses
+  // FIXME: for now, deoptimize all!
+  if (0 && JvmtiExport::all_dependencies_are_recorded()) {
+    CodeCache::flush_evol_dependents_on(k_h);
+  } else {
+    CodeCache::mark_all_nmethods_for_deoptimization();
+
+    ResourceMark rm(THREAD);
+    DeoptimizationMarker dm;
+
+    // Deoptimize all activations depending on marked nmethods
+    Deoptimization::deoptimize_dependents();
+
+    // Make the dependent methods not entrant
+    CodeCache::make_marked_nmethods_not_entrant();
+
+    // From now on we know that the dependency information is complete
+    JvmtiExport::set_all_dependencies_are_recorded(true);
+  }
+}
+
+/**
+  Compare _old_methods and _new_methods arrays and store the result into
+	_matching_old_methods, _matching_new_methods, _added_methods, _deleted_methods
+  
+  Setup _old_methods and _new_methods before the call - it should be called for one class only!
+*/
+void VM_EnhancedRedefineClasses::compute_added_deleted_matching_methods() {
+  Method* old_method;
+  Method* new_method;
+
+  _matching_old_methods = NEW_RESOURCE_ARRAY(Method*, _old_methods->length());
+  _matching_new_methods = NEW_RESOURCE_ARRAY(Method*, _old_methods->length());
+  _added_methods        = NEW_RESOURCE_ARRAY(Method*, _new_methods->length());
+  _deleted_methods      = NEW_RESOURCE_ARRAY(Method*, _old_methods->length());
+
+  _matching_methods_length = 0;
+  _deleted_methods_length  = 0;
+  _added_methods_length    = 0;
+
+  int nj = 0;
+  int oj = 0;
+  while (true) {
+    if (oj >= _old_methods->length()) {
+      if (nj >= _new_methods->length()) {
+        break; // we've looked at everything, done
+      }
+      // New method at the end
+      new_method = _new_methods->at(nj);
+      _added_methods[_added_methods_length++] = new_method;
+      ++nj;
+    } else if (nj >= _new_methods->length()) {
+      // Old method, at the end, is deleted
+      old_method = _old_methods->at(oj);
+      _deleted_methods[_deleted_methods_length++] = old_method;
+      ++oj;
+    } else {
+      old_method = _old_methods->at(oj);
+      new_method = _new_methods->at(nj);
+      if (old_method->name() == new_method->name()) {
+        if (old_method->signature() == new_method->signature()) {
+          _matching_old_methods[_matching_methods_length  ] = old_method;
+          _matching_new_methods[_matching_methods_length++] = new_method;
+          ++nj;
+          ++oj;
+        } else {
+          // added overloaded have already been moved to the end,
+          // so this is a deleted overloaded method
+          _deleted_methods[_deleted_methods_length++] = old_method;
+          ++oj;
+        }
+      } else { // names don't match
+        if (old_method->name()->fast_compare(new_method->name()) > 0) {
+          // new method
+          _added_methods[_added_methods_length++] = new_method;
+          ++nj;
+        } else {
+          // deleted method
+          _deleted_methods[_deleted_methods_length++] = old_method;
+          ++oj;
+        }
+      }
+    }
+  }
+  assert(_matching_methods_length + _deleted_methods_length == _old_methods->length(), "sanity");
+  assert(_matching_methods_length + _added_methods_length == _new_methods->length(), "sanity");
+}
+
+/**
+  FIXME - swap_annotations is never called, check that annotations work
+*/
+void VM_EnhancedRedefineClasses::swap_annotations(InstanceKlass* the_class,
+                                          InstanceKlass* new_class) {
+  // FIXME - probably original implementation only 
+  // Swap annotation fields values
+  Annotations* old_annotations = the_class->annotations();
+  the_class->set_annotations(new_class->annotations());
+  new_class->set_annotations(old_annotations);
+}
+
+
+// Install the redefinition of a class:
+//    - house keeping (flushing breakpoints and caches, deoptimizing
+//      dependent compiled code)
+//    - replacing parts in the_class with parts from new_class
+//    - adding a weak reference to track the obsolete but interesting
+//      parts of the_class
+//    - adjusting constant pool caches and vtables in other classes
+//      that refer to methods in the_class. These adjustments use the
+//      ClassLoaderDataGraph::classes_do() facility which only allows
+//      a helper method to be specified. The interesting parameters
+//      that we would like to pass to the helper method are saved in
+//      static global fields in the VM operation.
+void VM_EnhancedRedefineClasses::redefine_single_class(InstanceKlass* new_class_oop, TRAPS) {
+
+  HandleMark hm(THREAD);   // make sure handles from this call are freed
+
+  if (log_is_enabled(Info, redefine, class, timer)) {
+    _timer_rsc_phase1.start();
+  }
+
+  InstanceKlass* new_class = new_class_oop;
+  InstanceKlass* the_class = InstanceKlass::cast(new_class_oop->old_version());
+  assert(the_class != NULL, "must have old version");
+
+  // Remove all breakpoints in methods of this class
+  JvmtiBreakpoints& jvmti_breakpoints = JvmtiCurrentBreakpoints::get_jvmti_breakpoints();
+  jvmti_breakpoints.clearall_in_class_at_safepoint(the_class);
+
+  // DCEVM Deoptimization is always for whole java world, call only once after all classes are redefined
+  // Deoptimize all compiled code that depends on this class
+  //  flush_dependent_code(the_class, THREAD);
+
+  _old_methods = the_class->methods();
+  _new_methods = new_class->methods();
+  _the_class_oop = the_class;
+  compute_added_deleted_matching_methods();
+
+  // track number of methods that are EMCP for add_previous_version() call below
+  check_methods_and_mark_as_obsolete();
+  update_jmethod_ids();
+
+  _any_class_has_resolved_methods = the_class->has_resolved_methods() || _any_class_has_resolved_methods;
+
+  transfer_old_native_function_registrations(the_class);
+
+
+  // JSR-292 support
+  /* FIXME: j10 dropped support for it?
+  MemberNameTable* mnt = the_class->member_names();
+  assert(new_class->member_names() == NULL, "");
+  if (mnt != NULL) {
+    new_class->set_member_names(mnt);
+    the_class->set_member_names(NULL);
+
+    // FIXME: adjust_method_entries is used in standard hotswap JDK9
+    // bool trace_name_printed = false;
+    // mnt->adjust_method_entries(new_class(), &trace_name_printed);
+  }
+  */
+
+  // Adjust constantpool caches for all classes that reference methods of the evolved class.
+  ClearCpoolCacheAndUnpatch clear_cpool_cache(THREAD);
+  ClassLoaderDataGraph::classes_do(&clear_cpool_cache);
+
+  {
+    ResourceMark rm(THREAD);
+    // increment the classRedefinedCount field in the_class and in any
+    // direct and indirect subclasses of the_class
+    increment_class_counter(the_class, THREAD);
+    log_info(redefine, class, load)
+      ("redefined name=%s, count=%d (avail_mem=" UINT64_FORMAT "K)",
+       the_class->external_name(), java_lang_Class::classRedefinedCount(the_class->java_mirror()), os::available_memory() >> 10);
+    Events::log_redefinition(THREAD, "redefined class name=%s, count=%d",
+                             the_class->external_name(),
+                             java_lang_Class::classRedefinedCount(the_class->java_mirror()));
+
+  }
+  _timer_rsc_phase2.stop();
+} // end redefine_single_class()
+
+
+// Increment the classRedefinedCount field in the specific InstanceKlass
+// and in all direct and indirect subclasses.
+void VM_EnhancedRedefineClasses::increment_class_counter(InstanceKlass *ik, TRAPS) {
+  oop class_mirror = ik->java_mirror();
+  Klass* class_oop = java_lang_Class::as_Klass(class_mirror);
+  int new_count = java_lang_Class::classRedefinedCount(class_mirror) + 1;
+  java_lang_Class::set_classRedefinedCount(class_mirror, new_count);
+
+  if (class_oop != _the_class_oop) {
+    // _the_class_oop count is printed at end of redefine_single_class()
+    log_debug(redefine, class, subclass)("updated count in subclass=%s to %d", ik->external_name(), new_count);
+  }
+
+  for (Klass *subk = ik->subklass(); subk != NULL;
+       subk = subk->next_sibling()) {
+    if (subk->is_instance_klass()) {
+      // Only update instanceKlasses
+      InstanceKlass *subik = InstanceKlass::cast(subk);
+      // recursively do subclasses of the current subclass
+      increment_class_counter(subik, THREAD);
+    }
+  }
+}
+
+// FIXME - class check is currently disabled
+void VM_EnhancedRedefineClasses::CheckClass::do_klass(Klass* k) {
+  return;
+  bool no_old_methods = true;  // be optimistic
+
+  // Both array and instance classes have vtables.
+  // a vtable should never contain old or obsolete methods
+  ResourceMark rm(_thread);
+  if (k->vtable_length() > 0 &&
+      !k->vtable().check_no_old_or_obsolete_entries()) {
+    if (log_is_enabled(Trace, redefine, class, obsolete, metadata)) {
+      log_trace(redefine, class, obsolete, metadata)
+        ("klassVtable::check_no_old_or_obsolete_entries failure -- OLD or OBSOLETE method found -- class: %s",
+         k->signature_name());
+      k->vtable().dump_vtable();
+    }
+    no_old_methods = false;
+  }
+
+  if (k->is_instance_klass()) {
+    HandleMark hm(_thread);
+    InstanceKlass *ik = InstanceKlass::cast(k);
+
+    // an itable should never contain old or obsolete methods
+    if (ik->itable_length() > 0 &&
+        !ik->itable().check_no_old_or_obsolete_entries()) {
+      if (log_is_enabled(Trace, redefine, class, obsolete, metadata)) {
+        log_trace(redefine, class, obsolete, metadata)
+          ("klassItable::check_no_old_or_obsolete_entries failure -- OLD or OBSOLETE method found -- class: %s",
+           ik->signature_name());
+        ik->itable().dump_itable();
+      }
+      no_old_methods = false;
+    }
+
+    // the constant pool cache should never contain non-deleted old or obsolete methods
+    if (ik->constants() != NULL &&
+        ik->constants()->cache() != NULL &&
+        !ik->constants()->cache()->check_no_old_or_obsolete_entries()) {
+      if (log_is_enabled(Trace, redefine, class, obsolete, metadata)) {
+        log_trace(redefine, class, obsolete, metadata)
+          ("cp-cache::check_no_old_or_obsolete_entries failure -- OLD or OBSOLETE method found -- class: %s",
+           ik->signature_name());
+        ik->constants()->cache()->dump_cache();
+      }
+      no_old_methods = false;
+    }
+  }
+
+  // print and fail guarantee if old methods are found.
+  if (!no_old_methods) {
+    if (log_is_enabled(Trace, redefine, class, obsolete, metadata)) {
+      dump_methods();
+    } else {
+      log_trace(redefine, class)("Use the '-Xlog:redefine+class*:' option "
+        "to see more info about the following guarantee() failure.");
+    }
+    guarantee(false, "OLD and/or OBSOLETE method(s) found");
+  }
+}
+
+/**
+ * Logging of all methods (old, new, changed, ...)
+ */
+void VM_EnhancedRedefineClasses::dump_methods() {
+  int j;
+  log_trace(redefine, class, dump)("_old_methods --");
+  for (j = 0; j < _old_methods->length(); ++j) {
+    LogStreamHandle(Trace, redefine, class, dump) log_stream;
+    Method* m = _old_methods->at(j);
+    log_stream.print("%4d  (%5d)  ", j, m->vtable_index());
+    m->access_flags().print_on(&log_stream);
+    log_stream.print(" --  ");
+    m->print_name(&log_stream);
+    log_stream.cr();
+  }
+  log_trace(redefine, class, dump)("_new_methods --");
+  for (j = 0; j < _new_methods->length(); ++j) {
+    LogStreamHandle(Trace, redefine, class, dump) log_stream;
+    Method* m = _new_methods->at(j);
+    log_stream.print("%4d  (%5d)  ", j, m->vtable_index());
+    m->access_flags().print_on(&log_stream);
+    log_stream.print(" --  ");
+    m->print_name(&log_stream);
+    log_stream.cr();
+  }
+  log_trace(redefine, class, dump)("_matching_methods --");
+  for (j = 0; j < _matching_methods_length; ++j) {
+    LogStreamHandle(Trace, redefine, class, dump) log_stream;
+    Method* m = _matching_old_methods[j];
+    log_stream.print("%4d  (%5d)  ", j, m->vtable_index());
+    m->access_flags().print_on(&log_stream);
+    log_stream.print(" --  ");
+    m->print_name();
+    log_stream.cr();
+
+    m = _matching_new_methods[j];
+    log_stream.print("      (%5d)  ", m->vtable_index());
+    m->access_flags().print_on(&log_stream);
+    log_stream.cr();
+  }
+  log_trace(redefine, class, dump)("_deleted_methods --");
+  for (j = 0; j < _deleted_methods_length; ++j) {
+    LogStreamHandle(Trace, redefine, class, dump) log_stream;
+    Method* m = _deleted_methods[j];
+    log_stream.print("%4d  (%5d)  ", j, m->vtable_index());
+    m->access_flags().print_on(&log_stream);
+    log_stream.print(" --  ");
+    m->print_name(&log_stream);
+    log_stream.cr();
+  }
+  log_trace(redefine, class, dump)("_added_methods --");
+  for (j = 0; j < _added_methods_length; ++j) {
+    LogStreamHandle(Trace, redefine, class, dump) log_stream;
+    Method* m = _added_methods[j];
+    log_stream.print("%4d  (%5d)  ", j, m->vtable_index());
+    m->access_flags().print_on(&log_stream);
+    log_stream.print(" --  ");
+    m->print_name(&log_stream);
+    log_stream.cr();
+  }
+}
+
+// TODO - is it called anywhere?
+void VM_EnhancedRedefineClasses::print_on_error(outputStream* st) const {
+  VM_Operation::print_on_error(st);
+  if (_the_class_oop != NULL) {
+    ResourceMark rm;
+    st->print_cr(", redefining class %s", _the_class_oop->external_name());
+  }
+}
+
+/**
+ Helper class to traverse all loaded classes and figure out if the class is affected by redefinition.
+*/
+class AffectedKlassClosure : public KlassClosure {
+ private:
+   GrowableArray<Klass*>* _affected_klasses;
+ public:
+  AffectedKlassClosure(GrowableArray<Klass*>* affected_klasses) : _affected_klasses(affected_klasses) {}
+
+  void do_klass(Klass* klass) {
+    assert(!_affected_klasses->contains(klass), "must not occur more than once!");
+
+    if (klass->new_version() != NULL) {
+      return;
+    }
+    assert(klass->new_version() == NULL, "only last version is valid");
+
+    if (klass->check_redefinition_flag(Klass::MarkedAsAffected)) {
+      _affected_klasses->append(klass);
+      return;
+    }
+
+    int super_depth = klass->super_depth();
+    int idx;
+    for (idx = 0; idx < super_depth; idx++) {
+      Klass* primary = klass->primary_super_of_depth(idx);
+      if (primary == NULL) {
+        break;
+      }
+      if (primary->check_redefinition_flag(Klass::MarkedAsAffected)) {
+        log_trace(redefine, class, load)("found affected class: %s", klass->name()->as_C_string());
+        klass->set_redefinition_flag(Klass::MarkedAsAffected);
+        _affected_klasses->append(klass);
+         return;
+      }
+    }
+
+    int secondary_length = klass->secondary_supers()->length();
+    for (idx = 0; idx < secondary_length; idx++) {
+      Klass* secondary = klass->secondary_supers()->at(idx);
+      if (secondary->check_redefinition_flag(Klass::MarkedAsAffected)) {
+        log_trace(redefine, class, load)("found affected class: %s", klass->name()->as_C_string());
+        klass->set_redefinition_flag(Klass::MarkedAsAffected);
+        _affected_klasses->append(klass);
+        return;
+      }
+    }
+  }
+};
+
+/**
+  Find all affected classes by current redefinition (either because of redefine, class hierarchy or interface change).
+  Affected classes are stored in _affected_klasses and parent classes always precedes child class.  
+*/
+jvmtiError VM_EnhancedRedefineClasses::find_sorted_affected_classes(TRAPS) {
+  for (int i = 0; i < _class_count; i++) {
+    InstanceKlass* klass_handle = get_ik(_class_defs[i].klass);
+    klass_handle->set_redefinition_flag(Klass::MarkedAsAffected);
+    assert(klass_handle->new_version() == NULL, "must be new class");
+
+    log_trace(redefine, class, load)("marking class as being redefined: %s", klass_handle->name()->as_C_string());
+  }
+
+  // Find classes not directly redefined, but affected by a redefinition (because one of its supertypes is redefined)
+  AffectedKlassClosure closure(_affected_klasses);
+  // TODO: j10 - review chancge from SystemDictionary::classes_do(&closure);
+  ClassLoaderDataGraph::dictionary_classes_do(&closure);
+  log_trace(redefine, class, load)("%d classes affected", _affected_klasses->length());
+
+  // Sort the affected klasses such that a supertype is always on a smaller array index than its subtype.
+  jvmtiError result = do_topological_class_sorting(THREAD);
+
+  if (log_is_enabled(Trace, redefine, class, load)) {
+    log_trace(redefine, class, load)("redefine order:");
+    for (int i = 0; i < _affected_klasses->length(); i++) {
+      log_trace(redefine, class, load)("%s", _affected_klasses->at(i)->name()->as_C_string());
+    }
+  }
+  return JVMTI_ERROR_NONE;
+}
+
+/**
+  Pairs of class dependencies (for topological sort)
+*/
+struct KlassPair {
+  const Klass* _left;
+  const Klass* _right;
+
+  KlassPair() { }
+  KlassPair(const Klass* left, const Klass* right) : _left(left), _right(right) { }
+};
+
+static bool match_second(void* value, KlassPair elem) {
+  return elem._right == value;
+}
+
+/**
+ For each class to be redefined parse the bytecode and figure out the superclass and all interfaces.
+ First newly introduced classes (_class_defs) are scanned and then affected classed (_affected_klasses).
+ Affected flag is cleared (clear_redefinition_flag(Klass::MarkedAsAffected))
+
+ For each dependency create a KlassPair instance. Finnaly, affected classes (_affected_klasses) are sorted according to pairs.
+ 
+ TODO - the class file is potentionally parsed multiple times - introduce a cache?
+*/
+jvmtiError VM_EnhancedRedefineClasses::do_topological_class_sorting(TRAPS) {
+  ResourceMark mark(THREAD);
+
+  // Collect dependencies
+  GrowableArray<KlassPair> links;
+  for (int i = 0; i < _class_count; i++) {
+    InstanceKlass* klass = get_ik(_class_defs[i].klass);
+
+    ClassFileStream st((u1*)_class_defs[i].class_bytes,
+                           _class_defs[i].class_byte_count,
+                           "__VM_EnhancedRedefineClasses__",
+                           ClassFileStream::verify);
+
+    Handle protection_domain(THREAD, klass->protection_domain());
+
+    ClassFileParser parser(&st,
+                           klass->name(),
+                           klass->class_loader_data(),
+                           protection_domain,
+                           NULL, // host_klass
+                           NULL, // cp_patches
+                           ClassFileParser::INTERNAL, // publicity level
+                           true,
+                           THREAD);
+
+    const Klass* super_klass = parser.super_klass();
+    if (super_klass != NULL && _affected_klasses->contains((Klass*) super_klass)) {
+      links.append(KlassPair(super_klass, klass));
+    }
+
+    Array<Klass*>* local_interfaces = parser.local_interfaces();
+    for (int j = 0; j < local_interfaces->length(); j++) {
+      Klass* iface = local_interfaces->at(j);
+      if (iface != NULL && _affected_klasses->contains(iface)) {
+        links.append(KlassPair(iface, klass));
+      }
+    }
+
+    assert(klass->check_redefinition_flag(Klass::MarkedAsAffected), "");
+    klass->clear_redefinition_flag(Klass::MarkedAsAffected);
+  }
+
+  // Append dependencies based on current class definition
+  for (int i = 0; i < _affected_klasses->length(); i++) {
+    InstanceKlass* klass = InstanceKlass::cast(_affected_klasses->at(i));
+
+    if (klass->check_redefinition_flag(Klass::MarkedAsAffected)) {
+      klass->clear_redefinition_flag(Klass::MarkedAsAffected);
+      Klass* super_klass = klass->super();
+      if (_affected_klasses->contains(super_klass)) {
+        links.append(KlassPair(super_klass, klass));
+      }
+
+      Array<Klass*>* local_interfaces = klass->local_interfaces();
+      for (int j = 0; j < local_interfaces->length(); j++) {
+        Klass* interfaceKlass = local_interfaces->at(j);
+        if (_affected_klasses->contains(interfaceKlass)) {
+          links.append(KlassPair(interfaceKlass, klass));
+        }
+      }
+    }
+  }
+
+  for (int i = 0; i < _affected_klasses->length(); i++) {
+    int j;
+    for (j = i; j < _affected_klasses->length(); j++) {
+      // Search for node with no incoming edges
+      Klass* klass = _affected_klasses->at(j);
+      int k = links.find(klass, match_second);
+      if (k == -1) break;
+    }
+    if (j == _affected_klasses->length()) {
+      return JVMTI_ERROR_CIRCULAR_CLASS_DEFINITION;
+    }
+
+    // Remove all links from this node
+    const Klass* klass = _affected_klasses->at(j);
+    int k = 0;
+    while (k < links.length()) {
+      if (links.at(k)._left == klass) {
+        links.delete_at(k);
+      } else {
+        k++;
+      }
+    }
+
+    // Swap node
+    Klass* tmp = _affected_klasses->at(j);
+    _affected_klasses->at_put(j, _affected_klasses->at(i));
+    _affected_klasses->at_put(i, tmp);
+  }
+
+  return JVMTI_ERROR_NONE;
+}
diff --git a/src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.hpp b/src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.hpp
new file mode 100644
index 00000000000..b712d69a193
--- /dev/null
+++ b/src/hotspot/share/prims/jvmtiEnhancedRedefineClasses.hpp
@@ -0,0 +1,202 @@
+/*
+ * Copyright (c) 2003, 2016, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef SHARE_VM_PRIMS_JVMTIREDEFINECLASSES2_HPP
+#define SHARE_VM_PRIMS_JVMTIREDEFINECLASSES2_HPP
+
+#include "jvmtifiles/jvmtiEnv.hpp"
+#include "memory/oopFactory.hpp"
+#include "memory/resourceArea.hpp"
+#include "oops/objArrayKlass.hpp"
+#include "oops/objArrayOop.hpp"
+#include "runtime/vm_operations.hpp"
+#include "gc/shared/vmGCOperations.hpp"
+#include "../../../java.base/unix/native/include/jni_md.h"
+
+/**
+ * Enhanced class redefiner.
+ *
+ * This class implements VM_GC_Operation - the usual usage should be:
+ *     VM_EnhancedRedefineClasses op(class_count, class_definitions, jvmti_class_load_kind_redefine);
+ *     VMThread::execute(&op);
+ * Which in turn runs:
+ *   - doit_prologue() - calculate all affected classes (add subclasses etc) and load new class versions
+ *   - doit() - main redefition, adjust existing objects on the heap, clear caches
+ *   - doit_epilogue() - cleanup
+*/
+class VM_EnhancedRedefineClasses: public VM_GC_Operation {
+ private:
+  // These static fields are needed by ClassLoaderDataGraph::classes_do()
+  // facility and the AdjustCpoolCacheAndVtable helper:
+  static Array<Method*>* _old_methods;
+  static Array<Method*>* _new_methods;
+  static Method**      _matching_old_methods;
+  static Method**      _matching_new_methods;
+  static Method**      _deleted_methods;
+  static Method**      _added_methods;
+  static int             _matching_methods_length;
+  static int             _deleted_methods_length;
+  static int             _added_methods_length;
+  static Klass*          _the_class_oop;
+
+  // The instance fields are used to pass information from
+  // doit_prologue() to doit() and doit_epilogue().
+  jint                        _class_count;
+  const jvmtiClassDefinition *_class_defs;  // ptr to _class_count defs
+
+  // This operation is used by both RedefineClasses and
+  // RetransformClasses.  Indicate which.
+  JvmtiClassLoadKind          _class_load_kind;
+
+  // _index_map_count is just an optimization for knowing if
+  // _index_map_p contains any entries.
+  int                         _index_map_count;
+  intArray *                  _index_map_p;
+
+  // _operands_index_map_count is just an optimization for knowing if
+  // _operands_index_map_p contains any entries.
+  int                         _operands_cur_length;
+  int                         _operands_index_map_count;
+  intArray *                  _operands_index_map_p;
+
+  GrowableArray<InstanceKlass*>*      _new_classes;
+  jvmtiError                  _res;
+
+  // Set if any of the InstanceKlasses have entries in the ResolvedMethodTable
+  // to avoid walking after redefinition if the redefined classes do not
+  // have any entries.
+  bool _any_class_has_resolved_methods;
+
+  // Enhanced class redefinition, affected klasses contain all classes which should be redefined
+  // either because of redefine, class hierarchy or interface change
+  GrowableArray<Klass*>*      _affected_klasses;
+
+  int                         _max_redefinition_flags;
+
+  // Performance measurement support. These timers do not cover all
+  // the work done for JVM/TI RedefineClasses() but they do cover
+  // the heavy lifting.
+  elapsedTimer  _timer_rsc_phase1;
+  elapsedTimer  _timer_rsc_phase2;
+  elapsedTimer  _timer_vm_op_prologue;
+
+  // These routines are roughly in call order unless otherwise noted.
+
+  /**
+    Load and link new classes (either redefined or affected by redefinition - subclass, ...)
+
+    - find sorted affected classes
+    - resolve new class
+    - calculate redefine flags (field change, method change, supertype change, ...)
+    - calculate modified fields and mapping to old fields
+    - link new classes
+
+    The result is sotred in _affected_klasses(old definitions) and _new_classes(new definitions) arrays.
+  */
+  jvmtiError load_new_class_versions(TRAPS);
+
+  // Searches for all affected classes and performs a sorting such tha
+  // a supertype is always before a subtype.
+  jvmtiError find_sorted_affected_classes(TRAPS);
+
+  jvmtiError do_topological_class_sorting(TRAPS);
+
+  jvmtiError find_class_bytes(InstanceKlass* the_class, const unsigned char **class_bytes, jint *class_byte_count, jboolean *not_changed);
+  int calculate_redefinition_flags(InstanceKlass* new_class);
+  void calculate_instance_update_information(Klass* new_version);
+
+  void rollback();
+  static void mark_as_scavengable(nmethod* nm);
+  static void unpatch_bytecode(Method* method);
+
+  // Figure out which new methods match old methods in name and signature,
+  // which methods have been added, and which are no longer present
+  void compute_added_deleted_matching_methods();
+
+  // Change jmethodIDs to point to the new methods
+  void update_jmethod_ids();
+
+  // marking methods as old and/or obsolete
+  void check_methods_and_mark_as_obsolete();
+  void transfer_old_native_function_registrations(InstanceKlass* the_class);
+
+  // Install the redefinition of a class
+  void redefine_single_class(InstanceKlass* new_class_oop, TRAPS);
+
+  void swap_annotations(InstanceKlass* new_class,
+                        InstanceKlass* scratch_class);
+
+  // Increment the classRedefinedCount field in the specific InstanceKlass
+  // and in all direct and indirect subclasses.
+  void increment_class_counter(InstanceKlass *ik, TRAPS);
+
+  void flush_dependent_code(InstanceKlass* k_h, TRAPS);
+
+  static void dump_methods();
+
+  // Check that there are no old or obsolete methods
+  class CheckClass : public KlassClosure {
+    Thread* _thread;
+   public:
+    CheckClass(Thread* t) : _thread(t) {}
+    void do_klass(Klass* k);
+  };
+
+  // Unevolving classes may point to methods of the_class directly
+  // from their constant pool caches, itables, and/or vtables. We
+  // use the ClassLoaderDataGraph::classes_do() facility and this helper
+  // to fix up these pointers.
+  class ClearCpoolCacheAndUnpatch : public KlassClosure {
+    Thread* _thread;
+   public:
+    ClearCpoolCacheAndUnpatch(Thread* t) : _thread(t) {}
+    void do_klass(Klass* k);
+  };
+
+  // Clean MethodData out
+  class MethodDataCleaner : public KlassClosure {
+   public:
+    MethodDataCleaner() {}
+    void do_klass(Klass* k);
+  };
+ public:
+  VM_EnhancedRedefineClasses(jint class_count,
+                     const jvmtiClassDefinition *class_defs,
+                     JvmtiClassLoadKind class_load_kind);
+  VMOp_Type type() const { return VMOp_RedefineClasses; }
+  bool doit_prologue();
+  void doit();
+  void doit_epilogue();
+
+  bool allow_nested_vm_operations() const        { return true; }
+  jvmtiError check_error()                       { return _res; }
+
+  // Modifiable test must be shared between IsModifiableClass query
+  // and redefine implementation
+  static bool is_modifiable_class(oop klass_mirror);
+
+  // Error printing
+  void print_on_error(outputStream* st) const;
+};
+#endif // SHARE_VM_PRIMS_JVMTIREDEFINECLASSES2_HPP
diff --git a/src/hotspot/share/prims/jvmtiEnv.cpp b/src/hotspot/share/prims/jvmtiEnv.cpp
index 1dd911fd8d8..bc6ebb2d4af 100644
--- a/src/hotspot/share/prims/jvmtiEnv.cpp
+++ b/src/hotspot/share/prims/jvmtiEnv.cpp
@@ -50,6 +50,7 @@
 #include "prims/jvmtiManageCapabilities.hpp"
 #include "prims/jvmtiRawMonitor.hpp"
 #include "prims/jvmtiRedefineClasses.hpp"
+#include "prims/jvmtiEnhancedRedefineClasses.hpp"
 #include "prims/jvmtiTagMap.hpp"
 #include "prims/jvmtiThreadState.inline.hpp"
 #include "prims/jvmtiUtil.hpp"
@@ -381,8 +382,13 @@ JvmtiEnv::GetClassLoaderClasses(jobject initiating_loader, jint* class_count_ptr
 // is_modifiable_class_ptr - pre-checked for NULL
 jvmtiError
 JvmtiEnv::IsModifiableClass(oop k_mirror, jboolean* is_modifiable_class_ptr) {
-  *is_modifiable_class_ptr = VM_RedefineClasses::is_modifiable_class(k_mirror)?
-                                                       JNI_TRUE : JNI_FALSE;
+  if (AllowEnhancedClassRedefinition) {
+    *is_modifiable_class_ptr = VM_EnhancedRedefineClasses::is_modifiable_class(k_mirror)?
+                                                         JNI_TRUE : JNI_FALSE;
+  } else {
+    *is_modifiable_class_ptr = VM_RedefineClasses::is_modifiable_class(k_mirror)?
+                                                         JNI_TRUE : JNI_FALSE;
+  }
   return JVMTI_ERROR_NONE;
 } /* end IsModifiableClass */
 
@@ -412,7 +418,8 @@ JvmtiEnv::RetransformClasses(jint class_count, const jclass* classes) {
       return JVMTI_ERROR_INVALID_CLASS;
     }
 
-    if (!VM_RedefineClasses::is_modifiable_class(k_mirror)) {
+    if ((!AllowEnhancedClassRedefinition && !VM_RedefineClasses::is_modifiable_class(k_mirror)) ||
+        (AllowEnhancedClassRedefinition && !VM_EnhancedRedefineClasses::is_modifiable_class(k_mirror))) {
       return JVMTI_ERROR_UNMODIFIABLE_CLASS;
     }
 
@@ -444,6 +451,12 @@ JvmtiEnv::RetransformClasses(jint class_count, const jclass* classes) {
     }
     class_definitions[index].klass              = jcls;
   }
+  if (AllowEnhancedClassRedefinition) {
+    MutexLocker sd_mutex(EnhancedRedefineClasses_lock);
+    VM_EnhancedRedefineClasses op(class_count, class_definitions, jvmti_class_load_kind_retransform);
+    VMThread::execute(&op);
+    return (op.check_error());
+  }
   VM_RedefineClasses op(class_count, class_definitions, jvmti_class_load_kind_retransform);
   VMThread::execute(&op);
   return (op.check_error());
@@ -454,7 +467,12 @@ JvmtiEnv::RetransformClasses(jint class_count, const jclass* classes) {
 // class_definitions - pre-checked for NULL
 jvmtiError
 JvmtiEnv::RedefineClasses(jint class_count, const jvmtiClassDefinition* class_definitions) {
-//TODO: add locking
+  if (AllowEnhancedClassRedefinition) {
+    MutexLocker sd_mutex(EnhancedRedefineClasses_lock);
+    VM_EnhancedRedefineClasses op(class_count, class_definitions, jvmti_class_load_kind_redefine);
+    VMThread::execute(&op);
+    return (op.check_error());
+  }
   VM_RedefineClasses op(class_count, class_definitions, jvmti_class_load_kind_redefine);
   VMThread::execute(&op);
   return (op.check_error());
diff --git a/src/hotspot/share/prims/jvmtiExport.cpp b/src/hotspot/share/prims/jvmtiExport.cpp
index c2e86c2406a..2463c1f7bdb 100644
--- a/src/hotspot/share/prims/jvmtiExport.cpp
+++ b/src/hotspot/share/prims/jvmtiExport.cpp
@@ -2799,7 +2799,7 @@ JvmtiDynamicCodeEventCollector::JvmtiDynamicCodeEventCollector() : _code_blobs(N
 // iterate over any code blob descriptors collected and post a
 // DYNAMIC_CODE_GENERATED event to the profiler.
 JvmtiDynamicCodeEventCollector::~JvmtiDynamicCodeEventCollector() {
-  assert(!JavaThread::current()->owns_locks(), "all locks must be released to post deferred events");
+  assert(AllowEnhancedClassRedefinition || !JavaThread::current()->owns_locks(), "all locks must be released to post deferred events");
  // iterate over any code blob descriptors that we collected
  if (_code_blobs != NULL) {
    for (int i=0; i<_code_blobs->length(); i++) {
diff --git a/src/hotspot/share/prims/jvmtiExport.hpp b/src/hotspot/share/prims/jvmtiExport.hpp
index 99288f7590c..bb58ad9432c 100644
--- a/src/hotspot/share/prims/jvmtiExport.hpp
+++ b/src/hotspot/share/prims/jvmtiExport.hpp
@@ -178,6 +178,7 @@ class JvmtiExport : public AllStatic {
   // systems as needed to relax invariant checks.
   static bool _has_redefined_a_class;
   friend class VM_RedefineClasses;
+  friend class VM_EnhancedRedefineClasses;
   inline static void set_has_redefined_a_class() {
     JVMTI_ONLY(_has_redefined_a_class = true;)
   }
diff --git a/src/hotspot/share/prims/jvmtiGetLoadedClasses.cpp b/src/hotspot/share/prims/jvmtiGetLoadedClasses.cpp
index 61a616271f6..60604c645ff 100644
--- a/src/hotspot/share/prims/jvmtiGetLoadedClasses.cpp
+++ b/src/hotspot/share/prims/jvmtiGetLoadedClasses.cpp
@@ -47,7 +47,13 @@ public:
 
   void do_klass(Klass* k) {
     // Collect all jclasses
-    _classStack.push((jclass) _env->jni_reference(Handle(_cur_thread, k->java_mirror())));
+    // DCEVM : LoadedClassesClosure in dcevm7 iterates over classes from SystemDictionary therefore the class "k" is always
+    //         the new version (SystemDictionary stores only new versions). But the LoadedClassesClosure's functionality was
+    //         changed in java8  where jvmtiLoadedClasses collects all classes from all classloaders, therefore we
+    //         must use new versions only.
+    if (k->new_version()==NULL) {
+      _classStack.push((jclass) _env->jni_reference(Handle(_cur_thread, k->java_mirror())));
+    }
   }
 
   int extract(jclass* result_list) {
diff --git a/src/hotspot/share/prims/jvmtiImpl.cpp b/src/hotspot/share/prims/jvmtiImpl.cpp
index 001c592807a..2a92ece916e 100644
--- a/src/hotspot/share/prims/jvmtiImpl.cpp
+++ b/src/hotspot/share/prims/jvmtiImpl.cpp
@@ -293,6 +293,11 @@ void JvmtiBreakpoint::each_method_version_do(method_action meth_act) {
   Symbol* m_name = _method->name();
   Symbol* m_signature = _method->signature();
 
+  // (DCEVM) Go through old versions of method
+  for (Method* m = _method->old_version(); m != NULL; m = m->old_version()) {
+    (m->*meth_act)(_bci);
+  }
+
   // search previous versions if they exist
   for (InstanceKlass* pv_node = ik->previous_versions();
        pv_node != NULL;
diff --git a/src/hotspot/share/prims/resolvedMethodTable.cpp b/src/hotspot/share/prims/resolvedMethodTable.cpp
index c071e0641d4..a9057893368 100644
--- a/src/hotspot/share/prims/resolvedMethodTable.cpp
+++ b/src/hotspot/share/prims/resolvedMethodTable.cpp
@@ -201,6 +201,8 @@ void ResolvedMethodTable::print() {
 void ResolvedMethodTable::adjust_method_entries(bool * trace_name_printed) {
   assert(SafepointSynchronize::is_at_safepoint(), "only called at safepoint");
   // For each entry in RMT, change to new method
+  GrowableArray<oop>* oops_to_add = new GrowableArray<oop>();
+
   for (int i = 0; i < _the_table->table_size(); ++i) {
     for (ResolvedMethodEntry* entry = _the_table->bucket(i);
          entry != NULL;
@@ -215,18 +217,30 @@ void ResolvedMethodTable::adjust_method_entries(bool * trace_name_printed) {
 
       if (old_method->is_old()) {
 
-        Method* new_method;
+        // Method* new_method;
         if (old_method->is_deleted()) {
-          new_method = Universe::throw_no_such_method_error();
-        } else {
-          InstanceKlass* holder = old_method->method_holder();
-          new_method = holder->method_with_idnum(old_method->orig_method_idnum());
-          assert(holder == new_method->method_holder(), "call after swapping redefined guts");
-          assert(new_method != NULL, "method_with_idnum() should not be NULL");
-          assert(old_method != new_method, "sanity check");
+          // FIXME:(DCEVM) - check if exception can be thrown
+          // new_method = Universe::throw_no_such_method_error();
+          continue;
         }
 
-        java_lang_invoke_ResolvedMethodName::set_vmtarget(mem_name, new_method);
+        InstanceKlass* newer_klass = InstanceKlass::cast(old_method->method_holder()->new_version());
+        Method* newer_method = newer_klass->method_with_idnum(old_method->orig_method_idnum());
+
+        assert(newer_klass == newer_method->method_holder(), "call after swapping redefined guts");
+        assert(newer_method != NULL, "method_with_idnum() should not be NULL");
+        assert(old_method != newer_method, "sanity check");
+
+        if (_the_table->lookup(newer_method) != NULL) {
+          // old method was already adjusted if new method exists in _the_table
+            continue;
+        }
+
+        java_lang_invoke_ResolvedMethodName::set_vmtarget(mem_name, newer_method);
+        java_lang_invoke_ResolvedMethodName::set_vmholder_offset(mem_name, newer_method);
+
+        newer_klass->set_has_resolved_methods();
+        oops_to_add->append(mem_name);
 
         ResourceMark rm;
         if (!(*trace_name_printed)) {
@@ -235,9 +249,14 @@ void ResolvedMethodTable::adjust_method_entries(bool * trace_name_printed) {
         }
         log_debug(redefine, class, update, constantpool)
           ("ResolvedMethod method update: %s(%s)",
-           new_method->name()->as_C_string(), new_method->signature()->as_C_string());
+           newer_method->name()->as_C_string(), newer_method->signature()->as_C_string());
       }
     }
+    for (int i = 0; i < oops_to_add->length(); i++) {
+        oop mem_name = oops_to_add->at(i);
+        Method* method = (Method*)java_lang_invoke_ResolvedMethodName::vmtarget(mem_name);
+        _the_table->basic_add(method, Handle(Thread::current(), mem_name));
+    }
   }
 }
 #endif // INCLUDE_JVMTI
diff --git a/src/hotspot/share/runtime/arguments.cpp b/src/hotspot/share/runtime/arguments.cpp
index 740c44cded4..ca57b524593 100644
--- a/src/hotspot/share/runtime/arguments.cpp
+++ b/src/hotspot/share/runtime/arguments.cpp
@@ -2035,6 +2035,36 @@ unsigned int addopens_count = 0;
 unsigned int addmods_count = 0;
 unsigned int patch_mod_count = 0;
 
+// Check consistency of GC selection
+bool Arguments::check_gc_consistency() {
+  // Ensure that the user has not selected conflicting sets
+  // of collectors.
+  uint i = 0;
+  if (UseSerialGC)                       i++;
+  if (UseConcMarkSweepGC)                i++;
+  if (UseParallelGC || UseParallelOldGC) i++;
+  if (UseG1GC)                           i++;
+  if (AllowEnhancedClassRedefinition) {
+    // Must use serial GC. This limitation applies because the instance size changing GC modifications
+    // are only built into the mark and compact algorithm.
+    if (!UseSerialGC && i >= 1) {
+      jio_fprintf(defaultStream::error_stream(),
+                  "Must use the serial GC with enhanced class redefinition\n");
+      return false;
+    }
+  }
+
+  if (i > 1) {
+    jio_fprintf(defaultStream::error_stream(),
+                "Conflicting collector combinations in option list; "
+                "please refer to the release notes for the combinations "
+                "allowed\n");
+    return false;
+  }
+
+  return true;
+}
+
 // Check the consistency of vm_init_args
 bool Arguments::check_vm_args_consistency() {
   // Method for adding checks for flag consistency.
@@ -2051,6 +2081,8 @@ bool Arguments::check_vm_args_consistency() {
     status = false;
   }
 
+  status = status && check_gc_consistency();
+
   if (PrintNMTStatistics) {
 #if INCLUDE_NMT
     if (MemTracker::tracking_level() == NMT_off) {
diff --git a/src/hotspot/share/runtime/arguments.hpp b/src/hotspot/share/runtime/arguments.hpp
index 72bfd2bf9e3..46450cce5c9 100644
--- a/src/hotspot/share/runtime/arguments.hpp
+++ b/src/hotspot/share/runtime/arguments.hpp
@@ -505,6 +505,7 @@ class Arguments : AllStatic {
   static bool process_settings_file(const char* file_name, bool should_exist, jboolean ignore_unrecognized);
 
   static size_t conservative_max_heap_alignment() { return _conservative_max_heap_alignment; }
+
   // Return the maximum size a heap with compressed oops can take
   static size_t max_heap_for_compressed_oops();
 
diff --git a/src/hotspot/share/runtime/globals.hpp b/src/hotspot/share/runtime/globals.hpp
index 6b5713be433..f3cf08fffb6 100644
--- a/src/hotspot/share/runtime/globals.hpp
+++ b/src/hotspot/share/runtime/globals.hpp
@@ -2671,7 +2671,11 @@ define_pd_global(uint64_t,MaxRAM,                    1ULL*G);
           "Start flight recording with options"))                           \
                                                                             \
   experimental(bool, UseFastUnorderedTimeStamps, false,                     \
-          "Use platform unstable time where supported for timestamps only")
+          "Use platform unstable time where supported for timestamps only") \
+                                                                            \
+  product(bool, AllowEnhancedClassRedefinition, true,                       \
+             "Allow enhanced class redefinition beyond swapping method "    \
+             "bodies")
 
 #define VM_FLAGS(develop,                                                   \
                  develop_pd,                                                \
diff --git a/src/hotspot/share/runtime/interfaceSupport.inline.hpp b/src/hotspot/share/runtime/interfaceSupport.inline.hpp
index f5dac06750a..40a8bdd8e62 100644
--- a/src/hotspot/share/runtime/interfaceSupport.inline.hpp
+++ b/src/hotspot/share/runtime/interfaceSupport.inline.hpp
@@ -277,8 +277,8 @@ class ThreadToNativeFromVM : public ThreadStateTransition {
  public:
   ThreadToNativeFromVM(JavaThread *thread) : ThreadStateTransition(thread) {
     // We are leaving the VM at this point and going directly to native code.
-    // Block, if we are in the middle of a safepoint synchronization.
-    assert(!thread->owns_locks(), "must release all locks when leaving VM");
+    // DCEVM allow locks on leaving JVM
+    assert(AllowEnhancedClassRedefinition || !thread->owns_locks(), "must release all locks when leaving VM");
     thread->frame_anchor()->make_walkable(thread);
     trans_and_fence(_thread_in_vm, _thread_in_native);
     // Check for pending. async. exceptions or suspends.
diff --git a/src/hotspot/share/runtime/javaCalls.cpp b/src/hotspot/share/runtime/javaCalls.cpp
index 32040586b36..0eb2fc1ec6b 100644
--- a/src/hotspot/share/runtime/javaCalls.cpp
+++ b/src/hotspot/share/runtime/javaCalls.cpp
@@ -57,7 +57,8 @@ JavaCallWrapper::JavaCallWrapper(const methodHandle& callee_method, Handle recei
   bool clear_pending_exception = true;
 
   guarantee(thread->is_Java_thread(), "crucial check - the VM thread cannot and must not escape to Java code");
-  assert(!thread->owns_locks(), "must release all locks when leaving VM");
+  // DCEVM allow locks on leaving JVM
+  assert(AllowEnhancedClassRedefinition || !thread->owns_locks(), "must release all locks when leaving VM");
   guarantee(thread->can_call_java(), "cannot make java calls from the native compiler");
   _result   = result;
 
diff --git a/src/hotspot/share/runtime/mutexLocker.cpp b/src/hotspot/share/runtime/mutexLocker.cpp
index b258e347c94..0e60bb4b6bd 100644
--- a/src/hotspot/share/runtime/mutexLocker.cpp
+++ b/src/hotspot/share/runtime/mutexLocker.cpp
@@ -5,7 +5,7 @@
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
- *
+ *                     \
  * This code is distributed in the hope that it will be useful, but WITHOUT
  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
@@ -125,6 +125,8 @@ Mutex*   FreeList_lock                = NULL;
 Mutex*   OldSets_lock                 = NULL;
 Monitor* RootRegionScan_lock          = NULL;
 
+Mutex* EnhancedRedefineClasses_lock   = NULL;
+
 Monitor* GCTaskManager_lock           = NULL;
 
 Mutex*   Management_lock              = NULL;
@@ -275,6 +277,7 @@ void mutex_init() {
   def(JNIGlobalActive_lock         , PaddedMutex  , nonleaf-1,   true,  Monitor::_safepoint_check_never);
   def(JNIWeakAlloc_lock            , PaddedMutex  , nonleaf,     true,  Monitor::_safepoint_check_never);
   def(JNIWeakActive_lock           , PaddedMutex  , nonleaf-1,   true,  Monitor::_safepoint_check_never);
+  def(EnhancedRedefineClasses_lock , PaddedMutex  , nonleaf+7,   false, Monitor::_safepoint_check_always);     // for ensuring that class redefinition is not done in parallel
   def(JNICritical_lock             , PaddedMonitor, nonleaf,     true,  Monitor::_safepoint_check_always);     // used for JNI critical regions
   def(AdapterHandlerLibrary_lock   , PaddedMutex  , nonleaf,     true,  Monitor::_safepoint_check_always);
 
diff --git a/src/hotspot/share/runtime/mutexLocker.hpp b/src/hotspot/share/runtime/mutexLocker.hpp
index 12366c7ead1..70aeb1bd0c1 100644
--- a/src/hotspot/share/runtime/mutexLocker.hpp
+++ b/src/hotspot/share/runtime/mutexLocker.hpp
@@ -123,6 +123,8 @@ extern Mutex*   PerfDataManager_lock;            // a long on access to PerfData
 extern Mutex*   ParkerFreeList_lock;
 extern Mutex*   OopMapCacheAlloc_lock;           // protects allocation of oop_map caches
 
+extern Mutex* EnhancedRedefineClasses_lock;      // locks classes from parallel enhanced redefinition
+
 extern Mutex*   FreeList_lock;                   // protects the free region list during safepoints
 extern Mutex*   OldSets_lock;                    // protects the old region sets
 extern Monitor* RootRegionScan_lock;             // used to notify that the CM threads have finished scanning the IM snapshot regions
diff --git a/src/hotspot/share/runtime/reflection.cpp b/src/hotspot/share/runtime/reflection.cpp
index a49b5108821..ed789b0bc2b 100644
--- a/src/hotspot/share/runtime/reflection.cpp
+++ b/src/hotspot/share/runtime/reflection.cpp
@@ -658,6 +658,12 @@ bool Reflection::verify_member_access(const Klass* current_class,
                                       bool classloader_only,
                                       bool protected_restriction,
                                       TRAPS) {
+
+  // (DCEVM) Decide accessibility based on active version
+  if (current_class != NULL) {
+    current_class = current_class->active_version();
+  }
+
   // Verify that current_class can access a member of member_class, where that
   // field's access bits are "access".  We assume that we've already verified
   // that current_class can access member_class.
-- 
2.24.3 (Apple Git-128)

